{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Responsible AI: XAI GenAI project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Background\n",
    "\n",
    "\n",
    "\n",
    "Based on the previous lessons on explainability, post-hoc methods are used to explain the model, such as saliency map, SmoothGrad, LRP, LIME, and SHAP. Take LRP (Layer Wise Relevance Propagation) as an example; it highlights the most relevant pixels to obtain a prediction of the class \"cat\" by backpropagating the relevance. (image source: [Montavon et. al (2016)](https://giorgiomorales.github.io/Layer-wise-Relevance-Propagation-in-Pytorch/))\n",
    "\n",
    "<!-- %%[markdown] -->\n",
    "![LRP example](images/catLRP.jpg)\n",
    "\n",
    "Another example is about text sentiment classification, here we show a case of visualizing the importance of words given the prediction of 'positive':\n",
    "\n",
    "![text example](images/textGradL2.png)\n",
    "\n",
    "where the words highlight with darker colours indicate to be more critical in predicting the sentence to be 'positive' in sentiment.\n",
    "More examples could be found [here](http://34.160.227.66/?models=sst2-tiny&dataset=sst_dev&hidden_modules=Explanations_Attention&layout=default).\n",
    "\n",
    "Both cases above require the class or the prediction of the model. But:\n",
    "\n",
    "***How do you explain a model that does not predict but generates?***\n",
    "\n",
    "In this project, we will work on explaining the generative model based on the dependency between words. We will first look at a simple example, and using Point-wise Mutual Information (PMI) to compute the saliency map of the sentence. After that we will contruct the expereiment step by step, followed by exercises and questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A simple example to start with\n",
    "Given a sample sentence: \n",
    "> *Tokyo is the capital city of Japan.* \n",
    "\n",
    "We are going to explain this sentence by finding the dependency using a saliency map between words.\n",
    "The dependency of two words in the sentence could be measured by [Point-wise mutual information (PMI)](https://en.wikipedia.org/wiki/Pointwise_mutual_information): \n",
    "\n",
    "\n",
    "Mask two words out, e.g. \n",
    "> \\[MASK-1\\] is the captial city of \\[MASK-2\\].\n",
    "\n",
    "\n",
    "Ask the generative model to fill in the sentence 10 times, and we have:\n",
    "\n",
    "| MASK-1      | MASK-2 |\n",
    "| ----------- | ----------- |\n",
    "|    tokyo   |     japan   |\n",
    "|  paris  |     france    |\n",
    "|  london  |     england    |\n",
    "|  paris  |     france    |\n",
    "|  beijing |  china |\n",
    "|    tokyo   |     japan   |\n",
    "|  paris  |     france    |\n",
    "|  paris  |     france    |\n",
    "|  london  |     england    |\n",
    "|  beijing |  china |\n",
    "\n",
    "PMI is calculated by: \n",
    "\n",
    "$PMI(x,y)=log_2⁡ \\frac{p(\\{x,y\\}| s-\\{x,y\\})}{P(\\{x\\}|s-\\{x,y\\})P(\\{y\\}|s-\\{x,y\\})}$\n",
    "\n",
    "where $x$, $y$ represents the words that we masked out, $s$ represents the setence, and $s-\\{x,y\\}$ represents the sentences tokens after removing the words $x$ and $y$.\n",
    "\n",
    "In this example we have $PMI(Tokyo, capital) = log_2 \\frac{0.2}{0.2 * 0.2} = 2.32$\n",
    "\n",
    "Select an interesting word in the sentences; we can now compute the PMI between all other words and the chosen word using the generative model:\n",
    "(Here, we use a longer sentence and run 20 responses per word.)\n",
    "![](images/resPMI.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparation\n",
    "### 2.1 Conda enviroment\n",
    "\n",
    "```\n",
    "conda env create -f environment.yml\n",
    "conda activate xai_llm\n",
    "```\n",
    "\n",
    "\n",
    "### 2.2 Download the offline LLM\n",
    "\n",
    "We use the offline LLM model from hugging face. It's approximately 5 GB.\n",
    "Download it using the comman below, and save it under `./models/`.\n",
    "```\n",
    "huggingface-cli download TheBloke/openchat-3.5-0106-GGUF openchat-3.5-0106.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n",
    "# credit to https://huggingface.co/TheBloke/openchat-3.5-0106-GGUF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mask the sentence and get the responses from LLM\n",
    "### 3.1 Get the input sentence\n",
    "\n",
    "**Remember to change the anchor word index when changing the input sentence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  doctors assess symptoms to diagnose diseases\n"
     ]
    }
   ],
   "source": [
    "# Removed for consistency, so that every run uses the same sentence\n",
    "# def get_input():\n",
    "    # ideally this reads inputs from a file, now it just takes an input\n",
    "    #return input(\"Enter a sentence: \")\n",
    "    \n",
    "# Cell 23 - Reset the sentence\n",
    "sentence = \"doctors assess symptoms to diagnose diseases\"\n",
    "\n",
    "anchor_word_idx = 0 # the index of the interested word\n",
    "prompts_per_word = 20 # number of generated responses  \n",
    "\n",
    "#sentence = get_input()\n",
    "print(\"Sentence: \", sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /zhome/86/6/137189/XAI_GenAI_project-main/models/openchat-3.5-0106.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = openchat_openchat-3.5-0106\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: printing all EOG tokens:\n",
      "load:   - 32000 ('<|end_of_turn|>')\n",
      "load: special tokens cache size = 5\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 8192\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 8192\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = openchat_openchat-3.5-0106\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32002\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 32000 '<|end_of_turn|>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 32000 '<|end_of_turn|>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 98 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_REPACK model buffer size =  3204.00 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.38 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
      "..................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 150\n",
      "llama_context: n_ctx_per_seq = 150\n",
      "llama_context: n_batch       = 150\n",
      "llama_context: n_ubatch      = 150\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (150) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 160 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =    20.00 MiB\n",
      "llama_kv_cache_unified: size =   20.00 MiB (   160 cells,  32 layers,  1/1 seqs), K (f16):   10.00 MiB, V (f16):   10.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 2328\n",
      "llama_context: worst-case: n_tokens = 150, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  150, n_seqs =  1, n_outputs =  150\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  150, n_seqs =  1, n_outputs =  150\n",
      "llama_context:        CPU compute buffer size =    32.93 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{{ 'GPT4 Correct ' + message['role'].title() + ': ' + message['content'] + '<|end_of_turn|>'}}{% endfor %}{% if add_generation_prompt %}{{ 'GPT4 Correct Assistant:' }}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '8192', 'general.name': 'openchat_openchat-3.5-0106', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: openchat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gguf chat template: {{ bos_token }}{% for message in messages %}{{ 'GPT4 Correct ' + message['role'].title() + ': ' + message['content'] + '<|end_of_turn|>'}}{% endfor %}{% if add_generation_prompt %}{{ 'GPT4 Correct Assistant:' }}{% endif %}\n",
      "Using chat eos_token: <|end_of_turn|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "from models.ChatModel import ChatModel\n",
    "model_name = \"openchat\"\n",
    "model = ChatModel(model_name)\n",
    "print(f\"Model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Run the prompts and get all the responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] symptoms to diagnose diseases:   0%| | 0/20 [00:00<?, ?it/sllama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     976.70 ms /    48 tokens (   20.35 ms per token,    49.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2674.34 ms /    18 runs   (  148.57 ms per token,     6.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    3657.59 ms /    66 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Input: [MASK] [MASK] symptoms to diagnose diseases:   5%| | 1/20 [00:03<01:09,  Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1180.76 ms /    10 runs   (  118.08 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1184.06 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Input: [MASK] [MASK] symptoms to diagnose diseases:  10%| | 2/20 [00:04<00:39,  Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1477.48 ms /    13 runs   (  113.65 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1480.97 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Input: [MASK] [MASK] symptoms to diagnose diseases:  15%|▏| 3/20 [00:06<00:31,  Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1478.66 ms /    13 runs   (  113.74 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1482.06 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Input: [MASK] [MASK] symptoms to diagnose diseases:  20%|▏| 4/20 [00:07<00:27,  Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1121.08 ms /    10 runs   (  112.11 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1123.50 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] [MASK] symptoms to diagnose diseases:  25%|▎| 5/20 [00:08<00:22,  Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     918.80 ms /     8 runs   (  114.85 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =     920.86 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Input: [MASK] [MASK] symptoms to diagnose diseases:  30%|▎| 6/20 [00:09<00:18,  Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1125.98 ms /    10 runs   (  112.60 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1128.37 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] [MASK] symptoms to diagnose diseases:  35%|▎| 7/20 [00:11<00:16,  Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'symptoms', 'to', 'diagnose', 'diseases'] ['specific', '[mask]', 'symptoms', 'help', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1346.38 ms /    12 runs   (  112.20 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1349.19 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Input: [MASK] [MASK] symptoms to diagnose diseases:  40%|▍| 8/20 [00:12<00:15,  Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2189.06 ms /    20 runs   (  109.45 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    2193.97 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =         19\n",
      "Input: [MASK] [MASK] symptoms to diagnose diseases:  45%|▍| 9/20 [00:14<00:17,  Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'symptoms', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'carefully', 'examines', 'the', 'patients', '[symptoms]', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1087.80 ms /    10 runs   (  108.78 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1090.11 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Input: [MASK] [MASK] symptoms to diagnose diseases:  50%|▌| 10/20 [00:15<00:14, Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3011.97 ms /    27 runs   (  111.55 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    3018.74 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =         26\n",
      "Input: [MASK] [MASK] symptoms to diagnose diseases:  55%|▌| 11/20 [00:18<00:17, Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'symptoms', 'to', 'diagnose', 'diseases'] ['medical', 'professionals', 'use', 'specific', '[tests]', 'to', 'identify', 'and', '[diagnose]', 'various', 'diseases', 'based', 'on', '[mask]', 'symptoms']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1153.90 ms /    10 runs   (  115.39 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1156.33 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Input: [MASK] [MASK] symptoms to diagnose diseases:  60%|▌| 12/20 [00:19<00:13, Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1419.86 ms /    13 runs   (  109.22 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1422.91 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Input: [MASK] [MASK] symptoms to diagnose diseases:  65%|▋| 13/20 [00:21<00:11, Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1451.30 ms /    13 runs   (  111.64 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1454.52 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Input: [MASK] [MASK] symptoms to diagnose diseases:  70%|▋| 14/20 [00:22<00:09, Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1247.30 ms /    11 runs   (  113.39 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1250.15 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Input: [MASK] [MASK] symptoms to diagnose diseases:  75%|▊| 15/20 [00:23<00:07, Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     773.83 ms /     7 runs   (  110.55 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =     775.48 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Input: [MASK] [MASK] symptoms to diagnose diseases:  80%|▊| 16/20 [00:24<00:05, Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1347.36 ms /    12 runs   (  112.28 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1350.12 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Input: [MASK] [MASK] symptoms to diagnose diseases:  85%|▊| 17/20 [00:26<00:03, Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'symptoms', 'to', 'diagnose', 'diseases'] ['specific', '[mask]', 'symptoms', 'can', 'help', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1573.38 ms /    14 runs   (  112.38 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1576.64 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Input: [MASK] [MASK] symptoms to diagnose diseases:  90%|▉| 18/20 [00:27<00:02, Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1217.82 ms /    11 runs   (  110.71 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1220.39 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Input: [MASK] [MASK] symptoms to diagnose diseases:  95%|▉| 19/20 [00:28<00:01, Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1893.54 ms /    17 runs   (  111.38 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1897.37 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Input: [MASK] [MASK] symptoms to diagnose diseases: 100%|█| 20/20 [00:30<00:00, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'symptoms', 'to', 'diagnose', 'diseases'] ['patients', '[mask]', '[mask]', 'symptoms', 'can', 'help', 'doctors', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess [MASK] to diagnose diseases:   0%|  | 0/20 [00:00<?, ?it/s]Llama.generate: 32 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     327.65 ms /    16 tokens (   20.48 ms per token,    48.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1172.09 ms /    11 runs   (  106.55 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1502.67 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess [MASK] to diagnose diseases:   5%| | 1/20 [00:01<00:28,  1.Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'the', 'patient', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1065.93 ms /    10 runs   (  106.59 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1068.29 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess [MASK] to diagnose diseases:  10%| | 2/20 [00:02<00:22,  1.Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2669.47 ms /    25 runs   (  106.78 ms per token,     9.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    2675.31 ms /    26 tokens\n",
      "llama_perf_context_print:    graphs reused =         24\n",
      "Input: [MASK] assess [MASK] to diagnose diseases:  15%|▏| 3/20 [00:05<00:32,  1.Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1181.39 ms /    11 runs   (  107.40 ms per token,     9.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1183.87 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess [MASK] to diagnose diseases:  20%|▏| 4/20 [00:06<00:25,  1.Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1085.21 ms /    10 runs   (  108.52 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1087.54 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess [MASK] to diagnose diseases:  25%|▎| 5/20 [00:07<00:21,  1.Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1068.69 ms /    10 runs   (  106.87 ms per token,     9.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1070.95 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess [MASK] to diagnose diseases:  30%|▎| 6/20 [00:08<00:18,  1.Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1109.78 ms /    10 runs   (  110.98 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1112.02 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess [MASK] to diagnose diseases:  35%|▎| 7/20 [00:09<00:16,  1.Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1280.21 ms /    12 runs   (  106.68 ms per token,     9.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1282.90 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Input: [MASK] assess [MASK] to diagnose diseases:  40%|▍| 8/20 [00:11<00:15,  1.Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'the', 'patient', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1062.97 ms /    10 runs   (  106.30 ms per token,     9.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1065.72 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess [MASK] to diagnose diseases:  45%|▍| 9/20 [00:12<00:13,  1.Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1284.47 ms /    12 runs   (  107.04 ms per token,     9.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1287.21 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Input: [MASK] assess [MASK] to diagnose diseases:  50%|▌| 10/20 [00:13<00:12,  1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'the', 'patient', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1089.00 ms /    10 runs   (  108.90 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1091.49 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess [MASK] to diagnose diseases:  55%|▌| 11/20 [00:14<00:10,  1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1323.50 ms /    12 runs   (  110.29 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1326.25 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Input: [MASK] assess [MASK] to diagnose diseases:  60%|▌| 12/20 [00:15<00:09,  1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'the', 'patient', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1107.78 ms /    10 runs   (  110.78 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1110.27 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess [MASK] to diagnose diseases:  65%|▋| 13/20 [00:16<00:08,  1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1095.21 ms /    10 runs   (  109.52 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1097.66 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess [MASK] to diagnose diseases:  70%|▋| 14/20 [00:18<00:07,  1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1062.97 ms /    10 runs   (  106.30 ms per token,     9.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1065.46 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess [MASK] to diagnose diseases:  75%|▊| 15/20 [00:19<00:05,  1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1205.02 ms /    11 runs   (  109.55 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1208.18 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Input: [MASK] assess [MASK] to diagnose diseases:  80%|▊| 16/20 [00:20<00:04,  1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'patient', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1125.45 ms /    10 runs   (  112.55 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1127.95 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess [MASK] to diagnose diseases:  85%|▊| 17/20 [00:21<00:03,  1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1292.78 ms /    12 runs   (  107.73 ms per token,     9.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1295.89 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Input: [MASK] assess [MASK] to diagnose diseases:  90%|▉| 18/20 [00:22<00:02,  1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'a', 'patient', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1184.05 ms /    11 runs   (  107.64 ms per token,     9.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1187.02 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Input: [MASK] assess [MASK] to diagnose diseases:  95%|▉| 19/20 [00:23<00:01,  1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1349.79 ms /    12 runs   (  112.48 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1352.72 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Input: [MASK] assess [MASK] to diagnose diseases: 100%|█| 20/20 [00:25<00:00,  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'the', 'patient', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:   0%| | 0/20 [00:00<?, ?Llama.generate: 33 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     780.01 ms /    15 tokens (   52.00 ms per token,    19.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1072.43 ms /    10 runs   (  107.24 ms per token,     9.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1855.60 ms /    25 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:   5%| | 1/20 [00:01<00:3Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['the', 'nurse', 'assesses', 'symptoms', 'and', 'diagnoses', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1124.88 ms /    10 runs   (  112.49 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1127.15 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  10%| | 2/20 [00:02<00:2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['she', 'assesses', 'symptoms', 'and', 'diagnoses', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1094.12 ms /    10 runs   (  109.41 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1096.33 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  15%|▏| 3/20 [00:04<00:2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1509.76 ms /    14 runs   (  107.84 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1512.84 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  20%|▏| 4/20 [00:05<00:2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'to', '[diagnose]', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1194.66 ms /    11 runs   (  108.61 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1197.32 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  25%|▎| 5/20 [00:06<00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1138.76 ms /    10 runs   (  113.88 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1141.83 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  30%|▎| 6/20 [00:07<00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1091.57 ms /    10 runs   (  109.16 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1093.85 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  35%|▎| 7/20 [00:09<00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1120.27 ms /    10 runs   (  112.03 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1122.49 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  40%|▍| 8/20 [00:10<00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1130.98 ms /    10 runs   (  113.10 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1133.25 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  45%|▍| 9/20 [00:11<00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1551.61 ms /    14 runs   (  110.83 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1554.68 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  50%|▌| 10/20 [00:12<00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'to', '[diagnose]', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1239.44 ms /    11 runs   (  112.68 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1241.90 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  55%|▌| 11/20 [00:14<00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1177.71 ms /    10 runs   (  117.77 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1180.07 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  60%|▌| 12/20 [00:15<00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1090.49 ms /    10 runs   (  109.05 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1092.81 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  65%|▋| 13/20 [00:16<00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1248.44 ms /    11 runs   (  113.49 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1251.05 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  70%|▋| 14/20 [00:17<00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['the', 'nurse', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1116.24 ms /    10 runs   (  111.62 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1118.83 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  75%|▊| 15/20 [00:18<00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1124.31 ms /    10 runs   (  112.43 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1126.80 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  80%|▊| 16/20 [00:19<00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1491.77 ms /    14 runs   (  106.55 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1494.85 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  85%|▊| 17/20 [00:21<00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['a', 'doctor', 'assesses', 'symptoms', 'to', '[diagnose]', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1289.85 ms /    12 runs   (  107.49 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1292.51 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  90%|▉| 18/20 [00:22<00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['the', 'medical', 'professional', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1418.60 ms /    13 runs   (  109.12 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1421.51 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  95%|▉| 19/20 [00:24<00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['a', 'doctor', 'assesses', 'symptoms', 'in', 'order', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1077.05 ms /    10 runs   (  107.71 ms per token,     9.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1079.57 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms [MASK] diagnose diseases: 100%|█| 20/20 [00:25<00:\n",
      "Input: [MASK] assess symptoms to [MASK] diseases:   0%|  | 0/20 [00:00<?, ?it/s]Llama.generate: 34 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     309.05 ms /    13 tokens (   23.77 ms per token,    42.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =     959.12 ms /     9 runs   (  106.57 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1270.55 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Input: [MASK] assess symptoms to [MASK] diseases:   5%| | 1/20 [00:01<00:24,  1.Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1109.70 ms /    10 runs   (  110.97 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1111.97 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms to [MASK] diseases:  10%| | 2/20 [00:02<00:21,  1.Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1082.99 ms /    10 runs   (  108.30 ms per token,     9.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1085.83 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms to [MASK] diseases:  15%|▏| 3/20 [00:03<00:19,  1.Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     956.24 ms /     9 runs   (  106.25 ms per token,     9.41 tokens per second)\n",
      "llama_perf_context_print:       total time =     958.26 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Input: [MASK] assess symptoms to [MASK] diseases:  20%|▏| 4/20 [00:04<00:17,  1.Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1073.66 ms /    10 runs   (  107.37 ms per token,     9.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1076.15 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms to [MASK] diseases:  25%|▎| 5/20 [00:05<00:16,  1.Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     979.23 ms /     9 runs   (  108.80 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =     981.36 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Input: [MASK] assess symptoms to [MASK] diseases:  30%|▎| 6/20 [00:06<00:14,  1.Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1082.06 ms /    10 runs   (  108.21 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1084.31 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms to [MASK] diseases:  35%|▎| 7/20 [00:07<00:13,  1.Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     955.48 ms /     9 runs   (  106.16 ms per token,     9.42 tokens per second)\n",
      "llama_perf_context_print:       total time =     957.64 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Input: [MASK] assess symptoms to [MASK] diseases:  40%|▍| 8/20 [00:08<00:12,  1.Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1073.89 ms /    10 runs   (  107.39 ms per token,     9.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1076.12 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms to [MASK] diseases:  45%|▍| 9/20 [00:09<00:11,  1.Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     967.66 ms /     9 runs   (  107.52 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =     969.67 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Input: [MASK] assess symptoms to [MASK] diseases:  50%|▌| 10/20 [00:10<00:10,  1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1090.25 ms /    10 runs   (  109.02 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1092.84 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms to [MASK] diseases:  55%|▌| 11/20 [00:11<00:09,  1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     958.51 ms /     9 runs   (  106.50 ms per token,     9.39 tokens per second)\n",
      "llama_perf_context_print:       total time =     960.55 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Input: [MASK] assess symptoms to [MASK] diseases:  60%|▌| 12/20 [00:12<00:08,  1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1081.97 ms /    10 runs   (  108.20 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1084.26 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms to [MASK] diseases:  65%|▋| 13/20 [00:13<00:07,  1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1068.13 ms /    10 runs   (  106.81 ms per token,     9.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1070.38 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms to [MASK] diseases:  70%|▋| 14/20 [00:14<00:06,  1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1058.22 ms /    10 runs   (  105.82 ms per token,     9.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1060.42 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms to [MASK] diseases:  75%|▊| 15/20 [00:15<00:05,  1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     980.76 ms /     9 runs   (  108.97 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =     982.79 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Input: [MASK] assess symptoms to [MASK] diseases:  80%|▊| 16/20 [00:16<00:04,  1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     948.77 ms /     9 runs   (  105.42 ms per token,     9.49 tokens per second)\n",
      "llama_perf_context_print:       total time =     950.77 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Input: [MASK] assess symptoms to [MASK] diseases:  85%|▊| 17/20 [00:17<00:03,  1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1055.04 ms /    10 runs   (  105.50 ms per token,     9.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1057.27 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms to [MASK] diseases:  90%|▉| 18/20 [00:18<00:02,  1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1108.97 ms /    10 runs   (  110.90 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1111.64 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms to [MASK] diseases:  95%|▉| 19/20 [00:20<00:01,  1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1200.89 ms /    10 runs   (  120.09 ms per token,     8.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1203.69 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms to [MASK] diseases: 100%|█| 20/20 [00:21<00:00,  1\n",
      "Input: [MASK] assess symptoms to diagnose [MASK]:   0%|  | 0/20 [00:00<?, ?it/s]Llama.generate: 35 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     343.61 ms /    13 tokens (   26.43 ms per token,    37.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1089.45 ms /     9 runs   (  121.05 ms per token,     8.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1435.54 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Input: [MASK] assess symptoms to diagnose [MASK]:   5%| | 1/20 [00:01<00:27,  1.Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1451.69 ms /    12 runs   (  120.97 ms per token,     8.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1454.34 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Input: [MASK] assess symptoms to diagnose [MASK]:  10%| | 2/20 [00:02<00:26,  1.Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'physician', 'assesses', 'symptoms', 'to', 'diagnose', 'the', 'illness']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1272.23 ms /    11 runs   (  115.66 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1275.45 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Input: [MASK] assess symptoms to diagnose [MASK]:  15%|▏| 3/20 [00:04<00:23,  1.Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'illness']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1168.16 ms /    10 runs   (  116.82 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1170.38 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms to diagnose [MASK]:  20%|▏| 4/20 [00:05<00:20,  1.Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1415.94 ms /    12 runs   (  118.00 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1418.62 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Input: [MASK] assess symptoms to diagnose [MASK]:  25%|▎| 5/20 [00:06<00:20,  1.Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'the', 'illness']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1414.04 ms /    12 runs   (  117.84 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1417.09 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Input: [MASK] assess symptoms to diagnose [MASK]:  30%|▎| 6/20 [00:08<00:19,  1.Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'the', 'illness']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1248.80 ms /    11 runs   (  113.53 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1251.56 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Input: [MASK] assess symptoms to diagnose [MASK]:  35%|▎| 7/20 [00:09<00:17,  1.Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1476.58 ms /    13 runs   (  113.58 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1479.57 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Input: [MASK] assess symptoms to diagnose [MASK]:  40%|▍| 8/20 [00:10<00:16,  1.Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'physician', 'assesses', 'symptoms', 'to', 'diagnose', 'pneumonia']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1160.38 ms /    10 runs   (  116.04 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1163.36 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms to diagnose [MASK]:  45%|▍| 9/20 [00:12<00:14,  1.Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1506.48 ms /    11 runs   (  136.95 ms per token,     7.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1510.36 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Input: [MASK] assess symptoms to diagnose [MASK]:  50%|▌| 10/20 [00:13<00:13,  1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1330.10 ms /    12 runs   (  110.84 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1333.29 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Input: [MASK] assess symptoms to diagnose [MASK]:  55%|▌| 11/20 [00:14<00:12,  1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'the', 'condition']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1095.55 ms /    10 runs   (  109.56 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1097.98 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms to diagnose [MASK]:  60%|▌| 12/20 [00:16<00:10,  1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1140.07 ms /    10 runs   (  114.01 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1142.43 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms to diagnose [MASK]:  65%|▋| 13/20 [00:17<00:08,  1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1481.13 ms /    13 runs   (  113.93 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1484.16 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Input: [MASK] assess symptoms to diagnose [MASK]:  70%|▋| 14/20 [00:18<00:07,  1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'a', 'medical', 'condition']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1140.14 ms /    10 runs   (  114.01 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1142.52 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms to diagnose [MASK]:  75%|▊| 15/20 [00:19<00:06,  1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1233.49 ms /    11 runs   (  112.14 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1236.12 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Input: [MASK] assess symptoms to diagnose [MASK]:  80%|▊| 16/20 [00:21<00:05,  1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1442.95 ms /    13 runs   (  111.00 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1447.20 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Input: [MASK] assess symptoms to diagnose [MASK]:  85%|▊| 17/20 [00:22<00:03,  1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'pneumonia']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1082.39 ms /    10 runs   (  108.24 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1084.88 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Input: [MASK] assess symptoms to diagnose [MASK]:  90%|▉| 18/20 [00:23<00:02,  1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1191.51 ms /    11 runs   (  108.32 ms per token,     9.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1193.87 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Input: [MASK] assess symptoms to diagnose [MASK]:  95%|▉| 19/20 [00:24<00:01,  1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1317.53 ms /    12 runs   (  109.79 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1320.16 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Input: [MASK] assess symptoms to diagnose [MASK]: 100%|█| 20/20 [00:26<00:00,  1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['a', 'physician', 'assesses', 'symptoms', 'to', 'diagnose', 'a', 'disease']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tools.command_generator import generate_prompts, prefix_prompt\n",
    "from tools.evaluate_response import get_replacements\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_prompts(model, sentence, anchor_idx, prompts_per_word=20):\n",
    "    prompts = generate_prompts(sentence, anchor_idx)\n",
    "    all_replacements = []\n",
    "    for prompt in prompts:\n",
    "        replacements = []\n",
    "        for _ in tqdm(\n",
    "            range(prompts_per_word),\n",
    "            desc=f\"Input: {prompt}\",\n",
    "        ):\n",
    "            response = model.get_response(\n",
    "                prefix_prompt(prompt),\n",
    "            ).strip()\n",
    "            if response:\n",
    "                replacement = get_replacements(prompt, response)\n",
    "                if replacement:\n",
    "                    replacements.append(replacement)\n",
    "        if len(replacements) > 0:\n",
    "            all_replacements.append(replacements)\n",
    "    return all_replacements\n",
    "\n",
    "all_responses = run_prompts(model, sentence, anchor_word_idx, prompts_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['typically', 'doctors [mask] patients [mask]'],\n",
       "  ['specific', '[mask]'],\n",
       "  ['flulike', '[mask]'],\n",
       "  ['flulike', '[mask]'],\n",
       "  ['common', '[mask]'],\n",
       "  ['flu', ''],\n",
       "  ['', ''],\n",
       "  ['migraine', 'migraine'],\n",
       "  ['', ''],\n",
       "  ['flulike', ''],\n",
       "  ['', ''],\n",
       "  ['specific', '[mask]'],\n",
       "  ['flulike', '[mask]'],\n",
       "  ['flulike', '[mask]'],\n",
       "  ['flu', '[mask]'],\n",
       "  ['common', ''],\n",
       "  ['', ''],\n",
       "  ['patients', 'often exhibit [mask]'],\n",
       "  ['flu', '[mask]'],\n",
       "  ['', '']]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize responses\n",
    "all_responses[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load responses\n",
    "import json\n",
    "input_file = \"responses.json\"\n",
    "with open(input_file, \"r\") as f:\n",
    "    all_responses = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 EXERCISE: compute the PMI for each word\n",
    "\n",
    "$PMI(x,y)=log_2⁡ \\frac{p(\\{x,y\\}| s-\\{x,y\\})}{P(\\{x\\}|s-\\{x,y\\})P(\\{y\\}|s-\\{x,y\\})}$\n",
    "\n",
    "* Compute the $P(x)$, $P(y)$ and $P(x,y)$ first and print it out.\n",
    "* Compute the PMI for each word.\n",
    "* Visualize the result by coloring. Tips: you might need to normalize the result first. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from termcolor import colored\n",
    "\n",
    "def compute_pmi(sentence, all_responses, anchor_idx):\n",
    "    \"\"\"Compute PMI between anchor word and each other word.\"\"\"\n",
    "    words = sentence.lower().split()\n",
    "    anchor_word = words[anchor_idx]\n",
    "    pmi_scores = {}\n",
    "    \n",
    "    for other_idx in range(len(words)):\n",
    "        if other_idx == anchor_idx:\n",
    "            continue\n",
    "        \n",
    "        # Get pattern index (skips anchor position)\n",
    "        pattern_idx = other_idx if other_idx < anchor_idx else other_idx - 1\n",
    "        if pattern_idx >= len(all_responses):\n",
    "            continue\n",
    "            \n",
    "        responses = all_responses[pattern_idx]\n",
    "        if not responses:\n",
    "            continue\n",
    "        \n",
    "        # Extract anchor and other word replacements\n",
    "        anchor_replacements = [r[0].lower() for r in responses if len(r) == 2]\n",
    "        other_replacements = [r[1].lower() for r in responses if len(r) == 2]\n",
    "        total = len(anchor_replacements)\n",
    "        \n",
    "        # Calculate probabilities\n",
    "        count_x = sum(w == anchor_word for w in anchor_replacements)\n",
    "        count_y = sum(w == words[other_idx] for w in other_replacements)\n",
    "        count_xy = sum(anchor_replacements[i] == anchor_word and \n",
    "                      other_replacements[i] == words[other_idx] \n",
    "                      for i in range(total))\n",
    "        \n",
    "        P_x = count_x / total\n",
    "        P_y = count_y / total\n",
    "        P_xy = count_xy / total\n",
    "        \n",
    "        # Calculate PMI\n",
    "        if P_x > 0 and P_y > 0 and P_xy > 0:\n",
    "            pmi = math.log2(P_xy / (P_x * P_y))\n",
    "        else:\n",
    "            pmi = float('-inf')\n",
    "        \n",
    "        pmi_scores[other_idx] = {'word': words[other_idx], 'pmi': pmi, \n",
    "                                  'P_x': P_x, 'P_y': P_y, 'P_xy': P_xy}\n",
    "    \n",
    "    return pmi_scores\n",
    "\n",
    "def visualize_pmi(sentence, pmi_scores, anchor_idx):\n",
    "    \"\"\"Visualize PMI with colored words.\"\"\"\n",
    "    words = sentence.split()\n",
    "    \n",
    "    # Normalize PMI values\n",
    "    valid_pmis = [s['pmi'] for s in pmi_scores.values() if s['pmi'] != float('-inf')]\n",
    "    if not valid_pmis:\n",
    "        print(\"No valid PMI scores\")\n",
    "        return\n",
    "    \n",
    "    min_pmi, max_pmi = min(valid_pmis), max(valid_pmis)\n",
    "    pmi_range = max_pmi - min_pmi if max_pmi != min_pmi else 1\n",
    "    \n",
    "    # Color each word\n",
    "    colored_words = []\n",
    "    for i, word in enumerate(words):\n",
    "        if i == anchor_idx:\n",
    "            colored_words.append(colored(word, 'cyan', attrs=['bold']))\n",
    "        elif i in pmi_scores:\n",
    "            pmi = pmi_scores[i]['pmi']\n",
    "            if pmi != float('-inf'):\n",
    "                norm = (pmi - min_pmi) / pmi_range\n",
    "                color = 'green' if norm > 0.66 else 'yellow' if norm > 0.33 else 'red'\n",
    "                colored_words.append(colored(f\"{word}({pmi:.2f})\", color))\n",
    "            else:\n",
    "                colored_words.append(word)\n",
    "        else:\n",
    "            colored_words.append(word)\n",
    "    \n",
    "    print(\"\\n\" + \" \".join(colored_words) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor word: 'doctors'\n",
      "\n",
      "assess          PMI=   -inf  P(x)=0.000 P(y)=0.000 P(xy)=0.000\n",
      "symptoms        PMI=  1.737  P(x)=0.300 P(y)=0.200 P(xy)=0.200\n",
      "to              PMI=  1.152  P(x)=0.100 P(y)=0.450 P(xy)=0.100\n",
      "diagnose        PMI=  0.322  P(x)=0.100 P(y)=0.800 P(xy)=0.100\n",
      "diseases        PMI=  0.737  P(x)=0.200 P(y)=0.300 P(xy)=0.100\n",
      "\n",
      "\u001b[1m\u001b[36mdoctors\u001b[0m assess \u001b[32msymptoms(1.74)\u001b[0m \u001b[33mto(1.15)\u001b[0m \u001b[31mdiagnose(0.32)\u001b[0m \u001b[31mdiseases(0.74)\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute PMI scores\n",
    "pmi_scores = compute_pmi(sentence, all_responses, anchor_word_idx)\n",
    "\n",
    "# Print results\n",
    "words = sentence.lower().split()\n",
    "print(f\"Anchor word: '{words[anchor_word_idx]}'\\n\")\n",
    "for idx in sorted(pmi_scores.keys()):\n",
    "    data = pmi_scores[idx]\n",
    "    print(f\"{data['word']:<15} PMI={data['pmi']:7.3f}  \"\n",
    "          f\"P(x)={data['P_x']:.3f} P(y)={data['P_y']:.3f} P(xy)={data['P_xy']:.3f}\")\n",
    "\n",
    "# Visualize with colors\n",
    "visualize_pmi(sentence, pmi_scores, anchor_word_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PMI Results Interpretation (Higher PMI = stronger association)\n",
    "\n",
    "#### Results for \"doctors assess symptoms to diagnose diseases\"\n",
    "- The visualization shows that \"symptoms\" has the strongest semantic bond with \"doctors\" in this sentence\n",
    "\n",
    "**High PMI: Strong Dependency**\n",
    "- **symptoms (1.74)**: had the highest association with \"doctors\", this means that when both are amsked the model frequently generates them to fill the masked words.\n",
    "\n",
    "**Medium PMI: Moderate Dependency**  \n",
    "- **to (1.15)**: Moderate association.\n",
    "\n",
    "**Low PMI: Weak Dependency**\n",
    "- **diseases (0.74)**: Predictable from context but not uniquely tied to \"doctors\"\n",
    "- **diagnose (0.32)**: Despite being less associated with the word doctor (pmi=0.322), it is very predictable (P(y)=0.80). This means that the word itself (\"diagnose\") is very frequent but paired with alternatives to \"doctors\" (physicians, clinicians)\n",
    "\n",
    "**Negative PMI: No Dependency**\n",
    "- **assess (-inf)**: this means the model never generated \"doctors\" when both were masked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. EXERCISE: Try more examples; maybe come up with your own. Report the results.\n",
    "\n",
    "* Try to come up with more examples and, change the anchor word/number of responses, and observe the results. What does the explanation mean? Do you think it's a nice explanation? Why and why not? \n",
    "* What's the limitation of the current method? When does the method fail to explain? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##############################################################################################################\n",
      "ANALYZING SENTENCE:\n",
      "   'doctors assess symptoms to diagnose diseases'\n",
      "##############################################################################################################\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 0   |   Anchor word: 'doctors'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:   0%| | 0/20 [00Llama.generate: 32 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     514.71 ms /    16 tokens (   32.17 ms per token,    31.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1181.95 ms /    10 runs   (  118.20 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1699.52 ms /    26 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:   5%| | 1/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1245.95 ms /    10 runs   (  124.60 ms per token,     8.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1248.44 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  10%| | 2/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1373.51 ms /    11 runs   (  124.86 ms per token,     8.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1376.51 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  15%|▏| 3/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1729.35 ms /    14 runs   (  123.53 ms per token,     8.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1732.84 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  20%|▏| 4/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'symptoms', 'to', 'diagnose', 'diseases'] ['medical', 'professionals', 'use', '[symptoms]', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1245.10 ms /    10 runs   (  124.51 ms per token,     8.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1248.16 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  25%|▎| 5/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1496.70 ms /    12 runs   (  124.73 ms per token,     8.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1499.63 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  30%|▎| 6/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1926.12 ms /    16 runs   (  120.38 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1930.72 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  35%|▎| 7/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1281.14 ms /    11 runs   (  116.47 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1284.07 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  40%|▍| 8/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1292.61 ms /    11 runs   (  117.51 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1296.02 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  45%|▍| 9/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1441.76 ms /    12 runs   (  120.15 ms per token,     8.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1444.65 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  50%|▌| 10/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1950.65 ms /    17 runs   (  114.74 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1954.46 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  55%|▌| 11/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'symptoms', 'to', 'diagnose', 'diseases'] ['mri', 'scans', 'can', 'help', 'diagnose', 'diseases', 'by', 'identifying', '[mask]', 'symptoms']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1604.81 ms /    14 runs   (  114.63 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1607.91 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  60%|▌| 12/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'symptoms', 'to', 'diagnose', 'diseases'] ['physicians', 'use', '[symptoms]', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1325.62 ms /    10 runs   (  132.56 ms per token,     7.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1328.24 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  65%|▋| 13/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2126.27 ms /    18 runs   (  118.13 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    2130.85 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  70%|▋| 14/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'symptoms', 'to', 'diagnose', 'diseases'] ['patients', 'often', 'exhibit', '[specific]', '[symptoms]', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     848.24 ms /     7 runs   (  121.18 ms per token,     8.25 tokens per second)\n",
      "llama_perf_context_print:       total time =     850.00 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  75%|▊| 15/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1182.50 ms /    10 runs   (  118.25 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1184.91 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  80%|▊| 16/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1288.81 ms /    11 runs   (  117.16 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1291.45 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  85%|▊| 17/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1531.62 ms /    13 runs   (  117.82 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1534.69 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  90%|▉| 18/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1146.07 ms /    10 runs   (  114.61 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1148.50 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  95%|▉| 19/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     814.24 ms /     7 runs   (  116.32 ms per token,     8.60 tokens per second)\n",
      "llama_perf_context_print:       total time =     816.02 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:   0%| | 0/20 [00:0Llama.generate: 32 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     380.14 ms /    16 tokens (   23.76 ms per token,    42.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1269.82 ms /     9 runs   (  141.09 ms per token,     7.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1652.32 ms /    25 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:   5%| | 1/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1518.70 ms /    12 runs   (  126.56 ms per token,     7.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1521.82 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  10%| | 2/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'physician', 'assesses', 'the', 'patient', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1183.80 ms /    10 runs   (  118.38 ms per token,     8.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1186.16 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  15%|▏| 3/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1259.56 ms /    10 runs   (  125.96 ms per token,     7.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1261.83 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  20%|▏| 4/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1408.85 ms /    11 runs   (  128.08 ms per token,     7.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1411.37 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  25%|▎| 5/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1265.10 ms /    10 runs   (  126.51 ms per token,     7.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1267.56 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  30%|▎| 6/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1479.99 ms /    12 runs   (  123.33 ms per token,     8.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1483.08 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  35%|▎| 7/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'the', 'patient', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1211.94 ms /    10 runs   (  121.19 ms per token,     8.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1214.29 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  40%|▍| 8/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1158.23 ms /    10 runs   (  115.82 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1160.59 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  45%|▍| 9/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1134.28 ms /    10 runs   (  113.43 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1136.89 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  50%|▌| 10/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1134.09 ms /    10 runs   (  113.41 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1136.70 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  55%|▌| 11/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1113.39 ms /    10 runs   (  111.34 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1115.65 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  60%|▌| 12/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1162.10 ms /    11 runs   (  105.65 ms per token,     9.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1164.60 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  65%|▋| 13/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1101.10 ms /    10 runs   (  110.11 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1103.34 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  70%|▋| 14/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1062.65 ms /    10 runs   (  106.27 ms per token,     9.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1065.10 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  75%|▊| 15/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1255.78 ms /    12 runs   (  104.65 ms per token,     9.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1258.60 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  80%|▊| 16/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'the', 'patient', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1074.54 ms /    10 runs   (  107.45 ms per token,     9.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1077.07 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  85%|▊| 17/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1278.77 ms /    12 runs   (  106.56 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1281.59 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  90%|▉| 18/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'the', 'patient', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1066.42 ms /    10 runs   (  106.64 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1068.81 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  95%|▉| 19/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1082.16 ms /    10 runs   (  108.22 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1084.47 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:   0%| | 0/20Llama.generate: 33 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     456.42 ms /    15 tokens (   30.43 ms per token,    32.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1058.30 ms /     9 runs   (  117.59 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1517.95 ms /    24 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:   5%| | 1/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1192.61 ms /    10 runs   (  119.26 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1195.18 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  10%| | 2/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1606.82 ms /    14 runs   (  114.77 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1610.06 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  15%|▏| 3/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'to', '[diagnose]', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1151.51 ms /    10 runs   (  115.15 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1153.88 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  20%|▏| 4/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1138.62 ms /    10 runs   (  113.86 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1141.02 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  25%|▎| 5/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1157.87 ms /    10 runs   (  115.79 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1160.32 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  30%|▎| 6/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1173.41 ms /    10 runs   (  117.34 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1176.27 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  35%|▎| 7/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1158.68 ms /    10 runs   (  115.87 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1161.32 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  40%|▍| 8/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1123.54 ms /    10 runs   (  112.35 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1125.94 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  45%|▍| 9/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1111.86 ms /    10 runs   (  111.19 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1114.26 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  50%|▌| 10/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1464.08 ms /    13 runs   (  112.62 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1467.62 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  55%|▌| 11/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['physicians', 'assess', 'symptoms', 'to', '[diagnose]', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1508.67 ms /    14 runs   (  107.76 ms per token,     9.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1512.10 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  60%|▌| 12/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'to', '[diagnose]', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1900.90 ms /    17 runs   (  111.82 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1904.99 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  65%|▋| 13/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1111.87 ms /    10 runs   (  111.19 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1114.41 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  70%|▋| 14/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1236.92 ms /    11 runs   (  112.45 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1239.49 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  75%|▊| 15/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['the', 'physician', 'evaluates', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1116.60 ms /    10 runs   (  111.66 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1118.96 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  80%|▊| 16/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1080.27 ms /    10 runs   (  108.03 ms per token,     9.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1082.66 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  85%|▊| 17/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1084.19 ms /    10 runs   (  108.42 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1086.67 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  90%|▉| 18/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1104.54 ms /    10 runs   (  110.45 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1106.91 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  95%|▉| 19/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1075.16 ms /    10 runs   (  107.52 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1077.51 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:   0%| | 0/20 [00:0Llama.generate: 34 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     997.59 ms /    13 tokens (   76.74 ms per token,    13.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1168.72 ms /     9 runs   (  129.86 ms per token,     7.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    2168.74 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:   5%| | 1/20 [00:0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1090.78 ms /     9 runs   (  121.20 ms per token,     8.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1092.97 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  10%| | 2/20 [00:0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1213.48 ms /    10 runs   (  121.35 ms per token,     8.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1215.86 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  15%|▏| 3/20 [00:0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1121.30 ms /     9 runs   (  124.59 ms per token,     8.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1123.74 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  20%|▏| 4/20 [00:0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1122.00 ms /     9 runs   (  124.67 ms per token,     8.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1124.12 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  25%|▎| 5/20 [00:0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1123.14 ms /     9 runs   (  124.79 ms per token,     8.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1125.20 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  30%|▎| 6/20 [00:0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1229.62 ms /    10 runs   (  122.96 ms per token,     8.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1231.92 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  35%|▎| 7/20 [00:0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1221.26 ms /    10 runs   (  122.13 ms per token,     8.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1223.49 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  40%|▍| 8/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1215.97 ms /    10 runs   (  121.60 ms per token,     8.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1218.22 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  45%|▍| 9/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1180.34 ms /    10 runs   (  118.03 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1182.87 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  50%|▌| 10/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1274.06 ms /    11 runs   (  115.82 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1276.81 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  55%|▌| 11/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', '[mask]', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1282.24 ms /    11 runs   (  116.57 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1284.70 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  60%|▌| 12/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', '[mask]', 'diseases'] ['the', 'physician', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1028.91 ms /     9 runs   (  114.32 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1030.99 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  65%|▋| 13/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1128.57 ms /    10 runs   (  112.86 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1130.86 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  70%|▋| 14/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1014.97 ms /     9 runs   (  112.77 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1016.99 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  75%|▊| 15/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1113.71 ms /    10 runs   (  111.37 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1116.14 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  80%|▊| 16/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1097.35 ms /    10 runs   (  109.73 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1099.58 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  85%|▊| 17/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     966.78 ms /     9 runs   (  107.42 ms per token,     9.31 tokens per second)\n",
      "llama_perf_context_print:       total time =     968.82 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  90%|▉| 18/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1055.87 ms /    10 runs   (  105.59 ms per token,     9.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1058.12 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  95%|▉| 19/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1048.25 ms /    10 runs   (  104.83 ms per token,     9.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1050.52 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:   0%| | 0/20 [00:0Llama.generate: 35 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     420.02 ms /    13 tokens (   32.31 ms per token,    30.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1023.30 ms /    10 runs   (  102.33 ms per token,     9.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1446.05 ms /    23 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:   5%| | 1/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1040.22 ms /    10 runs   (  104.02 ms per token,     9.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1042.60 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  10%| | 2/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1055.44 ms /    10 runs   (  105.54 ms per token,     9.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1057.68 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  15%|▏| 3/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1064.46 ms /    10 runs   (  106.45 ms per token,     9.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1066.93 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  20%|▏| 4/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1253.44 ms /    12 runs   (  104.45 ms per token,     9.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1256.21 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  25%|▎| 5/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'physician', 'assesses', 'symptoms', 'to', 'diagnose', 'illnesses']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1365.41 ms /    13 runs   (  105.03 ms per token,     9.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1368.32 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  30%|▎| 6/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'nurse', 'assesses', 'symptoms', 'to', 'diagnose', 'pneumonia']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1264.96 ms /    12 runs   (  105.41 ms per token,     9.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1267.63 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  35%|▎| 7/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'the', 'condition']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1041.74 ms /    10 runs   (  104.17 ms per token,     9.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1044.05 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  40%|▍| 8/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1154.91 ms /    11 runs   (  104.99 ms per token,     9.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1157.37 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  45%|▍| 9/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'physician', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1154.62 ms /    11 runs   (  104.97 ms per token,     9.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1157.12 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  50%|▌| 10/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1154.54 ms /    11 runs   (  104.96 ms per token,     9.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1157.02 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  55%|▌| 11/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1040.60 ms /    10 runs   (  104.06 ms per token,     9.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1042.83 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  60%|▌| 12/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1156.92 ms /    11 runs   (  105.17 ms per token,     9.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1159.44 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  65%|▋| 13/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1044.36 ms /    10 runs   (  104.44 ms per token,     9.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1047.02 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  70%|▋| 14/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1139.43 ms /    11 runs   (  103.58 ms per token,     9.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1141.95 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  75%|▊| 15/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1040.38 ms /    10 runs   (  104.04 ms per token,     9.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1042.66 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  80%|▊| 16/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1245.17 ms /    12 runs   (  103.76 ms per token,     9.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1247.82 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  85%|▊| 17/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'nurse', 'assesses', 'symptoms', 'to', 'diagnose', 'a', 'condition']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1253.05 ms /    12 runs   (  104.42 ms per token,     9.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1255.86 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  90%|▉| 18/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['a', 'physician', 'assesses', 'symptoms', 'to', 'diagnose', 'a', 'condition']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1015.30 ms /    10 runs   (  101.53 ms per token,     9.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1017.51 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  95%|▉| 19/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1042.18 ms /    10 runs   (  104.22 ms per token,     9.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1044.42 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • fever and chills\n",
      " • specific [mask]\n",
      " • flu [mask]\n",
      " •  \n",
      " • common [mask]\n",
      " • migraine migraine\n",
      " • medical professionals observe [mask] [mask]\n",
      " • flu [mask]\n",
      " • flu [mask]\n",
      " • fever and [mask]\n",
      " •  \n",
      " •  \n",
      " • specific [mask]\n",
      " •  \n",
      " • common \n",
      " • common [mask]\n",
      " • flu [mask]\n",
      " • flulike [mask]\n",
      " • specific [mask]\n",
      " • specific \n",
      " • doctors patients\n",
      " •  \n",
      " • doctors patients\n",
      " • medical professionals patients\n",
      " •  \n",
      " • doctors symptoms\n",
      " •  \n",
      " • doctors patients\n",
      " • physicians patients\n",
      " • doctors symptoms\n",
      " • doctors patients\n",
      " • physicians patients\n",
      " •  \n",
      " • doctors patients\n",
      " • medical professionals symptoms\n",
      " •  \n",
      " • doctors symptoms\n",
      " •  \n",
      " • veterinarians animals\n",
      " • doctors patients\n",
      " • physicians and\n",
      " • doctors to\n",
      " •  \n",
      " • doctors to\n",
      " • medical professionals to\n",
      " • medical professionals to\n",
      " • clinicians and\n",
      " • doctors to\n",
      " • physicians and\n",
      " • doctors to\n",
      " •  \n",
      " •  \n",
      " • patients report [mask] and [mask]\n",
      " • physicians to\n",
      " •  \n",
      " • medical professionals and\n",
      " • physicians and\n",
      " • medical professionals to\n",
      " • physicians and\n",
      " • doctors to\n",
      " • physicians diagnose\n",
      " • physicians diagnose\n",
      " • physicians diagnose\n",
      " • doctors diagnose\n",
      " • medical professionals diagnose\n",
      " • physicians diagnose\n",
      " • physicians diagnose\n",
      " • doctors diagnose\n",
      " • doctors diagnose\n",
      " • doctors diagnose\n",
      " •  \n",
      " •  \n",
      " • doctors diagnose\n",
      " • medical professionals diagnose\n",
      " • medics diagnose\n",
      " • medical professionals diagnose\n",
      " • medical professionals diagnose\n",
      " • doctors diagnose\n",
      " • doctors diagnose\n",
      " • doctors diagnose\n",
      " •  \n",
      " • medical professionals conditions\n",
      " • medical professionals diseases\n",
      " • doctors diseases\n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • physicians diseases\n",
      " •  \n",
      " • medical professionals illnesses\n",
      " • doctors illnesses\n",
      " • doctors diseases\n",
      " • doctors illnesses\n",
      " • medical professionals diseases\n",
      " •  \n",
      " • doctors diseases\n",
      " •  \n",
      " •  \n",
      " • doctors diseases\n",
      " • doctors diseases\n",
      "\n",
      "Total generated: 100\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "assess          PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "symptoms        PMI=  0.737   P(x)=0.450  P(y)=0.200  P(xy)=0.150\n",
      "to              PMI=  1.152   P(x)=0.250  P(y)=0.450  P(xy)=0.250\n",
      "diagnose        PMI=  0.152   P(x)=0.400  P(y)=0.900  P(xy)=0.400\n",
      "diseases        PMI=  0.837   P(x)=0.350  P(y)=0.400  P(xy)=0.250\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 1   |   Anchor word: 'assess'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:   0%| | 0/20 [00Llama.generate: 32 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     349.23 ms /    16 tokens (   21.83 ms per token,    45.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1058.63 ms /    10 runs   (  105.86 ms per token,     9.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1410.54 ms /    26 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:   5%| | 1/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1162.01 ms /    11 runs   (  105.64 ms per token,     9.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1164.47 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  10%| | 2/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1193.97 ms /    11 runs   (  108.54 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1196.44 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  15%|▏| 3/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1931.55 ms /    18 runs   (  107.31 ms per token,     9.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1935.60 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  20%|▏| 4/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1366.79 ms /    13 runs   (  105.14 ms per token,     9.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1369.68 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  25%|▎| 5/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1296.57 ms /    12 runs   (  108.05 ms per token,     9.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1299.27 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  30%|▎| 6/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'symptoms', 'to', 'diagnose', 'diseases'] ['specific', '[mask]', 'symptoms', 'can', 'help', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1171.28 ms /    11 runs   (  106.48 ms per token,     9.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1173.74 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  35%|▎| 7/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1403.64 ms /    13 runs   (  107.97 ms per token,     9.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1406.54 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  40%|▍| 8/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1196.64 ms /    11 runs   (  108.79 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1199.47 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  45%|▍| 9/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1082.37 ms /    10 runs   (  108.24 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1084.70 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  50%|▌| 10/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1043.81 ms /    10 runs   (  104.38 ms per token,     9.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1046.04 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  55%|▌| 11/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1195.01 ms /    11 runs   (  108.64 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1197.48 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  60%|▌| 12/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1197.31 ms /    11 runs   (  108.85 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1199.94 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  65%|▋| 13/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     866.96 ms /     8 runs   (  108.37 ms per token,     9.23 tokens per second)\n",
      "llama_perf_context_print:       total time =     868.77 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  70%|▋| 14/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     869.59 ms /     8 runs   (  108.70 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =     871.40 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  75%|▊| 15/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1088.56 ms /    10 runs   (  108.86 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1091.04 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  80%|▊| 16/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1093.49 ms /    10 runs   (  109.35 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1095.92 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  85%|▊| 17/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1057.72 ms /    10 runs   (  105.77 ms per token,     9.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1060.07 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  90%|▉| 18/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     735.33 ms /     7 runs   (  105.05 ms per token,     9.52 tokens per second)\n",
      "llama_perf_context_print:       total time =     737.14 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] [MASK] symptoms to diagnose diseases:  95%|▉| 19/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1196.52 ms /    11 runs   (  108.77 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1199.04 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:   0%| | 0/20 [00:Llama.generate: 29 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     511.68 ms /    19 tokens (   26.93 ms per token,    37.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1026.37 ms /    10 runs   (  102.64 ms per token,     9.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1540.86 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:   5%| | 1/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1515.79 ms /    14 runs   (  108.27 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1519.07 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  10%| | 2/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', '[mask]', '[mask]', 'to', 'diagnose', 'diseases'] ['doctors', 'apply', '[techniques]', 'to', '[detect]', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2184.41 ms /    20 runs   (  109.22 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    2189.33 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =         19\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  15%|▏| 3/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1908.86 ms /    18 runs   (  106.05 ms per token,     9.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1912.87 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  20%|▏| 4/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1789.31 ms /    17 runs   (  105.25 ms per token,     9.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1793.08 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  25%|▎| 5/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2144.75 ms /    20 runs   (  107.24 ms per token,     9.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    2149.25 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =         19\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  30%|▎| 6/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1829.90 ms /    17 runs   (  107.64 ms per token,     9.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1833.95 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  35%|▎| 7/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2077.97 ms /    20 runs   (  103.90 ms per token,     9.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    2082.82 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =         19\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  40%|▍| 8/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1485.86 ms /    14 runs   (  106.13 ms per token,     9.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1488.96 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  45%|▍| 9/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1381.55 ms /    13 runs   (  106.27 ms per token,     9.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1384.53 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  50%|▌| 10/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1766.26 ms /    14 runs   (  126.16 ms per token,     7.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1770.77 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  55%|▌| 11/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1635.18 ms /    15 runs   (  109.01 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1639.42 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  60%|▌| 12/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2307.65 ms /    22 runs   (  104.89 ms per token,     9.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    2312.80 ms /    23 tokens\n",
      "llama_perf_context_print:    graphs reused =         21\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  65%|▋| 13/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1439.25 ms /    13 runs   (  110.71 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1442.35 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  70%|▋| 14/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1618.35 ms /    15 runs   (  107.89 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1621.91 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  75%|▊| 15/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1742.84 ms /    16 runs   (  108.93 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1746.57 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  80%|▊| 16/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1420.47 ms /    13 runs   (  109.27 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1423.41 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  85%|▊| 17/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1856.04 ms /    18 runs   (  103.11 ms per token,     9.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1860.03 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  90%|▉| 18/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1530.84 ms /    14 runs   (  109.35 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1533.95 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  95%|▉| 19/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1812.47 ms /    17 runs   (  106.62 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1816.45 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:   0%| | 0/2Llama.generate: 33 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     594.97 ms /    15 tokens (   39.66 ms per token,    25.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1550.36 ms /    15 runs   (  103.36 ms per token,     9.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    2149.21 ms /    30 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:   5%| | 1/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1318.16 ms /    13 runs   (  101.40 ms per token,     9.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1321.16 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  10%| | 2/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1094.34 ms /    10 runs   (  109.43 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1096.90 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  15%|▏| 3/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1093.47 ms /    10 runs   (  109.35 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1095.82 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  20%|▏| 4/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1097.68 ms /    10 runs   (  109.77 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1100.31 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  25%|▎| 5/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1053.53 ms /    10 runs   (  105.35 ms per token,     9.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1055.94 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  30%|▎| 6/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1093.62 ms /    10 runs   (  109.36 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1095.96 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  35%|▎| 7/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1313.81 ms /    12 runs   (  109.48 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1316.64 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  40%|▍| 8/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1084.61 ms /    10 runs   (  108.46 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1086.94 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  45%|▍| 9/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1431.78 ms /    13 runs   (  110.14 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1434.86 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  50%|▌| 10/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1220.23 ms /    12 runs   (  101.69 ms per token,     9.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1223.01 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  55%|▌| 11/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1855.86 ms /    17 runs   (  109.17 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1860.30 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  60%|▌| 12/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1974.97 ms /    18 runs   (  109.72 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1979.24 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  65%|▋| 13/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1967.29 ms /    18 runs   (  109.29 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1971.47 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  70%|▋| 14/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1775.63 ms /    16 runs   (  110.98 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1779.32 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  75%|▊| 15/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1144.80 ms /    10 runs   (  114.48 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1147.15 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  80%|▊| 16/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1078.27 ms /    10 runs   (  107.83 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1080.63 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  85%|▊| 17/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     845.26 ms /     8 runs   (  105.66 ms per token,     9.46 tokens per second)\n",
      "llama_perf_context_print:       total time =     847.15 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  90%|▉| 18/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1099.10 ms /    10 runs   (  109.91 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1101.40 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  95%|▉| 19/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1105.87 ms /    10 runs   (  110.59 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1108.19 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:   0%| | 0/20 [00:Llama.generate: 34 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     544.56 ms /    13 tokens (   41.89 ms per token,    23.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1619.45 ms /    16 runs   (  101.22 ms per token,     9.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    2168.26 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:   5%| | 1/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', '[mask]', 'symptoms', 'to', '[mask]', 'diseases'] ['doctors', 'recognize', '[symptoms]', 'to', '[diagnose]', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     874.27 ms /     8 runs   (  109.28 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =     876.42 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  10%| | 2/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1713.58 ms /    16 runs   (  107.10 ms per token,     9.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1717.27 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  15%|▏| 3/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', '[mask]', 'symptoms', 'to', '[mask]', 'diseases'] ['doctors', 'diagnose', '[symptoms]', 'to', '[diseases]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     873.46 ms /     8 runs   (  109.18 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =     875.35 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  20%|▏| 4/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', '[mask]', 'symptoms', 'to', '[mask]', 'diseases'] ['doctors', 'associate', 'symptoms', 'with', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     629.70 ms /     6 runs   (  104.95 ms per token,     9.53 tokens per second)\n",
      "llama_perf_context_print:       total time =     631.15 ms /     7 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  25%|▎| 5/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1857.44 ms /    17 runs   (  109.26 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1861.31 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  30%|▎| 6/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', '[mask]', 'symptoms', 'to', '[mask]', 'diseases'] ['doctors', 'examine', '[symptoms]', 'to', '[diagnose]', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     824.83 ms /     8 runs   (  103.10 ms per token,     9.70 tokens per second)\n",
      "llama_perf_context_print:       total time =     826.73 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  35%|▎| 7/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', '[mask]', 'symptoms', 'to', '[mask]', 'diseases'] ['doctors', 'associate', 'symptoms', 'with', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     657.12 ms /     6 runs   (  109.52 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =     658.58 ms /     7 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  40%|▍| 8/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1093.55 ms /    10 runs   (  109.35 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1095.86 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  45%|▍| 9/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1534.58 ms /    14 runs   (  109.61 ms per token,     9.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1537.85 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  50%|▌| 10/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1088.71 ms /    10 runs   (  108.87 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1090.95 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  55%|▌| 11/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     877.26 ms /     8 runs   (  109.66 ms per token,     9.12 tokens per second)\n",
      "llama_perf_context_print:       total time =     879.09 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  60%|▌| 12/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     766.64 ms /     7 runs   (  109.52 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =     768.50 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  65%|▋| 13/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     878.30 ms /     8 runs   (  109.79 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =     880.10 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  70%|▋| 14/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', '[mask]', 'symptoms', 'to', '[mask]', 'diseases'] ['doctors', 'associate', 'symptoms', 'with', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1299.50 ms /    10 runs   (  129.95 ms per token,     7.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1301.95 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  75%|▊| 15/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1556.10 ms /    14 runs   (  111.15 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1559.64 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  80%|▊| 16/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1826.78 ms /    17 runs   (  107.46 ms per token,     9.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1830.57 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  85%|▊| 17/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', '[mask]', 'symptoms', 'to', '[mask]', 'diseases'] ['doctors', 'diagnose', '[symptoms]', 'to', '[associate]', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     869.13 ms /     8 runs   (  108.64 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =     870.97 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  90%|▉| 18/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', '[mask]', 'symptoms', 'to', '[mask]', 'diseases'] ['doctors', 'associate', 'symptoms', 'with', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     886.30 ms /     8 runs   (  110.79 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =     888.44 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  95%|▉| 19/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     889.49 ms /     8 runs   (  111.19 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =     891.30 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', '[mask]', 'symptoms', 'to', '[mask]', 'diseases'] ['doctors', 'associate', 'symptoms', 'with', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:   0%| | 0/20 [00:Llama.generate: 35 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     405.18 ms /    13 tokens (   31.17 ms per token,    32.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =     949.63 ms /     9 runs   (  105.51 ms per token,     9.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1357.14 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:   5%| | 1/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2831.40 ms /    26 runs   (  108.90 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    2837.55 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =         25\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  10%| | 2/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1174.42 ms /    11 runs   (  106.77 ms per token,     9.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1176.99 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  15%|▏| 3/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     873.70 ms /     8 runs   (  109.21 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =     875.48 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  20%|▏| 4/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1309.04 ms /    12 runs   (  109.09 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1311.68 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  25%|▎| 5/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1524.10 ms /    14 runs   (  108.86 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1527.15 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  30%|▎| 6/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1088.29 ms /    10 runs   (  108.83 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1090.48 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  35%|▎| 7/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     851.19 ms /     8 runs   (  106.40 ms per token,     9.40 tokens per second)\n",
      "llama_perf_context_print:       total time =     852.96 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  40%|▍| 8/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1053.81 ms /    10 runs   (  105.38 ms per token,     9.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1056.19 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  45%|▍| 9/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1150.62 ms /    11 runs   (  104.60 ms per token,     9.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1153.07 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  50%|▌| 10/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1126.19 ms /    11 runs   (  102.38 ms per token,     9.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1128.83 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  55%|▌| 11/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1120.29 ms /    10 runs   (  112.03 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1122.83 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  60%|▌| 12/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1061.95 ms /    10 runs   (  106.20 ms per token,     9.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1064.28 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  65%|▋| 13/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     833.01 ms /     8 runs   (  104.13 ms per token,     9.60 tokens per second)\n",
      "llama_perf_context_print:       total time =     835.37 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  70%|▋| 14/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1376.96 ms /    13 runs   (  105.92 ms per token,     9.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1379.85 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  75%|▊| 15/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     941.50 ms /     9 runs   (  104.61 ms per token,     9.56 tokens per second)\n",
      "llama_perf_context_print:       total time =     943.54 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  80%|▊| 16/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     943.54 ms /     9 runs   (  104.84 ms per token,     9.54 tokens per second)\n",
      "llama_perf_context_print:       total time =     945.55 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  85%|▊| 17/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     968.80 ms /     9 runs   (  107.64 ms per token,     9.29 tokens per second)\n",
      "llama_perf_context_print:       total time =     970.99 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  90%|▉| 18/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     897.30 ms /     8 runs   (  112.16 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =     899.41 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  95%|▉| 19/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1127.43 ms /    10 runs   (  112.74 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1129.99 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • common flu [mask]\n",
      " • flu [mask]\n",
      " • flu [mask]\n",
      " • physicians observe [patients] [presenting]\n",
      " • flulike [mask]\n",
      " •  \n",
      " • flu [mask]\n",
      " • flulike [mask]\n",
      " • fever [mask]\n",
      " • flulike \n",
      " • flulike \n",
      " • fever [mask]\n",
      " • fever and headache\n",
      " • specific \n",
      " • common flu\n",
      " • mri mri\n",
      " • specific [mask]\n",
      " • flulike \n",
      " • common \n",
      " • flu [mask]\n",
      " • consult and investigate\n",
      " •  \n",
      " • carefully [examine] [patients]\n",
      " • consult [data] and [research]\n",
      " • consult [journals] and [tests]\n",
      " • carefully [examine] [patients]\n",
      " • closely [observe] [examine]\n",
      " • rely on [clinical] [evidence]\n",
      " • employ various [methods]\n",
      " • carefully [mask] [mask]\n",
      " • carefully examine [patients]\n",
      " • employ various [tools]\n",
      " • rely on [observations] and [tests]\n",
      " • consult [textbooks]\n",
      " • use various [tools]\n",
      " • use [stethoscopes]\n",
      " • must consult and examine patients\n",
      " • examine [patients] [thoroughly]\n",
      " • carefully examine [patients]\n",
      " • carefully [examine] [patients]\n",
      " • often encounter that help them [mask]\n",
      " • without [mask] [mask]\n",
      " • observe to\n",
      " • identify to\n",
      " • observe to\n",
      " • assess and\n",
      " • observe to\n",
      " • identify in order to\n",
      " • observe to\n",
      " • carefully [mask] [mask]\n",
      " • observe in order to\n",
      " • carefully [mask] to [mask]\n",
      " • carefully [observe] to [accurately]\n",
      " • usually [mask] in order to [mask]\n",
      " • often [mask] to [mask]\n",
      " • observe to\n",
      " • observe to\n",
      " • usually help\n",
      " • identify to\n",
      " • observe to\n",
      " •  \n",
      " • identify diagnose\n",
      " •  \n",
      " •  \n",
      " • associate \n",
      " •  \n",
      " •  \n",
      " • associate \n",
      " • identify diagnose\n",
      " • diagnose [mask] [mask]\n",
      " • detect diagnose\n",
      " • identify diagnose\n",
      " • diagnose \n",
      " •  \n",
      " • identify diagnose\n",
      " • identify [mask] [mask]\n",
      " •  \n",
      " •  \n",
      " • observe diagnose\n",
      " •  \n",
      " • evaluate conditions\n",
      " • use [stethoscopes] to listen to [pneumonia]\n",
      " • examine illnesses\n",
      " • evaluate conditions\n",
      " • carefully observe illnesses\n",
      " • often [mask] [mask]\n",
      " • evaluate diseases\n",
      " • examine diseases\n",
      " • observe diseases\n",
      " • observe illnesses\n",
      " • analyze illnesses\n",
      " • observe diseases\n",
      " • observe diseases\n",
      " • observe patients\n",
      " • carefully observe ailments\n",
      " • observe illnesses\n",
      " • observe illnesses\n",
      " • observe illnesses\n",
      " • examine conditions\n",
      " • observe conditions\n",
      "\n",
      "Total generated: 100\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "doctors         PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "symptoms        PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "to              PMI=   -inf   P(x)=0.050  P(y)=0.450  P(xy)=0.000\n",
      "diagnose        PMI=   -inf   P(x)=0.000  P(y)=0.300  P(xy)=0.000\n",
      "diseases        PMI=   -inf   P(x)=0.000  P(y)=0.250  P(xy)=0.000\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 2   |   Anchor word: 'symptoms'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:   0%| | 0/20 [00:0Llama.generate: 29 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     454.88 ms /    19 tokens (   23.94 ms per token,    41.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1133.55 ms /     9 runs   (  125.95 ms per token,     7.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1591.00 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:   5%| | 1/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1380.06 ms /    12 runs   (  115.00 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1383.88 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  10%| | 2/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'the', 'patient', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1135.51 ms /    10 runs   (  113.55 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1137.92 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  15%|▏| 3/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1362.01 ms /    12 runs   (  113.50 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1365.29 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  20%|▏| 4/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1364.59 ms /    12 runs   (  113.72 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1367.69 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  25%|▎| 5/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'blood', 'tests', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1357.81 ms /    12 runs   (  113.15 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1360.91 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  30%|▎| 6/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'blood', 'samples', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1346.89 ms /    12 runs   (  112.24 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1349.73 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  35%|▎| 7/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'the', 'patient', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1340.93 ms /    12 runs   (  111.74 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1344.05 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  40%|▍| 8/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'the', 'patient', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1208.96 ms /    11 runs   (  109.91 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1211.55 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  45%|▍| 9/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1298.61 ms /    12 runs   (  108.22 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1302.08 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  50%|▌| 10/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'the', 'patient', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1070.99 ms /    10 runs   (  107.10 ms per token,     9.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1073.38 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  55%|▌| 11/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1177.42 ms /    11 runs   (  107.04 ms per token,     9.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1179.91 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  60%|▌| 12/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1074.60 ms /    10 runs   (  107.46 ms per token,     9.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1076.88 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  65%|▋| 13/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1062.15 ms /    10 runs   (  106.21 ms per token,     9.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1064.57 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  70%|▋| 14/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1281.55 ms /    12 runs   (  106.80 ms per token,     9.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1284.25 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  75%|▊| 15/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'the', 'patient', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1066.69 ms /    10 runs   (  106.67 ms per token,     9.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1069.20 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  80%|▊| 16/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1278.44 ms /    12 runs   (  106.54 ms per token,     9.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1281.10 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  85%|▊| 17/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'blood', 'samples', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1066.71 ms /    10 runs   (  106.67 ms per token,     9.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1068.93 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  90%|▉| 18/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1066.40 ms /    10 runs   (  106.64 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1068.63 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess [MASK] to diagnose diseases:  95%|▉| 19/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1067.88 ms /    10 runs   (  106.79 ms per token,     9.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1070.22 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:   0%| | 0/20 [00:Llama.generate: 29 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     430.39 ms /    19 tokens (   22.65 ms per token,    44.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1775.39 ms /    15 runs   (  118.36 ms per token,     8.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    2209.31 ms /    34 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:   5%| | 1/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1878.06 ms /    16 runs   (  117.38 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1881.46 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  10%| | 2/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1761.44 ms /    13 runs   (  135.50 ms per token,     7.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1764.25 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  15%|▏| 3/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1641.32 ms /    14 runs   (  117.24 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1644.30 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  20%|▏| 4/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1522.12 ms /    13 runs   (  117.09 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1524.90 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  25%|▎| 5/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2251.90 ms /    19 runs   (  118.52 ms per token,     8.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    2256.30 ms /    20 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  30%|▎| 6/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1632.04 ms /    14 runs   (  116.57 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1635.32 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  35%|▎| 7/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1750.37 ms /    15 runs   (  116.69 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1753.96 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  40%|▍| 8/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1991.74 ms /    17 runs   (  117.16 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1995.36 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  45%|▍| 9/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2350.07 ms /    20 runs   (  117.50 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    2354.54 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =         19\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  50%|▌| 10/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1789.31 ms /    16 runs   (  111.83 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1793.93 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  55%|▌| 11/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1447.51 ms /    13 runs   (  111.35 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1450.60 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  60%|▌| 12/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2167.97 ms /    19 runs   (  114.10 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    2172.06 ms /    20 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  65%|▋| 13/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1660.56 ms /    15 runs   (  110.70 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1664.11 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  70%|▋| 14/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3826.45 ms /    34 runs   (  112.54 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    3834.02 ms /    35 tokens\n",
      "llama_perf_context_print:    graphs reused =         33\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  75%|▊| 15/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1487.29 ms /    13 runs   (  114.41 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1490.09 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  80%|▊| 16/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1764.46 ms /    16 runs   (  110.28 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1767.88 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  85%|▊| 17/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1473.92 ms /    13 runs   (  113.38 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1476.70 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  90%|▉| 18/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1984.04 ms /    18 runs   (  110.22 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1988.23 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: doctors [MASK] [MASK] to diagnose diseases:  95%|▉| 19/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1971.27 ms /    18 runs   (  109.51 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1975.34 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:   0%| | 0/20 Llama.generate: 30 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     580.82 ms /    18 tokens (   32.27 ms per token,    30.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1003.80 ms /     9 runs   (  111.53 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1586.88 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:   5%| | 1/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1286.23 ms /    12 runs   (  107.19 ms per token,     9.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1288.82 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  10%| | 2/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', 'assess', '[mask]', '[mask]', 'diagnose', 'diseases'] ['doctors', 'assess', 'patient', 'medical', 'history', 'before', 'diagnosing', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1091.09 ms /    10 runs   (  109.11 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1093.37 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  15%|▏| 3/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1209.61 ms /    11 runs   (  109.96 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1211.97 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  20%|▏| 4/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1404.17 ms /    13 runs   (  108.01 ms per token,     9.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1407.05 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  25%|▎| 5/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     981.09 ms /     9 runs   (  109.01 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =     983.49 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  30%|▎| 6/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1082.54 ms /    10 runs   (  108.25 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1084.72 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  35%|▎| 7/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1093.33 ms /    10 runs   (  109.33 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1095.57 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  40%|▍| 8/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1168.73 ms /    11 runs   (  106.25 ms per token,     9.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1171.18 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  45%|▍| 9/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1236.05 ms /    11 runs   (  112.37 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1238.44 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  50%|▌| 10/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1221.11 ms /    11 runs   (  111.01 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1223.71 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  55%|▌| 11/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1065.82 ms /    10 runs   (  106.58 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1068.08 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  60%|▌| 12/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1407.19 ms /    13 runs   (  108.25 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1410.01 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  65%|▋| 13/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', 'assess', '[mask]', '[mask]', 'diagnose', 'diseases'] ['doctors', 'assess', 'patient', 'medical', 'histories', 'before', 'diagnosing', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1084.06 ms /    10 runs   (  108.41 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1086.45 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  70%|▋| 14/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1075.24 ms /    10 runs   (  107.52 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1077.55 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  75%|▊| 15/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1073.49 ms /    10 runs   (  107.35 ms per token,     9.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1075.65 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  80%|▊| 16/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1080.61 ms /    10 runs   (  108.06 ms per token,     9.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1082.80 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  85%|▊| 17/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1081.05 ms /    10 runs   (  108.10 ms per token,     9.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1083.23 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  90%|▉| 18/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1213.99 ms /    11 runs   (  110.36 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1216.34 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  95%|▉| 19/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1072.93 ms /    10 runs   (  107.29 ms per token,     9.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1075.08 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:   0%| | 0/20 [00:00Llama.generate: 34 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     475.37 ms /    13 tokens (   36.57 ms per token,    27.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1217.13 ms /     9 runs   (  135.24 ms per token,     7.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1695.58 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:   5%| | 1/20 [00:01Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1558.08 ms /    10 runs   (  155.81 ms per token,     6.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1560.37 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  10%| | 2/20 [00:03Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1316.15 ms /    10 runs   (  131.62 ms per token,     7.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1318.56 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  15%|▏| 3/20 [00:04Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1288.95 ms /    10 runs   (  128.89 ms per token,     7.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1291.47 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  20%|▏| 4/20 [00:05Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1246.79 ms /    10 runs   (  124.68 ms per token,     8.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1248.94 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  25%|▎| 5/20 [00:07Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     990.90 ms /     8 runs   (  123.86 ms per token,     8.07 tokens per second)\n",
      "llama_perf_context_print:       total time =     992.68 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  30%|▎| 6/20 [00:08Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1074.19 ms /     9 runs   (  119.35 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1076.20 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  35%|▎| 7/20 [00:09Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1149.32 ms /    10 runs   (  114.93 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1151.59 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  40%|▍| 8/20 [00:10Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1115.29 ms /    10 runs   (  111.53 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1117.75 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  45%|▍| 9/20 [00:11Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1133.93 ms /    10 runs   (  113.39 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1136.30 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  50%|▌| 10/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1133.63 ms /    10 runs   (  113.36 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1135.82 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  55%|▌| 11/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1128.08 ms /    10 runs   (  112.81 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1130.40 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  60%|▌| 12/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     904.00 ms /     8 runs   (  113.00 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =     905.76 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  65%|▋| 13/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1129.27 ms /    10 runs   (  112.93 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1131.44 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  70%|▋| 14/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1127.06 ms /    10 runs   (  112.71 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1129.31 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  75%|▊| 15/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1113.04 ms /    10 runs   (  111.30 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1115.19 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  80%|▊| 16/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1130.91 ms /    10 runs   (  113.09 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1133.08 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  85%|▊| 17/20 [00:2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1125.31 ms /    10 runs   (  112.53 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1127.60 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  90%|▉| 18/20 [00:2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1123.51 ms /    10 runs   (  112.35 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1125.68 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  95%|▉| 19/20 [00:2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1128.08 ms /    10 runs   (  112.81 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1130.28 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:   0%| | 0/20 [00:00Llama.generate: 35 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     465.81 ms /    13 tokens (   35.83 ms per token,    27.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1174.73 ms /    10 runs   (  117.47 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1643.15 ms /    23 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:   5%| | 1/20 [00:01Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1276.90 ms /    11 runs   (  116.08 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1279.42 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  10%| | 2/20 [00:02Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1327.54 ms /    11 runs   (  120.69 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1330.21 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  15%|▏| 3/20 [00:04Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1313.07 ms /    11 runs   (  119.37 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1315.56 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  20%|▏| 4/20 [00:05Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1305.49 ms /    11 runs   (  118.68 ms per token,     8.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1308.18 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  25%|▎| 5/20 [00:06Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1217.86 ms /    10 runs   (  121.79 ms per token,     8.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1220.07 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  30%|▎| 6/20 [00:08Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1310.74 ms /    11 runs   (  119.16 ms per token,     8.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1313.41 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  35%|▎| 7/20 [00:09Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1181.80 ms /    10 runs   (  118.18 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1184.08 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  40%|▍| 8/20 [00:10Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1302.44 ms /    11 runs   (  118.40 ms per token,     8.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1304.83 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  45%|▍| 9/20 [00:11Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1327.74 ms /    11 runs   (  120.70 ms per token,     8.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1330.40 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  50%|▌| 10/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1192.00 ms /    10 runs   (  119.20 ms per token,     8.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1194.28 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  55%|▌| 11/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     965.24 ms /     8 runs   (  120.66 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:       total time =     967.10 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  60%|▌| 12/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1150.25 ms /    10 runs   (  115.03 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1152.57 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  65%|▋| 13/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1141.67 ms /    10 runs   (  114.17 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1144.15 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  70%|▋| 14/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1180.77 ms /    11 runs   (  107.34 ms per token,     9.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1183.23 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  75%|▊| 15/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1201.83 ms /    11 runs   (  109.26 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1204.17 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  80%|▊| 16/20 [00:2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1199.71 ms /    11 runs   (  109.06 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1202.25 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  85%|▊| 17/20 [00:2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1086.11 ms /    10 runs   (  108.61 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1088.32 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  90%|▉| 18/20 [00:2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1093.13 ms /    10 runs   (  109.31 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1095.35 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  95%|▉| 19/20 [00:2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1202.58 ms /    11 runs   (  109.33 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1205.01 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • doctors symptoms\n",
      " •  \n",
      " • physicians symptoms\n",
      " • doctors [patients]\n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • physicians symptoms\n",
      " •  \n",
      " • doctors symptoms\n",
      " • doctors patients\n",
      " •  \n",
      " • doctors patients\n",
      " •  \n",
      " • medical professionals symptoms\n",
      " • medical professionals patients\n",
      " • doctors patients\n",
      " • often consult medical books and use laboratory tests\n",
      " • examine [skin] [spots]\n",
      " • consult [textbooks]\n",
      " • consult [journals]\n",
      " • carefully [mask] [mask]\n",
      " • need [extensively] [research]\n",
      " • often consult [guides]\n",
      " • often [mask] [mask]\n",
      " • rely on [examinations]\n",
      " • use various [tools] and [methods]\n",
      " • must observe examine and inquire\n",
      " • examine [patients]\n",
      " • consult [journals] and [patients]\n",
      " • consult [textbooks]\n",
      " • typically use a combination of [clinical] examination [medical] history and [laboratory] tests\n",
      " • often [mask] [mask]\n",
      " • examine [clients] [thoroughly]\n",
      " • examine [patients]\n",
      " • use [technology] and [examination]\n",
      " • rely on [medical] [techniques]\n",
      " • symptoms and\n",
      " •  \n",
      " • patients and\n",
      " • patient information and\n",
      " • patient medical history and then\n",
      " • medical records and\n",
      " • symptoms and\n",
      " • patients and\n",
      " • patients and then\n",
      " • patient symptoms and\n",
      " • patient symptoms and\n",
      " • symptoms and\n",
      " •  \n",
      " • patients and\n",
      " • patients and\n",
      " • patients and\n",
      " • symptoms and\n",
      " • patients and\n",
      " • patient records and\n",
      " • symptoms and\n",
      " • patients diagnose\n",
      " • patients diagnose\n",
      " • symptoms diagnose\n",
      " • patients diagnose\n",
      " • symptoms diagnose\n",
      " • patients diagnose\n",
      " • symptoms diagnose\n",
      " • patients diagnose\n",
      " • symptoms diagnose\n",
      " • symptoms diagnose\n",
      " • patients diagnose\n",
      " • symptoms diagnose\n",
      " • patients diagnose\n",
      " • symptoms diagnose\n",
      " • symptoms diagnose\n",
      " • symptoms diagnose\n",
      " • symptoms diagnose\n",
      " • patients diagnose\n",
      " • patients diagnose\n",
      " • patients diagnose\n",
      " • symptoms illnesses\n",
      " • symptoms illnesses\n",
      " • symptoms illnesses\n",
      " • symptoms illnesses\n",
      " • symptoms illnesses\n",
      " • symptoms patients\n",
      " • patients illnesses\n",
      " • patients diseases\n",
      " • symptoms illnesses\n",
      " • patients illnesses\n",
      " • symptoms diseases\n",
      " • patients diseases\n",
      " • symptoms diseases\n",
      " • symptoms diseases\n",
      " • symptoms illnesses\n",
      " • symptoms illnesses\n",
      " • symptoms illnesses\n",
      " • symptoms patients\n",
      " • symptoms diseases\n",
      " • symptoms illnesses\n",
      "\n",
      "Total generated: 100\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "doctors         PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "assess          PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "to              PMI=   -inf   P(x)=0.250  P(y)=0.000  P(xy)=0.000\n",
      "diagnose        PMI=  0.000   P(x)=0.500  P(y)=1.000  P(xy)=0.500\n",
      "diseases        PMI= -0.263   P(x)=0.800  P(y)=0.300  P(xy)=0.200\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 3   |   Anchor word: 'to'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:   0%| | 0/20Llama.generate: 29 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     581.03 ms /    19 tokens (   30.58 ms per token,    32.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1083.37 ms /    10 runs   (  108.34 ms per token,     9.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1667.03 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:   5%| | 1/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['the', 'physician', 'assesses', 'symptoms', 'and', 'diagnoses', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1145.44 ms /    10 runs   (  114.54 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1147.70 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  10%| | 2/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1404.74 ms /    11 runs   (  127.70 ms per token,     7.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1407.21 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  15%|▏| 3/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['the', 'physician', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1280.40 ms /    10 runs   (  128.04 ms per token,     7.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1282.63 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  20%|▏| 4/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1184.49 ms /    10 runs   (  118.45 ms per token,     8.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1186.76 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  25%|▎| 5/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1275.09 ms /    10 runs   (  127.51 ms per token,     7.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1277.69 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  30%|▎| 6/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1255.40 ms /    10 runs   (  125.54 ms per token,     7.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1257.67 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  35%|▎| 7/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1213.56 ms /    10 runs   (  121.36 ms per token,     8.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1215.86 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  40%|▍| 8/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1261.21 ms /    10 runs   (  126.12 ms per token,     7.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1263.49 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  45%|▍| 9/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1297.36 ms /    11 runs   (  117.94 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1300.24 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  50%|▌| 10/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['the', 'nurse', 'assesses', 'symptoms', 'and', 'diagnoses', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1519.48 ms /    11 runs   (  138.13 ms per token,     7.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1522.07 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  55%|▌| 11/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'and', 'diagnoses', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1208.86 ms /    10 runs   (  120.89 ms per token,     8.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1211.09 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  60%|▌| 12/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1303.88 ms /    11 runs   (  118.53 ms per token,     8.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1306.29 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  65%|▋| 13/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'and', 'diagnoses', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1284.82 ms /    11 runs   (  116.80 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1287.16 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  70%|▋| 14/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['the', 'nurse', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1168.91 ms /    10 runs   (  116.89 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1171.09 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  75%|▊| 15/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1157.36 ms /    10 runs   (  115.74 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1159.56 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  80%|▊| 16/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1485.17 ms /    13 runs   (  114.24 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1488.00 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  85%|▊| 17/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['physicians', 'assess', 'symptoms', 'to', '[diagnose]', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1143.73 ms /    10 runs   (  114.37 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1145.91 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  90%|▉| 18/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1275.90 ms /    11 runs   (  115.99 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1278.57 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess symptoms [MASK] diagnose diseases:  95%|▉| 19/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1138.37 ms /    10 runs   (  113.84 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1140.75 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:   0%| | 0/2Llama.generate: 29 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     424.13 ms /    19 tokens (   22.32 ms per token,    44.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1094.39 ms /     9 runs   (  121.60 ms per token,     8.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1520.80 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:   5%| | 1/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     491.38 ms /     4 runs   (  122.84 ms per token,     8.14 tokens per second)\n",
      "llama_perf_context_print:       total time =     492.41 ms /     5 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  10%| | 2/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', '[mask]', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['doctors', 'notice', 'symptoms']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     979.32 ms /     8 runs   (  122.41 ms per token,     8.17 tokens per second)\n",
      "llama_perf_context_print:       total time =     981.23 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  15%|▏| 3/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2190.43 ms /    19 runs   (  115.29 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    2194.87 ms /    20 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  20%|▏| 4/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2064.45 ms /    17 runs   (  121.44 ms per token,     8.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    2068.31 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  25%|▎| 5/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1050.28 ms /     9 runs   (  116.70 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1052.35 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  30%|▎| 6/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     987.48 ms /     8 runs   (  123.43 ms per token,     8.10 tokens per second)\n",
      "llama_perf_context_print:       total time =     989.26 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  35%|▎| 7/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1695.45 ms /    14 runs   (  121.10 ms per token,     8.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1698.57 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  40%|▍| 8/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1551.19 ms /    14 runs   (  110.80 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1554.58 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  45%|▍| 9/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', '[mask]', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['doctors', 'often', '[mask]', 'symptoms', 'to', '[mask]', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1125.84 ms /    10 runs   (  112.58 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1128.37 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  50%|▌| 10/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     788.69 ms /     7 runs   (  112.67 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =     790.28 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  55%|▌| 11/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     895.12 ms /     8 runs   (  111.89 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =     896.93 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  60%|▌| 12/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     872.20 ms /     8 runs   (  109.02 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =     873.98 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  65%|▋| 13/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1114.74 ms /    10 runs   (  111.47 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1116.92 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  70%|▋| 14/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1798.28 ms /    16 runs   (  112.39 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1801.88 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  75%|▊| 15/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1115.94 ms /    10 runs   (  111.59 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1118.17 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  80%|▊| 16/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1123.26 ms /    10 runs   (  112.33 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1125.48 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  85%|▊| 17/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2273.19 ms /    20 runs   (  113.66 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    2277.58 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =         19\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  90%|▉| 18/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1176.84 ms /    11 runs   (  106.99 ms per token,     9.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1179.28 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms [MASK] diagnose diseases:  95%|▉| 19/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1817.31 ms /    16 runs   (  113.58 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1821.47 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:   0%| | 0/20 Llama.generate: 30 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     547.50 ms /    18 tokens (   30.42 ms per token,    32.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1141.15 ms /     9 runs   (  126.79 ms per token,     7.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1691.20 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:   5%| | 1/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1270.30 ms /    10 runs   (  127.03 ms per token,     7.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1272.71 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  10%| | 2/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1411.78 ms /    11 runs   (  128.34 ms per token,     7.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1414.23 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  15%|▏| 3/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1412.07 ms /    11 runs   (  128.37 ms per token,     7.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1414.57 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  20%|▏| 4/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1344.62 ms /    10 runs   (  134.46 ms per token,     7.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1346.82 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  25%|▎| 5/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1338.39 ms /    10 runs   (  133.84 ms per token,     7.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1340.57 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  30%|▎| 6/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1295.98 ms /    10 runs   (  129.60 ms per token,     7.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1298.17 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  35%|▎| 7/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1474.56 ms /    11 runs   (  134.05 ms per token,     7.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1477.56 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  40%|▍| 8/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1355.34 ms /    10 runs   (  135.53 ms per token,     7.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1358.39 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  45%|▍| 9/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1288.78 ms /    10 runs   (  128.88 ms per token,     7.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1291.11 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  50%|▌| 10/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1261.31 ms /    10 runs   (  126.13 ms per token,     7.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1263.60 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  55%|▌| 11/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1361.15 ms /    11 runs   (  123.74 ms per token,     8.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1363.80 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  60%|▌| 12/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     949.15 ms /     8 runs   (  118.64 ms per token,     8.43 tokens per second)\n",
      "llama_perf_context_print:       total time =     951.01 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  65%|▋| 13/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1227.27 ms /    10 runs   (  122.73 ms per token,     8.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1229.49 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  70%|▋| 14/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1359.52 ms /    11 runs   (  123.59 ms per token,     8.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1362.09 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  75%|▊| 15/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1445.57 ms /    10 runs   (  144.56 ms per token,     6.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1447.86 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  80%|▊| 16/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1191.96 ms /    10 runs   (  119.20 ms per token,     8.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1194.36 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  85%|▊| 17/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1174.96 ms /    10 runs   (  117.50 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1177.62 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  90%|▉| 18/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1195.08 ms /    10 runs   (  119.51 ms per token,     8.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1197.43 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] [MASK] diagnose diseases:  95%|▉| 19/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1142.18 ms /    10 runs   (  114.22 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1144.46 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:   0%| | 0/20 Llama.generate: 31 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     536.62 ms /    16 tokens (   33.54 ms per token,    29.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1957.38 ms /    15 runs   (  130.49 ms per token,     7.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    2498.50 ms /    31 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:   5%| | 1/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1948.92 ms /    17 runs   (  114.64 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1953.46 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  10%| | 2/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1738.06 ms /    14 runs   (  124.15 ms per token,     8.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1742.23 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  15%|▏| 3/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1496.06 ms /    13 runs   (  115.08 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1500.23 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  20%|▏| 4/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1825.01 ms /    15 runs   (  121.67 ms per token,     8.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1828.36 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  25%|▎| 5/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1889.43 ms /    16 runs   (  118.09 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1893.06 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  30%|▎| 6/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1331.56 ms /    11 runs   (  121.05 ms per token,     8.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1334.09 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  35%|▎| 7/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1911.23 ms /    16 runs   (  119.45 ms per token,     8.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1914.76 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  40%|▍| 8/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1885.25 ms /    16 runs   (  117.83 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1888.91 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  45%|▍| 9/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1663.75 ms /    14 runs   (  118.84 ms per token,     8.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1666.90 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  50%|▌| 10/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1472.25 ms /    13 runs   (  113.25 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1475.33 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  55%|▌| 11/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1592.34 ms /    14 runs   (  113.74 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1595.70 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  60%|▌| 12/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2957.15 ms /    27 runs   (  109.52 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    2963.24 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =         26\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  65%|▋| 13/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1243.28 ms /    11 runs   (  113.03 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1245.68 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  70%|▋| 14/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1140.30 ms /    10 runs   (  114.03 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1142.54 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  75%|▊| 15/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', 'assess', 'symptoms', '[mask]', '[mask]', 'diseases'] ['doctors', 'assess', 'symptoms', 'individually', 'for', 'each', 'disease']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1630.34 ms /    15 runs   (  108.69 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1633.69 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  80%|▊| 16/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1488.29 ms /    13 runs   (  114.48 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1491.43 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  85%|▊| 17/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1442.85 ms /    13 runs   (  110.99 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1445.85 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  90%|▉| 18/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1237.92 ms /    11 runs   (  112.54 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1240.45 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  95%|▉| 19/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1683.11 ms /    15 runs   (  112.21 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1686.34 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:   0%| | 0/20 Llama.generate: 35 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     501.55 ms /    13 tokens (   38.58 ms per token,    25.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =     834.43 ms /     8 runs   (  104.30 ms per token,     9.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1338.14 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:   5%| | 1/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1141.71 ms /    10 runs   (  114.17 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1144.32 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  10%| | 2/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', 'assess', 'symptoms', '[mask]', 'diagnose', '[mask]'] ['doctors', 'assess', 'symptoms', 'to', 'make', 'a', 'diagnosis']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1245.85 ms /    11 runs   (  113.26 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1248.32 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  15%|▏| 3/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', 'assess', 'symptoms', '[mask]', 'diagnose', '[mask]'] ['doctors', 'assess', 'symptoms', 'thoroughly', 'before', 'diagnosing', 'accurately']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1618.71 ms /    15 runs   (  107.91 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1622.04 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  20%|▏| 4/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', 'assess', 'symptoms', '[mask]', 'diagnose', '[mask]'] ['doctors', 'assess', 'symptoms', 'systematically', 'and', 'thoroughly', 'before', 'diagnosing', 'patients', 'accurately']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1386.65 ms /    13 runs   (  106.67 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1389.54 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  25%|▎| 5/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1021.37 ms /     9 runs   (  113.49 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1023.48 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  30%|▎| 6/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1222.75 ms /    11 runs   (  111.16 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1225.20 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  35%|▎| 7/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1506.48 ms /    14 runs   (  107.61 ms per token,     9.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1509.60 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  40%|▍| 8/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1216.07 ms /    11 runs   (  110.55 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1218.52 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  45%|▍| 9/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', 'assess', 'symptoms', '[mask]', 'diagnose', '[mask]'] ['doctors', 'assess', 'symptoms', 'to', 'make', 'an', 'accurate', 'diagnosis']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1106.85 ms /    10 runs   (  110.68 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1109.08 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  50%|▌| 10/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1023.96 ms /     9 runs   (  113.77 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1025.99 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  55%|▌| 11/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1357.15 ms /    12 runs   (  113.10 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1359.83 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  60%|▌| 12/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1017.35 ms /     9 runs   (  113.04 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1019.57 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  65%|▋| 13/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1173.17 ms /    11 runs   (  106.65 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1175.69 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  70%|▋| 14/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1185.08 ms /    11 runs   (  107.73 ms per token,     9.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1187.55 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  75%|▊| 15/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1110.98 ms /    10 runs   (  111.10 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1113.37 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  80%|▊| 16/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1007.27 ms /     9 runs   (  111.92 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1009.33 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  85%|▊| 17/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1613.59 ms /    12 runs   (  134.47 ms per token,     7.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1616.52 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  90%|▉| 18/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1156.34 ms /    10 runs   (  115.63 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1158.67 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  95%|▉| 19/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1145.92 ms /    10 runs   (  114.59 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1148.23 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " •  \n",
      " • physicians to\n",
      " •  \n",
      " • physicians to\n",
      " • medical professionals to\n",
      " • medical professionals to\n",
      " • medical professionals and\n",
      " • medical professionals to\n",
      " • medical professionals and\n",
      " •  \n",
      " •  \n",
      " • medical professionals and\n",
      " •  \n",
      " •  \n",
      " • physicians to\n",
      " • doctors and\n",
      " •  \n",
      " • physicians to\n",
      " • healthcare professionals and\n",
      " • doctors to\n",
      " • observe to\n",
      " •  \n",
      " • observe they\n",
      " • should examine [mask] in order to [mask]\n",
      " • carefully [mask] to [mask]\n",
      " • detect that help\n",
      " • observe to\n",
      " • carefully [mask] [mask] accurately\n",
      " •  \n",
      " • observe and\n",
      " • commonly \n",
      " • observe and\n",
      " • identify they\n",
      " • often that they can\n",
      " • usually [mask] to [mask]\n",
      " • observe to\n",
      " • observe to\n",
      " • carefully [observe] in order to [accurately]\n",
      " • usually observe to\n",
      " • often [mask] to [mask]\n",
      " • symptoms and\n",
      " • patients and\n",
      " • patient records and\n",
      " • patient data and\n",
      " • patients and\n",
      " • patients and\n",
      " • symptoms and\n",
      " • physical symptoms and\n",
      " • symptoms and\n",
      " • patients and\n",
      " • symptoms and\n",
      " • physical symptoms and\n",
      " • patients and\n",
      " • symptoms and\n",
      " • patient symptoms and\n",
      " • symptoms and\n",
      " • patients and\n",
      " • symptoms and\n",
      " • patients and\n",
      " • patients and\n",
      " • thoroughly and accurately making diagnoses of various\n",
      " • systematically in order to rule out [mask]\n",
      " • carefully and accurately in order to diagnose various\n",
      " • thoroughly and accurately to diagnose\n",
      " • in order to rule out [mask]\n",
      " • accurately and efficiently to rule out or confirm various\n",
      " • individually and diagnose\n",
      " • carefully and accurately to rule out [mask]\n",
      " • carefully and accurately to rule out [mask]\n",
      " • clinically and accurately to diagnose\n",
      " • thoroughly and accurately in order to diagnose\n",
      " • individually and accurately to diagnose specific\n",
      " • in detail considering the context of [preexisting] and [underlying]\n",
      " • individually and differentiate\n",
      " •  \n",
      " • closely and accurately in order to diagnose\n",
      " • thoroughly and accurately across a wide range of\n",
      " • clinically and diagnose various\n",
      " • individually for each specific\n",
      " • in [context] of [relation]\n",
      " • carefully and accurately\n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • carefully and thoroughly to accurately\n",
      " • carefully and accurately\n",
      " • carefully and accurately\n",
      " • thoroughly and carefully in order to accurately\n",
      " •  \n",
      " • and then \n",
      " • thoroughly and accurately\n",
      " • to accurately various conditions\n",
      " • carefully accurately\n",
      " • then patients\n",
      " • carefully and accurately\n",
      " • carefully and then accurately\n",
      " • carefully and accurately\n",
      " • thoroughly and then accurately\n",
      " • comprehensively and accurately\n",
      " • to conditions\n",
      "\n",
      "Total generated: 100\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "doctors         PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "assess          PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "symptoms        PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "diagnose        PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "diseases        PMI=   -inf   P(x)=0.050  P(y)=0.000  P(xy)=0.000\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 4   |   Anchor word: 'diagnose'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:   0%| | 0/20 [00:0Llama.generate: 29 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     494.00 ms /    18 tokens (   27.44 ms per token,    36.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1021.91 ms /     9 runs   (  113.55 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1518.58 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:   5%| | 1/20 [00:0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1136.10 ms /    10 runs   (  113.61 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1138.51 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  10%| | 2/20 [00:0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1144.55 ms /    10 runs   (  114.45 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1146.84 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  15%|▏| 3/20 [00:0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1207.57 ms /    11 runs   (  109.78 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1210.00 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  20%|▏| 4/20 [00:0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', '[mask]', 'diseases'] ['the', 'physician', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1120.84 ms /    10 runs   (  112.08 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1123.16 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  25%|▎| 5/20 [00:0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1206.42 ms /    10 runs   (  120.64 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1208.99 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  30%|▎| 6/20 [00:0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1225.36 ms /    10 runs   (  122.54 ms per token,     8.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1227.73 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  35%|▎| 7/20 [00:0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1261.67 ms /    10 runs   (  126.17 ms per token,     7.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1263.99 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  40%|▍| 8/20 [00:0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1271.38 ms /    10 runs   (  127.14 ms per token,     7.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1273.89 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  45%|▍| 9/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1228.92 ms /    10 runs   (  122.89 ms per token,     8.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1231.16 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  50%|▌| 10/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1290.01 ms /    11 runs   (  117.27 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1292.57 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  55%|▌| 11/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', '[mask]', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1060.69 ms /     9 runs   (  117.85 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1062.67 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  60%|▌| 12/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1195.25 ms /    10 runs   (  119.53 ms per token,     8.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1197.56 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  65%|▋| 13/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1298.16 ms /    11 runs   (  118.01 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1300.87 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  70%|▋| 14/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', '[mask]', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1297.54 ms /    11 runs   (  117.96 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1300.27 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  75%|▊| 15/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', '[mask]', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1186.49 ms /    10 runs   (  118.65 ms per token,     8.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1188.78 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  80%|▊| 16/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1052.61 ms /     9 runs   (  116.96 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1054.92 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  85%|▊| 17/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1295.23 ms /    11 runs   (  117.75 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1297.67 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  90%|▉| 18/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', '[mask]', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1183.43 ms /    10 runs   (  118.34 ms per token,     8.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1185.66 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to [MASK] diseases:  95%|▉| 19/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1175.80 ms /    10 runs   (  117.58 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1178.06 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:   0%| | 0/20 [00:Llama.generate: 29 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     414.04 ms /    18 tokens (   23.00 ms per token,    43.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =     847.41 ms /     7 runs   (  121.06 ms per token,     8.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1263.38 ms /    25 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:   5%| | 1/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', '[mask]', 'symptoms', 'to', '[mask]', 'diseases'] ['doctors', 'associate', 'symptoms', 'with', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1983.82 ms /    17 runs   (  116.70 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1987.70 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  10%| | 2/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', '[mask]', 'symptoms', 'to', '[mask]', 'diseases'] ['doctors', 'identify', '[symptoms]', 'to', '[diseases]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     948.21 ms /     8 runs   (  118.53 ms per token,     8.44 tokens per second)\n",
      "llama_perf_context_print:       total time =     950.11 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  15%|▏| 3/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     943.24 ms /     8 runs   (  117.90 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =     945.14 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  20%|▏| 4/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     933.47 ms /     8 runs   (  116.68 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =     935.37 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  25%|▎| 5/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', '[mask]', 'symptoms', 'to', '[mask]', 'diseases'] ['doctors', 'associate', 'symptoms', 'with', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     937.36 ms /     8 runs   (  117.17 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =     939.24 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  30%|▎| 6/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', '[mask]', 'symptoms', 'to', '[mask]', 'diseases'] ['doctors', 'associate', 'symptoms', 'with', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1521.00 ms /    13 runs   (  117.00 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1523.96 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  35%|▎| 7/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1521.72 ms /    13 runs   (  117.06 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1524.69 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  40%|▍| 8/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     925.46 ms /     8 runs   (  115.68 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =     927.35 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  45%|▍| 9/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     924.76 ms /     8 runs   (  115.60 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =     926.65 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  50%|▌| 10/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1180.58 ms /    10 runs   (  118.06 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1182.89 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  55%|▌| 11/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     926.28 ms /     8 runs   (  115.79 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =     928.16 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  60%|▌| 12/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     702.70 ms /     6 runs   (  117.12 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =     704.14 ms /     7 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  65%|▋| 13/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1961.74 ms /    17 runs   (  115.40 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1965.65 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  70%|▋| 14/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', '[mask]', 'symptoms', 'to', '[mask]', 'diseases'] ['doctors', 'identify', '[symptoms]', 'to', '[diagnose]', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2011.06 ms /    17 runs   (  118.30 ms per token,     8.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    2015.23 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  75%|▊| 15/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', '[mask]', 'symptoms', 'to', '[mask]', 'diseases'] ['doctors', 'associate', '[symptoms]', 'with', '[diseases]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1972.16 ms /    17 runs   (  116.01 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1976.37 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  80%|▊| 16/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', '[mask]', 'symptoms', 'to', '[mask]', 'diseases'] ['doctors', 'observe', '[symptoms]', 'to', '[diagnose]', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     930.23 ms /     8 runs   (  116.28 ms per token,     8.60 tokens per second)\n",
      "llama_perf_context_print:       total time =     932.27 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  85%|▊| 17/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     817.57 ms /     7 runs   (  116.80 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =     819.54 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  90%|▉| 18/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1606.71 ms /    14 runs   (  114.77 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1610.20 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: doctors [MASK] symptoms to [MASK] diseases:  95%|▉| 19/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1896.77 ms /    17 runs   (  111.57 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1900.65 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:   0%| | 0/20 [00:00Llama.generate: 30 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     488.89 ms /    17 tokens (   28.76 ms per token,    34.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1026.92 ms /     9 runs   (  114.10 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1518.24 ms /    26 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:   5%| | 1/20 [00:01Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1154.32 ms /    10 runs   (  115.43 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1156.61 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  10%| | 2/20 [00:02Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1149.85 ms /    10 runs   (  114.98 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1152.40 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  15%|▏| 3/20 [00:03Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1347.98 ms /    10 runs   (  134.80 ms per token,     7.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1350.62 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  20%|▏| 4/20 [00:05Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1106.44 ms /    10 runs   (  110.64 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1108.73 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  25%|▎| 5/20 [00:06Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1114.26 ms /    10 runs   (  111.43 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1116.59 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  30%|▎| 6/20 [00:07Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1036.14 ms /     9 runs   (  115.13 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1038.42 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  35%|▎| 7/20 [00:08Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1237.39 ms /    10 runs   (  123.74 ms per token,     8.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1239.77 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  40%|▍| 8/20 [00:09Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1196.32 ms /    10 runs   (  119.63 ms per token,     8.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1198.66 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  45%|▍| 9/20 [00:10Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1132.69 ms /    10 runs   (  113.27 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1134.99 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  50%|▌| 10/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     997.93 ms /     9 runs   (  110.88 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1000.02 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  55%|▌| 11/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1124.42 ms /    10 runs   (  112.44 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1126.71 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  60%|▌| 12/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1148.59 ms /    10 runs   (  114.86 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1150.96 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  65%|▋| 13/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1169.26 ms /    10 runs   (  116.93 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1171.64 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  70%|▋| 14/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     946.56 ms /     8 runs   (  118.32 ms per token,     8.45 tokens per second)\n",
      "llama_perf_context_print:       total time =     948.56 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  75%|▊| 15/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     916.34 ms /     8 runs   (  114.54 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =     918.25 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  80%|▊| 16/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1176.89 ms /    10 runs   (  117.69 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1179.63 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  85%|▊| 17/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1173.34 ms /    10 runs   (  117.33 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1175.87 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  90%|▉| 18/20 [00:2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1143.51 ms /    10 runs   (  114.35 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1146.13 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to [MASK] diseases:  95%|▉| 19/20 [00:2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1106.38 ms /    10 runs   (  110.64 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1108.69 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:   0%| | 0/20 Llama.generate: 31 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     434.40 ms /    16 tokens (   27.15 ms per token,    36.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1940.46 ms /    15 runs   (  129.36 ms per token,     7.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    2378.78 ms /    31 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:   5%| | 1/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1960.99 ms /    15 runs   (  130.73 ms per token,     7.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1964.72 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  10%| | 2/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1680.89 ms /    13 runs   (  129.30 ms per token,     7.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1684.14 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  15%|▏| 3/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1725.71 ms /    14 runs   (  123.26 ms per token,     8.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1729.05 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  20%|▏| 4/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2360.77 ms /    18 runs   (  131.15 ms per token,     7.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    2365.41 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  25%|▎| 5/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1904.91 ms /    15 runs   (  126.99 ms per token,     7.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1908.56 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  30%|▎| 6/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2027.82 ms /    15 runs   (  135.19 ms per token,     7.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    2031.49 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  35%|▎| 7/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1583.91 ms /    13 runs   (  121.84 ms per token,     8.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1588.46 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  40%|▍| 8/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1917.38 ms /    16 runs   (  119.84 ms per token,     8.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1921.24 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  45%|▍| 9/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2339.47 ms /    20 runs   (  116.97 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    2343.91 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =         19\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  50%|▌| 10/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1646.40 ms /    14 runs   (  117.60 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1649.37 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  55%|▌| 11/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1618.80 ms /    14 runs   (  115.63 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1621.81 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  60%|▌| 12/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1881.49 ms /    17 runs   (  110.68 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1885.33 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  65%|▋| 13/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1865.96 ms /    17 runs   (  109.76 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1869.78 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  70%|▋| 14/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1103.68 ms /    10 runs   (  110.37 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1106.04 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  75%|▊| 15/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', 'assess', 'symptoms', '[mask]', '[mask]', 'diseases'] ['doctors', 'assess', 'symptoms', 'individually', 'for', 'each', 'disease']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1915.85 ms /    17 runs   (  112.70 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1919.83 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  80%|▊| 16/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1642.88 ms /    15 runs   (  109.53 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1646.08 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  85%|▊| 17/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1451.58 ms /    13 runs   (  111.66 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1454.54 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  90%|▉| 18/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1786.50 ms /    16 runs   (  111.66 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1789.93 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: doctors assess symptoms [MASK] [MASK] diseases:  95%|▉| 19/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1227.23 ms /    11 runs   (  111.57 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1229.60 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:   0%| | 0/20 [00:00Llama.generate: 32 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     416.60 ms /    15 tokens (   27.77 ms per token,    36.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1000.20 ms /     7 runs   (  142.89 ms per token,     7.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1419.13 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:   5%| | 1/20 [00:01Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1320.15 ms /    10 runs   (  132.01 ms per token,     7.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1322.62 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  10%| | 2/20 [00:02Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1072.06 ms /     8 runs   (  134.01 ms per token,     7.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1073.86 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  15%|▏| 3/20 [00:03Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1144.76 ms /     9 runs   (  127.20 ms per token,     7.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1146.74 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  20%|▏| 4/20 [00:04Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1146.44 ms /     9 runs   (  127.38 ms per token,     7.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1148.42 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  25%|▎| 5/20 [00:06Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     968.09 ms /     8 runs   (  121.01 ms per token,     8.26 tokens per second)\n",
      "llama_perf_context_print:       total time =     969.89 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  30%|▎| 6/20 [00:07Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1105.50 ms /     9 runs   (  122.83 ms per token,     8.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1107.58 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  35%|▎| 7/20 [00:08Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1023.96 ms /     8 runs   (  128.00 ms per token,     7.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1026.37 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  40%|▍| 8/20 [00:09Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1306.89 ms /    11 runs   (  118.81 ms per token,     8.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1309.41 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  45%|▍| 9/20 [00:10Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     938.60 ms /     8 runs   (  117.32 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =     940.36 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  50%|▌| 10/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1083.88 ms /     9 runs   (  120.43 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1085.83 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  55%|▌| 11/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     943.91 ms /     8 runs   (  117.99 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =     945.73 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  60%|▌| 12/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1343.00 ms /    11 runs   (  122.09 ms per token,     8.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1345.43 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  65%|▋| 13/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1050.63 ms /     9 runs   (  116.74 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1052.63 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  70%|▋| 14/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1049.42 ms /     9 runs   (  116.60 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1051.41 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  75%|▊| 15/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1084.60 ms /     9 runs   (  120.51 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1086.60 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  80%|▊| 16/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1131.17 ms /    10 runs   (  113.12 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1133.46 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  85%|▊| 17/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     943.27 ms /     8 runs   (  117.91 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =     945.15 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  90%|▉| 18/20 [00:2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     823.54 ms /     7 runs   (  117.65 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =     825.16 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  95%|▉| 19/20 [00:2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1155.56 ms /    10 runs   (  115.56 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1157.76 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • physicians diagnose\n",
      " • doctors diagnose\n",
      " • medical professionals diagnose\n",
      " •  \n",
      " • physicians diagnose\n",
      " • physicians diagnose\n",
      " • doctors diagnose\n",
      " • physicians diagnose\n",
      " • doctors diagnose\n",
      " • medics diagnose\n",
      " •  \n",
      " • doctors diagnose\n",
      " • physicians diagnose\n",
      " •  \n",
      " •  \n",
      " • physicians diagnose\n",
      " • physicians diagnose\n",
      " •  \n",
      " • physicians diagnose\n",
      " • doctors diagnose\n",
      " •  \n",
      " •  \n",
      " • identify diagnose\n",
      " • identify diagnose\n",
      " •  \n",
      " •  \n",
      " • identify [mask] [mask]\n",
      " • identify [mask] [mask]\n",
      " • identify diagnose\n",
      " • identify diagnose\n",
      " • identify diagnose\n",
      " • identify diagnose\n",
      " • attribute \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • identify diagnose\n",
      " • diagnose \n",
      " • identify [mask] [mask]\n",
      " • [diagnose] [identify]\n",
      " • patients diagnose\n",
      " • symptoms diagnose\n",
      " • symptoms diagnose\n",
      " • patients diagnose\n",
      " • symptoms diagnose\n",
      " • symptoms diagnose\n",
      " • symptoms identify\n",
      " • symptoms diagnose\n",
      " • patients diagnose\n",
      " • symptoms diagnose\n",
      " • symptoms diagnose\n",
      " • patients diagnose\n",
      " • symptoms diagnose\n",
      " • symptoms diagnose\n",
      " • patients diagnose\n",
      " • symptoms diagnose\n",
      " • symptoms diagnose\n",
      " • patients diagnose\n",
      " • patients diagnose\n",
      " • patients diagnose\n",
      " • accurately and promptly in order to diagnose\n",
      " • thoroughly and accurately in order to diagnose\n",
      " • carefully and accurately in order to diagnose\n",
      " • closely in order to diagnose\n",
      " • individually for each patient and distinguish them from [mask]\n",
      " • individually to accurately diagnose and treat specific\n",
      " • individually in order to accurately diagnose specific\n",
      " • carefully and accurately to diagnose\n",
      " • individually and determine the likelihood of [mask]\n",
      " • in [patients] to identify and manage [medical]\n",
      " • carefully and accurately diagnosing various\n",
      " • carefully and accurately to diagnose various\n",
      " • carefully and accurately to rule out or diagnose various\n",
      " • clinically and accurately to diagnose [mask]\n",
      " •  \n",
      " • clinically in order to diagnose and treat various\n",
      " • systematically to rule out [mask]\n",
      " • systematically to diagnose various\n",
      " • carefully and accurately to rule out [mask]\n",
      " • at early stages of\n",
      " • diagnose diseases\n",
      " • diagnose patients\n",
      " • diagnose conditions\n",
      " • diagnose and treat\n",
      " • diagnose and treat\n",
      " • diagnose conditions\n",
      " • diagnose conditions accurately\n",
      " • diagnose patients\n",
      " • diagnose patients accurately\n",
      " • diagnose conditions\n",
      " • diagnose conditions accurately\n",
      " • determine diagnoses\n",
      " • diagnose and treat\n",
      " • diagnose and treat\n",
      " • diagnose illnesses\n",
      " • diagnose and treat\n",
      " • diagnose patients\n",
      " • diagnose patients\n",
      " • identify conditions\n",
      " • diagnose conditions\n",
      "\n",
      "Total generated: 100\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "doctors         PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "assess          PMI=   -inf   P(x)=0.050  P(y)=0.000  P(xy)=0.000\n",
      "symptoms        PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "to              PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "diseases        PMI=  0.152   P(x)=0.900  P(y)=0.050  P(xy)=0.050\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 5   |   Anchor word: 'diseases'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:   0%| | 0/20 [00:0Llama.generate: 29 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     540.30 ms /    19 tokens (   28.44 ms per token,    35.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1150.35 ms /     9 runs   (  127.82 ms per token,     7.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1693.20 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:   5%| | 1/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1532.86 ms /    12 runs   (  127.74 ms per token,     7.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1535.67 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  10%| | 2/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'a', 'condition']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1466.63 ms /    12 runs   (  122.22 ms per token,     8.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1469.26 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  15%|▏| 3/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'the', 'disease']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1321.99 ms /    11 runs   (  120.18 ms per token,     8.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1324.33 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  20%|▏| 4/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1351.72 ms /    11 runs   (  122.88 ms per token,     8.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1354.13 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  25%|▎| 5/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1209.25 ms /    10 runs   (  120.92 ms per token,     8.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1211.44 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  30%|▎| 6/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1236.14 ms /    10 runs   (  123.61 ms per token,     8.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1238.35 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  35%|▎| 7/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1238.56 ms /    10 runs   (  123.86 ms per token,     8.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1240.75 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  40%|▍| 8/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1440.33 ms /    12 runs   (  120.03 ms per token,     8.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1443.21 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  45%|▍| 9/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'the', 'illness']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1417.31 ms /    12 runs   (  118.11 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1420.20 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  50%|▌| 10/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'a', 'patient']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1457.88 ms /    12 runs   (  121.49 ms per token,     8.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1460.52 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  55%|▌| 11/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'physician', 'assesses', 'symptoms', 'to', 'diagnose', 'the', 'patient']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1405.45 ms /    12 runs   (  117.12 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1408.06 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  60%|▌| 12/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'physician', 'assesses', 'symptoms', 'to', 'diagnose', 'the', 'condition']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1291.91 ms /    11 runs   (  117.45 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1294.29 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  65%|▋| 13/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1210.63 ms /    10 runs   (  121.06 ms per token,     8.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1212.86 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  70%|▋| 14/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1154.42 ms /    10 runs   (  115.44 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1156.60 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  75%|▊| 15/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1379.16 ms /    12 runs   (  114.93 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1381.77 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  80%|▊| 16/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'the', 'illness']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1537.43 ms /    13 runs   (  118.26 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1540.31 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  85%|▊| 17/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'physician', 'assesses', 'symptoms', 'to', 'diagnose', 'pneumonia']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1266.98 ms /    11 runs   (  115.18 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1269.44 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  90%|▉| 18/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'physician', 'assesses', 'symptoms', 'to', 'diagnose', 'illness']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1286.66 ms /    11 runs   (  116.97 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1289.16 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] assess symptoms to diagnose [MASK]:  95%|▉| 19/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1404.10 ms /    12 runs   (  117.01 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1406.72 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'a', 'disease']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:   0%| | 0/20 [00:Llama.generate: 29 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     523.90 ms /    19 tokens (   27.57 ms per token,    36.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1683.71 ms /    12 runs   (  140.31 ms per token,     7.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    2210.82 ms /    31 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:   5%| | 1/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1377.51 ms /    10 runs   (  137.75 ms per token,     7.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1380.00 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  10%| | 2/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     957.09 ms /     7 runs   (  136.73 ms per token,     7.31 tokens per second)\n",
      "llama_perf_context_print:       total time =     958.97 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  15%|▏| 3/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1346.04 ms /    10 runs   (  134.60 ms per token,     7.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1348.46 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  20%|▏| 4/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1108.11 ms /     8 runs   (  138.51 ms per token,     7.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1109.88 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  25%|▎| 5/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2616.23 ms /    19 runs   (  137.70 ms per token,     7.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    2620.34 ms /    20 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  30%|▎| 6/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1521.31 ms /    11 runs   (  138.30 ms per token,     7.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1523.99 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  35%|▎| 7/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1341.87 ms /    10 runs   (  134.19 ms per token,     7.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1344.38 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  40%|▍| 8/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1015.26 ms /     8 runs   (  126.91 ms per token,     7.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1017.08 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  45%|▍| 9/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1157.89 ms /     9 runs   (  128.66 ms per token,     7.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1159.90 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  50%|▌| 10/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1219.65 ms /    10 runs   (  121.97 ms per token,     8.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1221.96 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  55%|▌| 11/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     944.32 ms /     8 runs   (  118.04 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =     946.14 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  60%|▌| 12/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1181.83 ms /    10 runs   (  118.18 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1184.01 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  65%|▋| 13/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1209.44 ms /    11 runs   (  109.95 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1211.87 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  70%|▋| 14/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     982.21 ms /     9 runs   (  109.13 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =     984.21 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  75%|▊| 15/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1193.58 ms /    11 runs   (  108.51 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1195.93 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  80%|▊| 16/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1056.20 ms /    10 runs   (  105.62 ms per token,     9.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1058.61 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  85%|▊| 17/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     916.99 ms /     8 runs   (  114.62 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =     919.10 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  90%|▉| 18/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1066.40 ms /    10 runs   (  106.64 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1068.62 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors [MASK] symptoms to diagnose [MASK]:  95%|▉| 19/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     895.94 ms /     8 runs   (  111.99 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =     897.73 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:   0%| | 0/20 [00:00Llama.generate: 30 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     478.80 ms /    18 tokens (   26.60 ms per token,    37.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1022.22 ms /     7 runs   (  146.03 ms per token,     6.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1503.80 ms /    25 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:   5%| | 1/20 [00:01Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1373.23 ms /    11 runs   (  124.84 ms per token,     8.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1375.81 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  10%| | 2/20 [00:02Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1353.47 ms /    11 runs   (  123.04 ms per token,     8.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1355.93 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  15%|▏| 3/20 [00:04Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1300.53 ms /    10 runs   (  130.05 ms per token,     7.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1302.76 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  20%|▏| 4/20 [00:05Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1407.47 ms /    11 runs   (  127.95 ms per token,     7.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1410.12 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  25%|▎| 5/20 [00:06Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1236.79 ms /    10 runs   (  123.68 ms per token,     8.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1239.37 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  30%|▎| 6/20 [00:08Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1609.11 ms /    13 runs   (  123.78 ms per token,     8.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1612.06 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  35%|▎| 7/20 [00:09Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1395.95 ms /    11 runs   (  126.90 ms per token,     7.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1398.40 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  40%|▍| 8/20 [00:11Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1422.64 ms /    11 runs   (  129.33 ms per token,     7.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1425.12 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  45%|▍| 9/20 [00:12Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1441.91 ms /    11 runs   (  131.08 ms per token,     7.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1444.41 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  50%|▌| 10/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1362.96 ms /    11 runs   (  123.91 ms per token,     8.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1365.52 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  55%|▌| 11/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1372.78 ms /    11 runs   (  124.80 ms per token,     8.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1375.31 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  60%|▌| 12/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1346.81 ms /    11 runs   (  122.44 ms per token,     8.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1349.41 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  65%|▋| 13/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1286.01 ms /    11 runs   (  116.91 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1289.05 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  70%|▋| 14/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1285.45 ms /    11 runs   (  116.86 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1288.48 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  75%|▊| 15/20 [00:2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1293.55 ms /    11 runs   (  117.60 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1296.15 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  80%|▊| 16/20 [00:2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1258.96 ms /    11 runs   (  114.45 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1261.47 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  85%|▊| 17/20 [00:2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1276.77 ms /    11 runs   (  116.07 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1279.30 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  90%|▉| 18/20 [00:2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1178.25 ms /    10 runs   (  117.83 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1180.46 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess [MASK] to diagnose [MASK]:  95%|▉| 19/20 [00:2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     934.77 ms /     8 runs   (  116.85 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =     936.56 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:   0%| | 0/20 Llama.generate: 31 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     537.61 ms /    17 tokens (   31.62 ms per token,    31.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =     812.55 ms /     7 runs   (  116.08 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1352.05 ms /    24 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:   5%| | 1/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', 'assess', 'symptoms', '[mask]', 'diagnose', '[mask]'] ['doctors', 'assess', 'symptoms', 'carefully', 'before', 'diagnosing']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1276.43 ms /    11 runs   (  116.04 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1278.86 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  10%| | 2/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1188.29 ms /    10 runs   (  118.83 ms per token,     8.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1190.47 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  15%|▏| 3/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     913.50 ms /     8 runs   (  114.19 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =     915.27 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  20%|▏| 4/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1064.80 ms /     9 runs   (  118.31 ms per token,     8.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1066.76 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  25%|▎| 5/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1187.99 ms /    10 runs   (  118.80 ms per token,     8.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1190.17 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  30%|▎| 6/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1519.44 ms /    13 runs   (  116.88 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1522.24 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  35%|▎| 7/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     936.91 ms /     8 runs   (  117.11 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =     938.73 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  40%|▍| 8/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1044.19 ms /     9 runs   (  116.02 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1046.45 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  45%|▍| 9/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1521.43 ms /    13 runs   (  117.03 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1524.54 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  50%|▌| 10/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1025.67 ms /     9 runs   (  113.96 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1027.64 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  55%|▌| 11/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1061.99 ms /     9 runs   (  118.00 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1063.96 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  60%|▌| 12/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1059.29 ms /     9 runs   (  117.70 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1061.26 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  65%|▋| 13/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1283.37 ms /    11 runs   (  116.67 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1285.76 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  70%|▋| 14/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1168.85 ms /    10 runs   (  116.88 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1171.27 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  75%|▊| 15/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1168.81 ms /    10 runs   (  116.88 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1171.27 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  80%|▊| 16/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     703.92 ms /     6 runs   (  117.32 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =     705.30 ms /     7 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  85%|▊| 17/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['doctors', 'assess', 'symptoms', '[mask]', 'diagnose', '[mask]'] ['doctors', 'assess', 'symptoms', 'and', 'determine']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1150.00 ms /    10 runs   (  115.00 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1152.26 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  90%|▉| 18/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1282.64 ms /    11 runs   (  116.60 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1284.97 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess symptoms [MASK] diagnose [MASK]:  95%|▉| 19/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1159.94 ms /    10 runs   (  115.99 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1162.15 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:   0%| | 0/20 [00:00Llama.generate: 32 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     697.03 ms /    15 tokens (   46.47 ms per token,    21.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =     792.12 ms /     7 runs   (  113.16 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1490.99 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:   5%| | 1/20 [00:01Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     824.65 ms /     7 runs   (  117.81 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =     826.43 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  10%| | 2/20 [00:02Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1024.28 ms /     9 runs   (  113.81 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1026.33 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  15%|▏| 3/20 [00:03Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1281.63 ms /    11 runs   (  116.51 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1284.02 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  20%|▏| 4/20 [00:04Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1233.19 ms /    11 runs   (  112.11 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1235.73 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  25%|▎| 5/20 [00:05Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     805.81 ms /     7 runs   (  115.12 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =     807.54 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  30%|▎| 6/20 [00:06Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     898.33 ms /     8 runs   (  112.29 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =     900.11 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  35%|▎| 7/20 [00:07Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     899.57 ms /     8 runs   (  112.45 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =     901.35 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  40%|▍| 8/20 [00:08Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     903.62 ms /     8 runs   (  112.95 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =     905.36 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  45%|▍| 9/20 [00:09Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     914.63 ms /     8 runs   (  114.33 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =     916.37 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  50%|▌| 10/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     908.02 ms /     8 runs   (  113.50 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =     909.75 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  55%|▌| 11/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1379.82 ms /    12 runs   (  114.98 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1382.42 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  60%|▌| 12/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     912.75 ms /     8 runs   (  114.09 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =     914.50 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  65%|▋| 13/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1158.25 ms /    10 runs   (  115.82 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1160.62 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  70%|▋| 14/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1264.34 ms /    11 runs   (  114.94 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1266.86 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  75%|▊| 15/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1476.37 ms /    13 runs   (  113.57 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1479.16 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  80%|▊| 16/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1336.92 ms /    12 runs   (  111.41 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1339.66 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  85%|▊| 17/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1032.95 ms /     9 runs   (  114.77 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1034.90 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  90%|▉| 18/20 [00:1Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     901.45 ms /     8 runs   (  112.68 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =     903.18 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: doctors assess symptoms to [MASK] [MASK]:  95%|▉| 19/20 [00:2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1028.46 ms /     9 runs   (  114.27 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1030.40 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • doctors diseases\n",
      " •  \n",
      " •  \n",
      " • doctors illnesses\n",
      " • veterinarians diseases\n",
      " • physicians diseases\n",
      " • doctors diseases\n",
      " • doctors diseases\n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • veterinarians diseases\n",
      " • doctors diseases\n",
      " • patients diseases\n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • medical professionals illnesses\n",
      " •  \n",
      " • carefully observe ailments\n",
      " • carefully observe illnesses\n",
      " • examine \n",
      " • examine conditions\n",
      " • analyze diseases\n",
      " • use a [process] to diagnose [illnesses]\n",
      " • analyze illnesses\n",
      " • assess diseases\n",
      " • observe diseases\n",
      " • carefully observe conditions\n",
      " • assess patients\n",
      " • examine conditions\n",
      " • observe diseases\n",
      " • observe illnesses\n",
      " • identify illnesses\n",
      " • carefully analyze conditions\n",
      " • observe conditions\n",
      " • observe patients\n",
      " • examine diseases\n",
      " • identify diseases\n",
      " • patients diseases\n",
      " • symptoms illnesses\n",
      " • patients illnesses\n",
      " • symptoms diseases\n",
      " • symptoms illnesses\n",
      " • symptoms diseases\n",
      " • patients medical records symptoms\n",
      " • symptoms illnesses\n",
      " • symptoms illnesses\n",
      " • symptoms illnesses\n",
      " • symptoms illnesses\n",
      " • symptoms illnesses\n",
      " • symptoms illnesses\n",
      " • patients illnesses\n",
      " • symptoms illnesses\n",
      " • symptoms illnesses\n",
      " • patients illnesses\n",
      " • patients illnesses\n",
      " • symptoms patients\n",
      " • symptoms illness\n",
      " •  \n",
      " • carefully and accurately\n",
      " • and then \n",
      " • carefully accurately\n",
      " • thoroughly and accurately\n",
      " • thoroughly and carefully to \n",
      " • thoroughly and then patients accurately\n",
      " • carefully accurately\n",
      " • patiently accurately\n",
      " • thoroughly and accurately to accurately and efficiently\n",
      " • thoroughly and accurately\n",
      " • patiently accurately\n",
      " • carefully and accurately\n",
      " • and then patients\n",
      " • systematically methodically\n",
      " • to conditions\n",
      " •  \n",
      " • thoroughly and then accurately\n",
      " • and then patients\n",
      " • thoroughly and accurately to \n",
      " • diagnose conditions\n",
      " • identify treatment\n",
      " • diagnose and treat\n",
      " • identify conditions and prescribe treatment\n",
      " • diagnose illnesses\n",
      " • identify causes\n",
      " • determine diagnoses\n",
      " • diagnose patients\n",
      " • diagnose patients\n",
      " • diagnose patients\n",
      " • diagnose patients\n",
      " • diagnose and treat conditions\n",
      " • diagnose patients\n",
      " • diagnose conditions\n",
      " • diagnose conditions accurately\n",
      " • diagnose and treat illnesses\n",
      " • diagnose and treat conditions\n",
      " • diagnose and treat\n",
      " • diagnose diseases\n",
      " • diagnose and treat\n",
      "\n",
      "Total generated: 100\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "doctors         PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "assess          PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "symptoms        PMI=   -inf   P(x)=0.000  P(y)=0.050  P(xy)=0.000\n",
      "to              PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "diagnose        PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "\n",
      "##############################################################################################################\n",
      "ANALYZING SENTENCE:\n",
      "   'artificial intelligence transforms modern industries'\n",
      "##############################################################################################################\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 0   |   Anchor word: 'artificial'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] [MASK] transforms modern industries:   0%| | 0/20 [00:Llama.generate: 29 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     499.94 ms /    18 tokens (   27.77 ms per token,    36.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =     124.40 ms /     1 runs   (  124.40 ms per token,     8.04 tokens per second)\n",
      "llama_perf_context_print:       total time =     625.03 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =          0\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:   5%| | 1/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'transforms', 'modern', 'industries'] ['ai']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1462.18 ms /    13 runs   (  112.48 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1465.20 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  10%| | 2/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     816.27 ms /     7 runs   (  116.61 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =     817.95 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  15%|▏| 3/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     783.15 ms /     7 runs   (  111.88 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =     784.68 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  20%|▏| 4/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'transforms', 'modern', 'industries'] ['technology', 'revolutionizes', 'modern', 'industries']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1488.30 ms /    13 runs   (  114.48 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1491.04 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  25%|▎| 5/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     803.34 ms /     7 runs   (  114.76 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =     804.88 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  30%|▎| 6/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1472.06 ms /    13 runs   (  113.24 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1474.96 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  35%|▎| 7/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1028.27 ms /     9 runs   (  114.25 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1030.19 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  40%|▍| 8/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1482.39 ms /    13 runs   (  114.03 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1485.14 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  45%|▍| 9/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1026.31 ms /     9 runs   (  114.03 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1028.28 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  50%|▌| 10/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     883.02 ms /     8 runs   (  110.38 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =     884.77 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  55%|▌| 11/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     578.64 ms /     5 runs   (  115.73 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =     579.79 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  60%|▌| 12/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'transforms', 'modern', 'industries'] ['ai', 'and', 'blockchain']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1029.45 ms /     9 runs   (  114.38 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1031.40 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  65%|▋| 13/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     801.34 ms /     7 runs   (  114.48 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =     802.90 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  70%|▋| 14/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1027.41 ms /     9 runs   (  114.16 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1030.01 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  75%|▊| 15/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     914.95 ms /     8 runs   (  114.37 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =     916.84 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  80%|▊| 16/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1826.26 ms /    16 runs   (  114.14 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1829.63 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  85%|▊| 17/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1030.77 ms /     9 runs   (  114.53 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1032.76 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  90%|▉| 18/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1362.39 ms /    12 runs   (  113.53 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1364.95 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  95%|▉| 19/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     800.36 ms /     7 runs   (  114.34 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =     801.93 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:   0%| | 0/20 [0Llama.generate: 32 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     397.57 ms /    14 tokens (   28.40 ms per token,    35.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =     792.35 ms /     7 runs   (  113.19 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1191.80 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:   5%| | 1/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     800.20 ms /     7 runs   (  114.31 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =     801.88 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  10%| | 2/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     785.32 ms /     7 runs   (  112.19 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =     786.94 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  15%|▏| 3/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1257.81 ms /    11 runs   (  114.35 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1260.24 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  20%|▏| 4/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     686.02 ms /     6 runs   (  114.34 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =     687.44 ms /     7 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  25%|▎| 5/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1218.29 ms /    11 runs   (  110.75 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1220.69 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  30%|▎| 6/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     867.76 ms /     8 runs   (  108.47 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =     869.54 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  35%|▎| 7/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     878.48 ms /     8 runs   (  109.81 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =     880.31 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  40%|▍| 8/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1029.01 ms /     9 runs   (  114.33 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1031.06 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  45%|▍| 9/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     800.49 ms /     7 runs   (  114.36 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =     802.09 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  50%|▌| 10/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1031.57 ms /     9 runs   (  114.62 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1033.92 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  55%|▌| 11/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1143.53 ms /    10 runs   (  114.35 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1145.74 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  60%|▌| 12/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1143.08 ms /    10 runs   (  114.31 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1145.27 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  65%|▋| 13/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1035.28 ms /     9 runs   (  115.03 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1037.65 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  70%|▋| 14/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     979.98 ms /     9 runs   (  108.89 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =     982.07 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  75%|▊| 15/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     915.00 ms /     8 runs   (  114.38 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =     917.00 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  80%|▊| 16/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     898.77 ms /     8 runs   (  112.35 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =     900.60 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  85%|▊| 17/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1618.77 ms /    12 runs   (  134.90 ms per token,     7.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1621.70 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  90%|▉| 18/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1039.80 ms /     9 runs   (  115.53 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1042.49 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  95%|▉| 19/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1049.85 ms /     9 runs   (  116.65 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1051.87 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:   0%| | 0/2Llama.generate: 33 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     489.29 ms /    14 tokens (   34.95 ms per token,    28.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =     795.14 ms /     7 runs   (  113.59 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1286.70 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:   5%| | 1/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     796.74 ms /     7 runs   (  113.82 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =     798.68 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  10%| | 2/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     944.22 ms /     8 runs   (  118.03 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =     946.47 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  15%|▏| 3/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     815.51 ms /     7 runs   (  116.50 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =     817.37 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  20%|▏| 4/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     921.73 ms /     8 runs   (  115.22 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =     923.80 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  25%|▎| 5/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     937.89 ms /     8 runs   (  117.24 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =     939.93 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  30%|▎| 6/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     817.86 ms /     7 runs   (  116.84 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =     819.64 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  35%|▎| 7/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     846.83 ms /     7 runs   (  120.98 ms per token,     8.27 tokens per second)\n",
      "llama_perf_context_print:       total time =     848.63 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  40%|▍| 8/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     868.12 ms /     7 runs   (  124.02 ms per token,     8.06 tokens per second)\n",
      "llama_perf_context_print:       total time =     869.96 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  45%|▍| 9/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     948.98 ms /     8 runs   (  118.62 ms per token,     8.43 tokens per second)\n",
      "llama_perf_context_print:       total time =     951.00 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  50%|▌| 10/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     806.87 ms /     7 runs   (  115.27 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =     808.98 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  55%|▌| 11/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     950.99 ms /     8 runs   (  118.87 ms per token,     8.41 tokens per second)\n",
      "llama_perf_context_print:       total time =     954.18 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  60%|▌| 12/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     858.82 ms /     7 runs   (  122.69 ms per token,     8.15 tokens per second)\n",
      "llama_perf_context_print:       total time =     860.66 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  65%|▋| 13/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     851.22 ms /     7 runs   (  121.60 ms per token,     8.22 tokens per second)\n",
      "llama_perf_context_print:       total time =     852.99 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  70%|▋| 14/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     950.26 ms /     8 runs   (  118.78 ms per token,     8.42 tokens per second)\n",
      "llama_perf_context_print:       total time =     952.22 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  75%|▊| 15/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     818.50 ms /     7 runs   (  116.93 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =     820.18 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  80%|▊| 16/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     800.54 ms /     7 runs   (  114.36 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =     802.23 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  85%|▊| 17/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     820.92 ms /     7 runs   (  117.27 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =     822.67 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  90%|▉| 18/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1025.37 ms /     9 runs   (  113.93 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1027.95 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  95%|▉| 19/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     765.12 ms /     7 runs   (  109.30 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =     767.00 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:   0%| | 0/20 [0Llama.generate: 35 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     514.41 ms /    12 tokens (   42.87 ms per token,    23.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1074.61 ms /     8 runs   (  134.33 ms per token,     7.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1591.44 ms /    20 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:   5%| | 1/20 [0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1167.84 ms /     9 runs   (  129.76 ms per token,     7.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1169.98 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  10%| | 2/20 [0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1132.07 ms /     9 runs   (  125.79 ms per token,     7.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1135.30 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  15%|▏| 3/20 [0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1127.58 ms /     9 runs   (  125.29 ms per token,     7.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1129.79 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  20%|▏| 4/20 [0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1086.69 ms /     9 runs   (  120.74 ms per token,     8.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1088.77 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  25%|▎| 5/20 [0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1093.32 ms /     9 runs   (  121.48 ms per token,     8.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1095.43 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  30%|▎| 6/20 [0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1018.56 ms /     9 runs   (  113.17 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1020.62 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  35%|▎| 7/20 [0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1028.18 ms /     9 runs   (  114.24 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1030.81 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  40%|▍| 8/20 [0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     983.26 ms /     9 runs   (  109.25 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =     985.45 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  45%|▍| 9/20 [0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     995.21 ms /     9 runs   (  110.58 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =     997.18 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  50%|▌| 10/20 [Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     838.51 ms /     8 runs   (  104.81 ms per token,     9.54 tokens per second)\n",
      "llama_perf_context_print:       total time =     840.27 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  55%|▌| 11/20 [Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1001.89 ms /     9 runs   (  111.32 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1004.24 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  60%|▌| 12/20 [Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1001.34 ms /     9 runs   (  111.26 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1003.36 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  65%|▋| 13/20 [Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     990.34 ms /     9 runs   (  110.04 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =     992.63 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  70%|▋| 14/20 [Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     941.33 ms /     9 runs   (  104.59 ms per token,     9.56 tokens per second)\n",
      "llama_perf_context_print:       total time =     943.31 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  75%|▊| 15/20 [Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     964.17 ms /     9 runs   (  107.13 ms per token,     9.33 tokens per second)\n",
      "llama_perf_context_print:       total time =     966.15 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  80%|▊| 16/20 [Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     988.27 ms /     9 runs   (  109.81 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =     990.28 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  85%|▊| 17/20 [Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     988.78 ms /     9 runs   (  109.86 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =     990.88 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  90%|▉| 18/20 [Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     951.12 ms /     9 runs   (  105.68 ms per token,     9.46 tokens per second)\n",
      "llama_perf_context_print:       total time =     953.08 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  95%|▉| 19/20 [Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     988.75 ms /     9 runs   (  109.86 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =     990.74 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " •  \n",
      " • artificial [intelligence]\n",
      " • ai \n",
      " •  \n",
      " • artificial [intelligence]\n",
      " • ai ai\n",
      " • artificial [intelligence]\n",
      " • artificial intelligence ai\n",
      " • artificial [intelligence]\n",
      " • ai [ai]\n",
      " • artificial intelligence\n",
      " •  \n",
      " • artificial intelligence ai\n",
      " • ai ai\n",
      " • the ai revolution\n",
      " • artificial intelligence\n",
      " • artificial [intelligence] artificially\n",
      " • artificial intelligence ai\n",
      " • artificial intelligence ai\n",
      " • ai ai\n",
      " • artificial drives\n",
      " • artificial drives\n",
      " • artificial drives\n",
      " • artificial has significantly impacted\n",
      " • artificial \n",
      " • artificial has greatly impacted\n",
      " • artificial drives\n",
      " • artificial drives\n",
      " • artificial greatly impacts\n",
      " • artificial shapes\n",
      " • artificial is transforming\n",
      " • artificial has revolutionized\n",
      " • artificial has revolutionized\n",
      " • artificial revolutionizes\n",
      " • artificial revolutionizes\n",
      " • artificial drives\n",
      " • artificial drives\n",
      " • artificial has revolutionized [mask]\n",
      " • artificial greatly benefits\n",
      " • artificial revolutionizes\n",
      " • artificial \n",
      " • artificial \n",
      " • artificial \n",
      " • artificial \n",
      " • artificial \n",
      " • artificial \n",
      " • artificial \n",
      " • artificial \n",
      " • artificial \n",
      " • artificial \n",
      " • artificial \n",
      " • artificial \n",
      " • artificial \n",
      " • artificial \n",
      " • artificial \n",
      " • artificial \n",
      " • artificial \n",
      " • artificial \n",
      " • artificial technology\n",
      " • artificial \n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      "\n",
      "Total generated: 80\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "intelligence    PMI=  0.862   P(x)=0.550  P(y)=0.100  P(xy)=0.100\n",
      "transforms      PMI=   -inf   P(x)=1.000  P(y)=0.000  P(xy)=0.000\n",
      "modern          PMI=   -inf   P(x)=1.000  P(y)=0.000  P(xy)=0.000\n",
      "industries      PMI=   -inf   P(x)=1.000  P(y)=0.000  P(xy)=0.000\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 1   |   Anchor word: 'intelligence'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] [MASK] transforms modern industries:   0%| | 0/20 [00:Llama.generate: 32 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     601.93 ms /    15 tokens (   40.13 ms per token,    24.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =     849.21 ms /     8 runs   (  106.15 ms per token,     9.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1453.73 ms /    23 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:   5%| | 1/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     895.17 ms /     8 runs   (  111.90 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =     897.18 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  10%| | 2/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'transforms', 'modern', 'industries'] ['artificial', 'intelligence', 'revolutionizes', 'modern', 'industries']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     939.59 ms /     9 runs   (  104.40 ms per token,     9.58 tokens per second)\n",
      "llama_perf_context_print:       total time =     941.65 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  15%|▏| 3/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     779.37 ms /     7 runs   (  111.34 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =     780.96 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  20%|▏| 4/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1004.20 ms /     9 runs   (  111.58 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1006.53 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  25%|▎| 5/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     792.40 ms /     7 runs   (  113.20 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =     794.03 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  30%|▎| 6/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'transforms', 'modern', 'industries'] ['digital', '\\ntechnology']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     653.66 ms /     6 runs   (  108.94 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =     655.06 ms /     7 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  35%|▎| 7/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'transforms', 'modern', 'industries'] ['technology', 'revolutionizes', 'modern', 'industries']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     996.22 ms /     9 runs   (  110.69 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =     998.24 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  40%|▍| 8/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1405.34 ms /    13 runs   (  108.10 ms per token,     9.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1408.61 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  45%|▍| 9/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     876.49 ms /     8 runs   (  109.56 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =     878.44 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  50%|▌| 10/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'transforms', 'modern', 'industries'] ['artificial', 'intelligence', 'revolutionizes', 'modern', 'industries']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1389.47 ms /    13 runs   (  106.88 ms per token,     9.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1392.26 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  55%|▌| 11/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     447.76 ms /     4 runs   (  111.94 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =     448.76 ms /     5 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  60%|▌| 12/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'transforms', 'modern', 'industries'] ['artificial', 'intelligence']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2096.52 ms /    19 runs   (  110.34 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    2100.95 ms /    20 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  65%|▋| 13/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1359.42 ms /    13 runs   (  104.57 ms per token,     9.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1362.48 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  70%|▋| 14/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     986.25 ms /     9 runs   (  109.58 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =     988.23 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  75%|▊| 15/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     783.07 ms /     7 runs   (  111.87 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =     784.63 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  80%|▊| 16/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     643.97 ms /     6 runs   (  107.33 ms per token,     9.32 tokens per second)\n",
      "llama_perf_context_print:       total time =     645.36 ms /     7 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  85%|▊| 17/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'transforms', 'modern', 'industries'] ['technology', 'revolutionizes', 'modern', 'industries']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1172.01 ms /     9 runs   (  130.22 ms per token,     7.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1174.07 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  90%|▉| 18/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1194.46 ms /    11 runs   (  108.59 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1196.92 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] [MASK] transforms modern industries:  95%|▉| 19/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     548.98 ms /     5 runs   (  109.80 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =     550.32 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'transforms', 'modern', 'industries'] ['artificial', 'intelligence', 'ai']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: artificial [MASK] [MASK] modern industries:   0%| | 0/20 [00:Llama.generate: 29 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     377.29 ms /    17 tokens (   22.19 ms per token,    45.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =     433.47 ms /     4 runs   (  108.37 ms per token,     9.23 tokens per second)\n",
      "llama_perf_context_print:       total time =     812.29 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:   5%| | 1/20 [00:Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     541.60 ms /     5 runs   (  108.32 ms per token,     9.23 tokens per second)\n",
      "llama_perf_context_print:       total time =     542.79 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  10%| | 2/20 [00:Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     540.09 ms /     5 runs   (  108.02 ms per token,     9.26 tokens per second)\n",
      "llama_perf_context_print:       total time =     541.29 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  15%|▏| 3/20 [00:Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     516.64 ms /     5 runs   (  103.33 ms per token,     9.68 tokens per second)\n",
      "llama_perf_context_print:       total time =     517.84 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  20%|▏| 4/20 [00:Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2737.89 ms /    26 runs   (  105.30 ms per token,     9.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    2743.62 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =         25\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  25%|▎| 5/20 [00:Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     553.62 ms /     5 runs   (  110.72 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =     554.84 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  30%|▎| 6/20 [00:Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     539.81 ms /     5 runs   (  107.96 ms per token,     9.26 tokens per second)\n",
      "llama_perf_context_print:       total time =     541.01 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  35%|▎| 7/20 [00:Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     550.00 ms /     5 runs   (  110.00 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =     551.33 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  40%|▍| 8/20 [00:Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     540.29 ms /     5 runs   (  108.06 ms per token,     9.25 tokens per second)\n",
      "llama_perf_context_print:       total time =     541.57 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  45%|▍| 9/20 [00:Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     539.11 ms /     5 runs   (  107.82 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =     540.29 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  50%|▌| 10/20 [00Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     545.06 ms /     5 runs   (  109.01 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =     546.57 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  55%|▌| 11/20 [00Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2772.26 ms /    26 runs   (  106.63 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    2778.18 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =         25\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  60%|▌| 12/20 [00Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2791.48 ms /    26 runs   (  107.36 ms per token,     9.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    2797.20 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =         24\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  65%|▋| 13/20 [00Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     538.31 ms /     5 runs   (  107.66 ms per token,     9.29 tokens per second)\n",
      "llama_perf_context_print:       total time =     539.79 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  70%|▋| 14/20 [00Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2773.28 ms /    26 runs   (  106.66 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    2778.97 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =         25\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  75%|▊| 15/20 [00Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     539.41 ms /     5 runs   (  107.88 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =     540.61 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  80%|▊| 16/20 [00Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     512.13 ms /     5 runs   (  102.43 ms per token,     9.76 tokens per second)\n",
      "llama_perf_context_print:       total time =     513.61 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  85%|▊| 17/20 [00Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2773.61 ms /    26 runs   (  106.68 ms per token,     9.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    2779.34 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =         25\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  90%|▉| 18/20 [00Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     534.83 ms /     5 runs   (  106.97 ms per token,     9.35 tokens per second)\n",
      "llama_perf_context_print:       total time =     536.13 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  95%|▉| 19/20 [00Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     519.20 ms /     5 runs   (  103.84 ms per token,     9.63 tokens per second)\n",
      "llama_perf_context_print:       total time =     520.52 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:   0%| | 0/20 Llama.generate: 33 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     475.77 ms /    14 tokens (   33.98 ms per token,    29.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1019.38 ms /     8 runs   (  127.42 ms per token,     7.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1497.32 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:   5%| | 1/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     956.47 ms /     8 runs   (  119.56 ms per token,     8.36 tokens per second)\n",
      "llama_perf_context_print:       total time =     958.25 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  10%| | 2/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     731.50 ms /     6 runs   (  121.92 ms per token,     8.20 tokens per second)\n",
      "llama_perf_context_print:       total time =     732.87 ms /     7 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  15%|▏| 3/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     857.48 ms /     7 runs   (  122.50 ms per token,     8.16 tokens per second)\n",
      "llama_perf_context_print:       total time =     859.21 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  20%|▏| 4/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     929.59 ms /     7 runs   (  132.80 ms per token,     7.53 tokens per second)\n",
      "llama_perf_context_print:       total time =     931.20 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  25%|▎| 5/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     987.80 ms /     8 runs   (  123.48 ms per token,     8.10 tokens per second)\n",
      "llama_perf_context_print:       total time =     989.65 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  30%|▎| 6/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     970.55 ms /     8 runs   (  121.32 ms per token,     8.24 tokens per second)\n",
      "llama_perf_context_print:       total time =     972.34 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  35%|▎| 7/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     963.54 ms /     8 runs   (  120.44 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:       total time =     965.53 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  40%|▍| 8/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     723.50 ms /     6 runs   (  120.58 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:       total time =     725.29 ms /     7 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  45%|▍| 9/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     892.19 ms /     7 runs   (  127.46 ms per token,     7.85 tokens per second)\n",
      "llama_perf_context_print:       total time =     893.78 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  50%|▌| 10/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     891.22 ms /     7 runs   (  127.32 ms per token,     7.85 tokens per second)\n",
      "llama_perf_context_print:       total time =     892.80 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  55%|▌| 11/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1008.19 ms /     8 runs   (  126.02 ms per token,     7.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1009.99 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  60%|▌| 12/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     736.93 ms /     6 runs   (  122.82 ms per token,     8.14 tokens per second)\n",
      "llama_perf_context_print:       total time =     738.33 ms /     7 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  65%|▋| 13/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1243.23 ms /    10 runs   (  124.32 ms per token,     8.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1245.63 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  70%|▋| 14/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     891.18 ms /     7 runs   (  127.31 ms per token,     7.85 tokens per second)\n",
      "llama_perf_context_print:       total time =     892.80 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  75%|▊| 15/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1016.84 ms /     8 runs   (  127.10 ms per token,     7.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1018.61 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  80%|▊| 16/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     976.26 ms /     8 runs   (  122.03 ms per token,     8.19 tokens per second)\n",
      "llama_perf_context_print:       total time =     978.06 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  85%|▊| 17/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1136.62 ms /     9 runs   (  126.29 ms per token,     7.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1138.81 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  90%|▉| 18/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1008.55 ms /     8 runs   (  126.07 ms per token,     7.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1010.38 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  95%|▉| 19/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1028.98 ms /     8 runs   (  128.62 ms per token,     7.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1031.03 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:   0%| | 0/20 [00:Llama.generate: 35 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     477.82 ms /    12 tokens (   39.82 ms per token,    25.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =     834.40 ms /     6 runs   (  139.07 ms per token,     7.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1314.03 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:   5%| | 1/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1226.16 ms /     9 runs   (  136.24 ms per token,     7.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1228.66 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  10%| | 2/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1189.92 ms /     9 runs   (  132.21 ms per token,     7.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1192.49 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  15%|▏| 3/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     927.90 ms /     7 runs   (  132.56 ms per token,     7.54 tokens per second)\n",
      "llama_perf_context_print:       total time =     929.68 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  20%|▏| 4/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1186.38 ms /     9 runs   (  131.82 ms per token,     7.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1188.60 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  25%|▎| 5/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     924.92 ms /     7 runs   (  132.13 ms per token,     7.57 tokens per second)\n",
      "llama_perf_context_print:       total time =     926.70 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  30%|▎| 6/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1235.08 ms /     9 runs   (  137.23 ms per token,     7.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1237.33 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  35%|▎| 7/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     932.03 ms /     7 runs   (  133.15 ms per token,     7.51 tokens per second)\n",
      "llama_perf_context_print:       total time =     933.88 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  40%|▍| 8/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1140.63 ms /     9 runs   (  126.74 ms per token,     7.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1143.13 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  45%|▍| 9/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1125.08 ms /     9 runs   (  125.01 ms per token,     8.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1127.99 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  50%|▌| 10/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     842.28 ms /     7 runs   (  120.33 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =     844.34 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  55%|▌| 11/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     851.96 ms /     7 runs   (  121.71 ms per token,     8.22 tokens per second)\n",
      "llama_perf_context_print:       total time =     854.51 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  60%|▌| 12/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1070.18 ms /     9 runs   (  118.91 ms per token,     8.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1072.26 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  65%|▋| 13/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1066.54 ms /     9 runs   (  118.50 ms per token,     8.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1068.63 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  70%|▋| 14/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1072.00 ms /     7 runs   (  153.14 ms per token,     6.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1073.68 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  75%|▊| 15/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     995.03 ms /     9 runs   (  110.56 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =     997.21 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  80%|▊| 16/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     813.55 ms /     7 runs   (  116.22 ms per token,     8.60 tokens per second)\n",
      "llama_perf_context_print:       total time =     815.47 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  85%|▊| 17/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1002.50 ms /     9 runs   (  111.39 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1004.53 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  90%|▉| 18/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     785.65 ms /     7 runs   (  112.24 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =     787.25 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  95%|▉| 19/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     980.98 ms /     9 runs   (  109.00 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =     982.98 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • artificial intelligence ai\n",
      " •  \n",
      " • ai artificial intelligence\n",
      " • ai ai\n",
      " • ai [mask]\n",
      " •  \n",
      " •  \n",
      " • ai [mask]\n",
      " • artificial [intelligence]\n",
      " •  \n",
      " • artificial [intelligence]\n",
      " •  \n",
      " • artificial [intelligence] [revolution]\n",
      " • artificial [intelligence]\n",
      " • artificial intelligence ai\n",
      " • ai ai\n",
      " •  \n",
      " • artificial intelligence rapidly\n",
      " • digital \n",
      "digital \n",
      " •  \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • all instances of [mask] in the following sentence with one word each that make sense artificial intelligence\n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • all instances of [mask] in the following sentence with one word each that make sense artificial intelligence\n",
      " • all instances of [mask] in the following sentence with one word each that make sense artificial intelligence\n",
      " • intelligence \n",
      " • all instances of [mask] in the following sentence with one word each that make sense artificial intelligence\n",
      " • intelligence \n",
      " • intelligence \n",
      " • all instances of [mask] in the following sentence with one word each that make sense artificial intelligence\n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence various\n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence [mask]\n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence various\n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      "\n",
      "Total generated: 80\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "artificial      PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "transforms      PMI=   -inf   P(x)=0.750  P(y)=0.000  P(xy)=0.000\n",
      "modern          PMI=   -inf   P(x)=1.000  P(y)=0.000  P(xy)=0.000\n",
      "industries      PMI=   -inf   P(x)=1.000  P(y)=0.000  P(xy)=0.000\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 2   |   Anchor word: 'transforms'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] intelligence [MASK] modern industries:   0%| | 0/20 [0Llama.generate: 29 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     599.91 ms /    17 tokens (   35.29 ms per token,    28.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1143.49 ms /     9 runs   (  127.05 ms per token,     7.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1746.06 ms /    26 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:   5%| | 1/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     791.92 ms /     6 runs   (  131.99 ms per token,     7.58 tokens per second)\n",
      "llama_perf_context_print:       total time =     793.45 ms /     7 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  10%| | 2/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1027.45 ms /     8 runs   (  128.43 ms per token,     7.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1029.31 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  15%|▏| 3/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1044.18 ms /     8 runs   (  130.52 ms per token,     7.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1046.00 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  20%|▏| 4/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1156.21 ms /     9 runs   (  128.47 ms per token,     7.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1158.65 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  25%|▎| 5/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     780.84 ms /     6 runs   (  130.14 ms per token,     7.68 tokens per second)\n",
      "llama_perf_context_print:       total time =     782.27 ms /     7 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  30%|▎| 6/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     792.31 ms /     6 runs   (  132.05 ms per token,     7.57 tokens per second)\n",
      "llama_perf_context_print:       total time =     793.71 ms /     7 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  35%|▎| 7/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1178.48 ms /     9 runs   (  130.94 ms per token,     7.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1180.62 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  40%|▍| 8/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     780.47 ms /     6 runs   (  130.08 ms per token,     7.69 tokens per second)\n",
      "llama_perf_context_print:       total time =     781.98 ms /     7 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  45%|▍| 9/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1157.14 ms /     9 runs   (  128.57 ms per token,     7.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1159.71 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  50%|▌| 10/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1030.31 ms /     8 runs   (  128.79 ms per token,     7.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1032.13 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  55%|▌| 11/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1156.37 ms /     9 runs   (  128.49 ms per token,     7.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1158.49 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  60%|▌| 12/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1064.68 ms /     8 runs   (  133.08 ms per token,     7.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1066.58 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  65%|▋| 13/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1053.95 ms /     8 runs   (  131.74 ms per token,     7.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1055.81 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  70%|▋| 14/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1644.32 ms /    13 runs   (  126.49 ms per token,     7.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1647.50 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  75%|▊| 15/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1011.62 ms /     8 runs   (  126.45 ms per token,     7.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1013.52 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  80%|▊| 16/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     999.51 ms /     8 runs   (  124.94 ms per token,     8.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1001.58 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  85%|▊| 17/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1126.56 ms /     9 runs   (  125.17 ms per token,     7.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1128.82 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  90%|▉| 18/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1008.85 ms /     8 runs   (  126.11 ms per token,     7.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1010.70 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence [MASK] modern industries:  95%|▉| 19/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     996.85 ms /     8 runs   (  124.61 ms per token,     8.03 tokens per second)\n",
      "llama_perf_context_print:       total time =     998.72 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:   0%| | 0/20 [00:Llama.generate: 29 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     452.60 ms /    17 tokens (   26.62 ms per token,    37.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =     496.11 ms /     4 runs   (  124.03 ms per token,     8.06 tokens per second)\n",
      "llama_perf_context_print:       total time =     950.14 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:   5%| | 1/20 [00:Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     637.99 ms /     5 runs   (  127.60 ms per token,     7.84 tokens per second)\n",
      "llama_perf_context_print:       total time =     639.27 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  10%| | 2/20 [00:Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     603.71 ms /     5 runs   (  120.74 ms per token,     8.28 tokens per second)\n",
      "llama_perf_context_print:       total time =     605.04 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  15%|▏| 3/20 [00:Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     635.77 ms /     5 runs   (  127.15 ms per token,     7.86 tokens per second)\n",
      "llama_perf_context_print:       total time =     636.99 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  20%|▏| 4/20 [00:Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     600.74 ms /     5 runs   (  120.15 ms per token,     8.32 tokens per second)\n",
      "llama_perf_context_print:       total time =     602.10 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  25%|▎| 5/20 [00:Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3051.57 ms /    26 runs   (  117.37 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    3057.85 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =         25\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  30%|▎| 6/20 [00:Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3074.36 ms /    26 runs   (  118.24 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    3080.38 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =         24\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  35%|▎| 7/20 [00:Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3064.54 ms /    26 runs   (  117.87 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    3070.57 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =         24\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  40%|▍| 8/20 [00:Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     601.77 ms /     5 runs   (  120.35 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =     602.98 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  45%|▍| 9/20 [00:Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     608.31 ms /     5 runs   (  121.66 ms per token,     8.22 tokens per second)\n",
      "llama_perf_context_print:       total time =     609.64 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  50%|▌| 10/20 [00Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     596.36 ms /     5 runs   (  119.27 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =     597.64 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  55%|▌| 11/20 [00Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     609.68 ms /     5 runs   (  121.94 ms per token,     8.20 tokens per second)\n",
      "llama_perf_context_print:       total time =     611.01 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  60%|▌| 12/20 [00Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     604.49 ms /     5 runs   (  120.90 ms per token,     8.27 tokens per second)\n",
      "llama_perf_context_print:       total time =     605.73 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  65%|▋| 13/20 [00Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     571.55 ms /     5 runs   (  114.31 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =     572.77 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  70%|▋| 14/20 [00Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     599.37 ms /     5 runs   (  119.87 ms per token,     8.34 tokens per second)\n",
      "llama_perf_context_print:       total time =     600.61 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  75%|▊| 15/20 [00Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     574.13 ms /     5 runs   (  114.83 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =     575.54 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  80%|▊| 16/20 [00Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     566.58 ms /     5 runs   (  113.32 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =     567.83 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  85%|▊| 17/20 [00Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     595.60 ms /     5 runs   (  119.12 ms per token,     8.39 tokens per second)\n",
      "llama_perf_context_print:       total time =     596.81 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  90%|▉| 18/20 [00Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     586.60 ms /     5 runs   (  117.32 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =     587.93 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] [MASK] modern industries:  95%|▉| 19/20 [00Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2933.65 ms /    26 runs   (  112.83 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    2939.43 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =         25\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:   0%| | 0/2Llama.generate: 30 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     588.21 ms /    16 tokens (   36.76 ms per token,    27.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1551.92 ms /    13 runs   (  119.38 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    2143.51 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:   5%| | 1/2Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     944.04 ms /     8 runs   (  118.00 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =     946.02 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  10%| | 2/2Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1013.58 ms /     9 runs   (  112.62 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1015.77 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  15%|▏| 3/2Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     930.88 ms /     8 runs   (  116.36 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =     932.84 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  20%|▏| 4/2Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2320.52 ms /    21 runs   (  110.50 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    2325.56 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =         20\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  25%|▎| 5/2Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     980.59 ms /     9 runs   (  108.95 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =     982.84 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  30%|▎| 6/2Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     964.71 ms /     9 runs   (  107.19 ms per token,     9.33 tokens per second)\n",
      "llama_perf_context_print:       total time =     966.90 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  35%|▎| 7/2Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     985.99 ms /     9 runs   (  109.55 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =     988.19 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  40%|▍| 8/2Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1926.18 ms /    16 runs   (  120.39 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1929.94 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  45%|▍| 9/2Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1208.10 ms /    11 runs   (  109.83 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1211.11 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  50%|▌| 10/Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1261.20 ms /    11 runs   (  114.65 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1264.20 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  55%|▌| 11/Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1144.64 ms /    10 runs   (  114.46 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1147.26 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  60%|▌| 12/Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     778.25 ms /     7 runs   (  111.18 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =     779.98 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  65%|▋| 13/Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     896.00 ms /     8 runs   (  112.00 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =     897.94 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  70%|▋| 14/Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1187.19 ms /    11 runs   (  107.93 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1189.94 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  75%|▊| 15/Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1416.96 ms /    13 runs   (  109.00 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1420.06 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  80%|▊| 16/Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1182.66 ms /    11 runs   (  107.51 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1185.35 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  85%|▊| 17/Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     994.78 ms /     9 runs   (  110.53 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =     997.36 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  90%|▉| 18/Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1017.02 ms /     9 runs   (  113.00 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1020.30 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  95%|▉| 19/Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     932.35 ms /     9 runs   (  103.59 ms per token,     9.65 tokens per second)\n",
      "llama_perf_context_print:       total time =     934.39 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:   0%| | 0/20 [0Llama.generate: 34 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     549.47 ms /    12 tokens (   45.79 ms per token,    21.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1009.26 ms /     9 runs   (  112.14 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1561.18 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:   5%| | 1/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1569.44 ms /    15 runs   (  104.63 ms per token,     9.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1572.71 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  10%| | 2/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1226.95 ms /    11 runs   (  111.54 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1229.42 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  15%|▏| 3/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1211.90 ms /    11 runs   (  110.17 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1214.88 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  20%|▏| 4/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2745.94 ms /    26 runs   (  105.61 ms per token,     9.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    2752.17 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =         25\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  25%|▎| 5/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1319.90 ms /    12 runs   (  109.99 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1322.70 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  30%|▎| 6/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1201.70 ms /    11 runs   (  109.25 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1204.12 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  35%|▎| 7/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1830.37 ms /    18 runs   (  101.69 ms per token,     9.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1834.49 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  40%|▍| 8/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     763.76 ms /     7 runs   (  109.11 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =     765.39 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  45%|▍| 9/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     764.61 ms /     7 runs   (  109.23 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =     766.22 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  50%|▌| 10/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1777.66 ms /    17 runs   (  104.57 ms per token,     9.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1781.39 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  55%|▌| 11/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1972.81 ms /    18 runs   (  109.60 ms per token,     9.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1976.82 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  60%|▌| 12/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['artificial', 'intelligence', '[mask]', 'modern', '[mask]'] ['artificial', 'intelligence', 'advances', 'rapidly', 'driving', '[mask]', 'technological', 'innovations', 'in', 'various', 'industries']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     873.56 ms /     8 runs   (  109.19 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =     875.38 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  65%|▋| 13/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1121.13 ms /    11 runs   (  101.92 ms per token,     9.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1123.50 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  70%|▋| 14/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     753.39 ms /     7 runs   (  107.63 ms per token,     9.29 tokens per second)\n",
      "llama_perf_context_print:       total time =     754.95 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  75%|▊| 15/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     981.15 ms /     9 runs   (  109.02 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =     983.17 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  80%|▊| 16/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1412.02 ms /    13 runs   (  108.62 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1414.82 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  85%|▊| 17/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1200.56 ms /    11 runs   (  109.14 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1203.19 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  90%|▉| 18/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     988.14 ms /     9 runs   (  109.79 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =     990.12 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  95%|▉| 19/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     769.69 ms /     7 runs   (  109.96 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =     771.32 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • artificial is transforming\n",
      " • artificial \n",
      " • artificial shapes\n",
      " • artificial revolutionizes\n",
      " • artificial greatly influences\n",
      " • artificial \n",
      " • artificial \n",
      " • artificial fuels\n",
      " • artificial \n",
      " • artificial revolutionizes\n",
      " • artificial shapes\n",
      " • artificial revolutionizes\n",
      " • artificial is driving\n",
      " • artificial drives\n",
      " • artificial is revolutionizing [mask]\n",
      " • artificial drives\n",
      " • artificial drives\n",
      " • artificial is driving\n",
      " • artificial drives\n",
      " • artificial drives\n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • all instances of [mask] in the following sentence with one word each that make sense artificial intelligence\n",
      " • all instances of [mask] in the following sentence with one word each that make sense artificial intelligence\n",
      " • all instances of [mask] in the following sentence with one word each that make sense artificial intelligence\n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • all instances of [mask] in the following sentence with one word each that make sense artificial intelligence\n",
      " • is [revolutionizing] [transforming]\n",
      " • in [mask]\n",
      " • revolutionizes [mask]\n",
      " • advance [mask]\n",
      " • is [driving] [revolutionizing] various\n",
      " • revolutionizes [mask]\n",
      " • revolutionizes [mask]\n",
      " • revolutionizes [mask]\n",
      " • [influences] [shape]\n",
      " • advances [mask] [mask]\n",
      " • revolutionizes [mask]\n",
      " • is transforming [mask]\n",
      " • technology revolutionizes\n",
      " • advance [mask]\n",
      " • revolutionizes and dominates\n",
      " • surpasses [mask] [mask]\n",
      " • revolutionizes [mask]\n",
      " • enhances [mask]\n",
      " • enhances [mask]\n",
      " • disrupts [mask]\n",
      " • transcends [mask]\n",
      " • has significantly [mask] society in various aspects\n",
      " • drives [innovation]\n",
      " • greatly [mask] [mask]\n",
      " • instances of [mask] in the following sentence with one word each that make sense artificial intelligence revolution \n",
      " • drives [technology]\n",
      " • has [mask] [mask]\n",
      " • has revolutionized [mask] [mask] in various industries\n",
      " • revolutionizes society\n",
      " • accelerates society\n",
      " • [replaces] [augments]\n",
      " •  \n",
      " • surpasses traditional \n",
      " • cannot [mask] [mask]\n",
      " • optimizes technology\n",
      " • exceeds [mask]\n",
      " • [advanced] [technology]\n",
      " • technology [mask] [mask]\n",
      " • enhances [mask]\n",
      " • revolutionizes society\n",
      "\n",
      "Total generated: 80\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "artificial      PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "intelligence    PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "modern          PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "industries      PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 3   |   Anchor word: 'modern'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:   0%| | 0/2Llama.generate: 29 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     601.72 ms /    18 tokens (   33.43 ms per token,    29.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =     627.95 ms /     6 runs   (  104.66 ms per token,     9.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1231.33 ms /    24 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:   5%| | 1/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     941.45 ms /     9 runs   (  104.61 ms per token,     9.56 tokens per second)\n",
      "llama_perf_context_print:       total time =     943.43 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  10%| | 2/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     741.77 ms /     7 runs   (  105.97 ms per token,     9.44 tokens per second)\n",
      "llama_perf_context_print:       total time =     743.39 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  15%|▏| 3/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     754.30 ms /     7 runs   (  107.76 ms per token,     9.28 tokens per second)\n",
      "llama_perf_context_print:       total time =     755.86 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  20%|▏| 4/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     979.63 ms /     9 runs   (  108.85 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =     981.64 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  25%|▎| 5/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     719.26 ms /     7 runs   (  102.75 ms per token,     9.73 tokens per second)\n",
      "llama_perf_context_print:       total time =     720.83 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  30%|▎| 6/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     836.76 ms /     8 runs   (  104.59 ms per token,     9.56 tokens per second)\n",
      "llama_perf_context_print:       total time =     838.74 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  35%|▎| 7/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     764.14 ms /     7 runs   (  109.16 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =     766.01 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  40%|▍| 8/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     945.61 ms /     9 runs   (  105.07 ms per token,     9.52 tokens per second)\n",
      "llama_perf_context_print:       total time =     947.75 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  45%|▍| 9/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     752.71 ms /     7 runs   (  107.53 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =     754.30 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  50%|▌| 10/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     722.57 ms /     7 runs   (  103.22 ms per token,     9.69 tokens per second)\n",
      "llama_perf_context_print:       total time =     724.14 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  55%|▌| 11/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     875.58 ms /     8 runs   (  109.45 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =     877.34 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  60%|▌| 12/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     856.74 ms /     8 runs   (  107.09 ms per token,     9.34 tokens per second)\n",
      "llama_perf_context_print:       total time =     858.66 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  65%|▋| 13/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     869.36 ms /     8 runs   (  108.67 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =     871.09 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  70%|▋| 14/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     876.33 ms /     8 runs   (  109.54 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =     878.13 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  75%|▊| 15/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     852.25 ms /     8 runs   (  106.53 ms per token,     9.39 tokens per second)\n",
      "llama_perf_context_print:       total time =     854.00 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  80%|▊| 16/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     750.18 ms /     7 runs   (  107.17 ms per token,     9.33 tokens per second)\n",
      "llama_perf_context_print:       total time =     751.76 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  85%|▊| 17/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     766.43 ms /     7 runs   (  109.49 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =     768.00 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  90%|▉| 18/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     737.70 ms /     7 runs   (  105.39 ms per token,     9.49 tokens per second)\n",
      "llama_perf_context_print:       total time =     739.43 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence transforms [MASK] industries:  95%|▉| 19/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     763.22 ms /     7 runs   (  109.03 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =     764.80 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:   0%| | 0/20 Llama.generate: 29 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     486.00 ms /    18 tokens (   27.00 ms per token,    37.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =     577.94 ms /     5 runs   (  115.59 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1065.38 ms /    23 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:   5%| | 1/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     833.84 ms /     8 runs   (  104.23 ms per token,     9.59 tokens per second)\n",
      "llama_perf_context_print:       total time =     835.61 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  10%| | 2/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     772.64 ms /     7 runs   (  110.38 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =     774.19 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  15%|▏| 3/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1084.65 ms /     8 runs   (  135.58 ms per token,     7.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1086.45 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  20%|▏| 4/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     770.25 ms /     7 runs   (  110.04 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =     771.88 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  25%|▎| 5/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     861.38 ms /     8 runs   (  107.67 ms per token,     9.29 tokens per second)\n",
      "llama_perf_context_print:       total time =     863.15 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  30%|▎| 6/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     737.26 ms /     7 runs   (  105.32 ms per token,     9.49 tokens per second)\n",
      "llama_perf_context_print:       total time =     738.82 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  35%|▎| 7/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     945.38 ms /     9 runs   (  105.04 ms per token,     9.52 tokens per second)\n",
      "llama_perf_context_print:       total time =     948.54 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  40%|▍| 8/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     842.40 ms /     8 runs   (  105.30 ms per token,     9.50 tokens per second)\n",
      "llama_perf_context_print:       total time =     844.44 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  45%|▍| 9/20 Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     737.93 ms /     7 runs   (  105.42 ms per token,     9.49 tokens per second)\n",
      "llama_perf_context_print:       total time =     739.49 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  50%|▌| 10/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     838.48 ms /     8 runs   (  104.81 ms per token,     9.54 tokens per second)\n",
      "llama_perf_context_print:       total time =     840.24 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  55%|▌| 11/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     837.31 ms /     8 runs   (  104.66 ms per token,     9.55 tokens per second)\n",
      "llama_perf_context_print:       total time =     839.05 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  60%|▌| 12/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     848.12 ms /     8 runs   (  106.02 ms per token,     9.43 tokens per second)\n",
      "llama_perf_context_print:       total time =     849.85 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  65%|▋| 13/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     757.35 ms /     7 runs   (  108.19 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =     758.90 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  70%|▋| 14/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     838.07 ms /     8 runs   (  104.76 ms per token,     9.55 tokens per second)\n",
      "llama_perf_context_print:       total time =     839.82 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  75%|▊| 15/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     850.59 ms /     8 runs   (  106.32 ms per token,     9.41 tokens per second)\n",
      "llama_perf_context_print:       total time =     852.89 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  80%|▊| 16/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     736.17 ms /     7 runs   (  105.17 ms per token,     9.51 tokens per second)\n",
      "llama_perf_context_print:       total time =     737.84 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  85%|▊| 17/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     847.14 ms /     8 runs   (  105.89 ms per token,     9.44 tokens per second)\n",
      "llama_perf_context_print:       total time =     849.07 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  90%|▉| 18/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     841.75 ms /     8 runs   (  105.22 ms per token,     9.50 tokens per second)\n",
      "llama_perf_context_print:       total time =     843.48 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial [MASK] transforms [MASK] industries:  95%|▉| 19/20Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     843.83 ms /     8 runs   (  105.48 ms per token,     9.48 tokens per second)\n",
      "llama_perf_context_print:       total time =     845.58 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:   0%| | 0/2Llama.generate: 30 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     485.37 ms /    16 tokens (   30.34 ms per token,    32.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1021.70 ms /     9 runs   (  113.52 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1509.42 ms /    25 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:   5%| | 1/2Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1110.17 ms /    11 runs   (  100.92 ms per token,     9.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1112.48 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  10%| | 2/2Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1031.12 ms /    10 runs   (  103.11 ms per token,     9.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1033.30 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  15%|▏| 3/2Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     868.74 ms /     8 runs   (  108.59 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =     870.49 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  20%|▏| 4/2Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     868.93 ms /     8 runs   (  108.62 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =     870.79 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  25%|▎| 5/2Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     815.32 ms /     8 runs   (  101.92 ms per token,     9.81 tokens per second)\n",
      "llama_perf_context_print:       total time =     817.10 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  30%|▎| 6/2Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     978.43 ms /     9 runs   (  108.71 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =     980.40 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  35%|▎| 7/2Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1049.30 ms /    10 runs   (  104.93 ms per token,     9.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1051.45 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  40%|▍| 8/2Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1273.62 ms /    12 runs   (  106.14 ms per token,     9.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1276.43 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  45%|▍| 9/2Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1007.15 ms /    10 runs   (  100.71 ms per token,     9.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1009.42 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  50%|▌| 10/Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     743.89 ms /     7 runs   (  106.27 ms per token,     9.41 tokens per second)\n",
      "llama_perf_context_print:       total time =     745.45 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  55%|▌| 11/Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1025.80 ms /    10 runs   (  102.58 ms per token,     9.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1028.16 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  60%|▌| 12/Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1078.73 ms /    10 runs   (  107.87 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1080.95 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  65%|▋| 13/Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1220.96 ms /    12 runs   (  101.75 ms per token,     9.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1223.56 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  70%|▋| 14/Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     912.90 ms /     9 runs   (  101.43 ms per token,     9.86 tokens per second)\n",
      "llama_perf_context_print:       total time =     914.90 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  75%|▊| 15/Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     932.70 ms /     9 runs   (  103.63 ms per token,     9.65 tokens per second)\n",
      "llama_perf_context_print:       total time =     934.68 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  80%|▊| 16/Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1182.93 ms /    11 runs   (  107.54 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1185.31 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  85%|▊| 17/Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     869.44 ms /     8 runs   (  108.68 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =     871.42 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  90%|▉| 18/Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1155.22 ms /    11 runs   (  105.02 ms per token,     9.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1157.87 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence [MASK] [MASK] industries:  95%|▉| 19/Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     759.68 ms /     7 runs   (  108.53 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =     761.27 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:   0%| | 0/2Llama.generate: 31 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     599.90 ms /    16 tokens (   37.49 ms per token,    26.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1193.11 ms /    11 runs   (  108.46 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1795.70 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:   5%| | 1/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1153.07 ms /    11 runs   (  104.82 ms per token,     9.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1155.51 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  10%| | 2/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1064.69 ms /    10 runs   (  106.47 ms per token,     9.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1066.94 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  15%|▏| 3/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     646.26 ms /     6 runs   (  107.71 ms per token,     9.28 tokens per second)\n",
      "llama_perf_context_print:       total time =     647.70 ms /     7 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  20%|▏| 4/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1063.04 ms /    10 runs   (  106.30 ms per token,     9.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1065.28 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  25%|▎| 5/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     931.17 ms /     9 runs   (  103.46 ms per token,     9.67 tokens per second)\n",
      "llama_perf_context_print:       total time =     933.23 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  30%|▎| 6/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1055.12 ms /    10 runs   (  105.51 ms per token,     9.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1057.31 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  35%|▎| 7/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1054.39 ms /    10 runs   (  105.44 ms per token,     9.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1056.57 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  40%|▍| 8/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1056.02 ms /    10 runs   (  105.60 ms per token,     9.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1058.20 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  45%|▍| 9/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1141.04 ms /    11 runs   (  103.73 ms per token,     9.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1143.45 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  50%|▌| 10/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1265.84 ms /    12 runs   (  105.49 ms per token,     9.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1268.64 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  55%|▌| 11/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     739.07 ms /     7 runs   (  105.58 ms per token,     9.47 tokens per second)\n",
      "llama_perf_context_print:       total time =     740.81 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  60%|▌| 12/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1054.38 ms /    10 runs   (  105.44 ms per token,     9.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1056.58 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  65%|▋| 13/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1030.78 ms /    10 runs   (  103.08 ms per token,     9.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1033.25 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  70%|▋| 14/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     844.62 ms /     8 runs   (  105.58 ms per token,     9.47 tokens per second)\n",
      "llama_perf_context_print:       total time =     846.39 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  75%|▊| 15/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     944.47 ms /     9 runs   (  104.94 ms per token,     9.53 tokens per second)\n",
      "llama_perf_context_print:       total time =     946.44 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  80%|▊| 16/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1060.22 ms /    10 runs   (  106.02 ms per token,     9.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1062.55 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  85%|▊| 17/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1139.05 ms /    11 runs   (  103.55 ms per token,     9.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1141.52 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  90%|▉| 18/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     845.93 ms /     8 runs   (  105.74 ms per token,     9.46 tokens per second)\n",
      "llama_perf_context_print:       total time =     847.70 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  95%|▉| 19/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1065.96 ms /    10 runs   (  106.60 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1068.15 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • artificial \n",
      " • artificial technology\n",
      " • artificial \n",
      " • artificial \n",
      " • artificial technology\n",
      " • artificial \n",
      " • artificial \n",
      " • artificial \n",
      " • artificial various\n",
      " • artificial \n",
      " • artificial \n",
      " • artificial \n",
      " • artificial \n",
      " • artificial \n",
      " • artificial \n",
      " • artificial technology\n",
      " • artificial \n",
      " • artificial \n",
      " • artificial \n",
      " • artificial \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence various\n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • intelligence \n",
      " • is transforming [mask]\n",
      " • drives [mask] [mask]\n",
      " • is revolutionizing [mask]\n",
      " • revolutionizes \n",
      " • drives [mask]\n",
      " • powers [mask]\n",
      " • revolutionizes [mask]\n",
      " • is revolutionizing [mask]\n",
      " • is revolutionizing [mask]\n",
      " • is revolutionizing [mask]\n",
      " • revolutionizes \n",
      " • is transforming [mask]\n",
      " • advances [mask]\n",
      " • is revolutionizing [mask]\n",
      " • enhances [mask]\n",
      " • revolutionizes [mask]\n",
      " • enables [mask] [mask]\n",
      " • transcends \n",
      " • revolutionizes [mask]\n",
      " • revolutionizes \n",
      " • human life rapidly and significantly\n",
      " • the world of technology\n",
      " • industries and applications\n",
      " • industries \n",
      " • society and industries\n",
      " • industries rapidly\n",
      " • industries and society\n",
      " • industries and society\n",
      " • industries and jobs\n",
      " • industries and economies\n",
      " • industries rapidly and fundamentally\n",
      " • industries rapidly\n",
      " • industries and workflows\n",
      " • society and industry\n",
      " • industries and society\n",
      " • the world\n",
      " • society and industries\n",
      " • the future of technology\n",
      " • industries and society\n",
      " • industries and businesses\n",
      "\n",
      "Total generated: 80\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "artificial      PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "intelligence    PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "transforms      PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "industries      PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 4   |   Anchor word: 'industries'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:   0%| | 0/20 [0Llama.generate: 29 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     709.18 ms /    18 tokens (   39.40 ms per token,    25.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =     854.85 ms /     8 runs   (  106.86 ms per token,     9.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1566.14 ms /    26 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:   5%| | 1/20 [0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     896.08 ms /     8 runs   (  112.01 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =     898.04 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  10%| | 2/20 [0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     972.41 ms /     9 runs   (  108.05 ms per token,     9.26 tokens per second)\n",
      "llama_perf_context_print:       total time =     974.57 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  15%|▏| 3/20 [0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     978.44 ms /     9 runs   (  108.72 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =     980.59 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  20%|▏| 4/20 [0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     953.53 ms /     9 runs   (  105.95 ms per token,     9.44 tokens per second)\n",
      "llama_perf_context_print:       total time =     955.52 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  25%|▎| 5/20 [0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     920.79 ms /     9 runs   (  102.31 ms per token,     9.77 tokens per second)\n",
      "llama_perf_context_print:       total time =     922.79 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  30%|▎| 6/20 [0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     991.69 ms /     9 runs   (  110.19 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =     993.65 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  35%|▎| 7/20 [0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     981.50 ms /     9 runs   (  109.06 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =     983.64 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  40%|▍| 8/20 [0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     977.22 ms /     9 runs   (  108.58 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =     979.26 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  45%|▍| 9/20 [0Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     828.53 ms /     8 runs   (  103.57 ms per token,     9.66 tokens per second)\n",
      "llama_perf_context_print:       total time =     830.30 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  50%|▌| 10/20 [Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     989.08 ms /     9 runs   (  109.90 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =     991.04 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  55%|▌| 11/20 [Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     996.84 ms /     9 runs   (  110.76 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =     998.83 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  60%|▌| 12/20 [Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1001.22 ms /     9 runs   (  111.25 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1003.20 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  65%|▋| 13/20 [Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     946.57 ms /     9 runs   (  105.17 ms per token,     9.51 tokens per second)\n",
      "llama_perf_context_print:       total time =     948.57 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  70%|▋| 14/20 [Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     994.44 ms /     9 runs   (  110.49 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =     996.47 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  75%|▊| 15/20 [Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     949.54 ms /     9 runs   (  105.50 ms per token,     9.48 tokens per second)\n",
      "llama_perf_context_print:       total time =     951.72 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  80%|▊| 16/20 [Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     951.22 ms /     9 runs   (  105.69 ms per token,     9.46 tokens per second)\n",
      "llama_perf_context_print:       total time =     953.19 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  85%|▊| 17/20 [Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     950.78 ms /     9 runs   (  105.64 ms per token,     9.47 tokens per second)\n",
      "llama_perf_context_print:       total time =     952.74 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  90%|▉| 18/20 [Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     854.30 ms /     8 runs   (  106.79 ms per token,     9.36 tokens per second)\n",
      "llama_perf_context_print:       total time =     856.31 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] intelligence transforms modern [MASK]:  95%|▉| 19/20 [Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     983.43 ms /     9 runs   (  109.27 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =     985.70 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:   0%| | 0/20 [00:Llama.generate: 29 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     452.24 ms /    18 tokens (   25.12 ms per token,    39.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =     700.43 ms /     6 runs   (  116.74 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1154.30 ms /    24 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:   5%| | 1/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1084.22 ms /     9 runs   (  120.47 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1086.19 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  10%| | 2/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1139.12 ms /     9 runs   (  126.57 ms per token,     7.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1141.08 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  15%|▏| 3/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1141.79 ms /     9 runs   (  126.87 ms per token,     7.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1143.76 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  20%|▏| 4/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1142.85 ms /     9 runs   (  126.98 ms per token,     7.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1144.97 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  25%|▎| 5/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1145.22 ms /     9 runs   (  127.25 ms per token,     7.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1147.22 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  30%|▎| 6/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1149.07 ms /     9 runs   (  127.67 ms per token,     7.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1151.05 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  35%|▎| 7/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1151.65 ms /     9 runs   (  127.96 ms per token,     7.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1153.85 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  40%|▍| 8/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1121.46 ms /     9 runs   (  124.61 ms per token,     8.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1123.68 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  45%|▍| 9/20 [00:Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     902.79 ms /     7 runs   (  128.97 ms per token,     7.75 tokens per second)\n",
      "llama_perf_context_print:       total time =     904.44 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  50%|▌| 10/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     884.68 ms /     7 runs   (  126.38 ms per token,     7.91 tokens per second)\n",
      "llama_perf_context_print:       total time =     886.45 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  55%|▌| 11/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1141.48 ms /     9 runs   (  126.83 ms per token,     7.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1143.53 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  60%|▌| 12/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1084.96 ms /     9 runs   (  120.55 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1086.99 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  65%|▋| 13/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1099.62 ms /     9 runs   (  122.18 ms per token,     8.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1101.63 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  70%|▋| 14/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1066.97 ms /     9 runs   (  118.55 ms per token,     8.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1069.84 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  75%|▊| 15/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1036.32 ms /     9 runs   (  115.15 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1038.77 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  80%|▊| 16/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     969.95 ms /     9 runs   (  107.77 ms per token,     9.28 tokens per second)\n",
      "llama_perf_context_print:       total time =     972.95 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  85%|▊| 17/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1033.87 ms /     9 runs   (  114.87 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1036.74 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  90%|▉| 18/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1031.32 ms /     9 runs   (  114.59 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1033.35 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial [MASK] transforms modern [MASK]:  95%|▉| 19/20 [00Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     803.00 ms /     7 runs   (  114.71 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =     805.60 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:   0%| | 0/20 [0Llama.generate: 30 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     535.21 ms /    16 tokens (   33.45 ms per token,    29.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =     884.28 ms /     7 runs   (  126.33 ms per token,     7.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1422.50 ms /    23 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:   5%| | 1/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     891.30 ms /     7 runs   (  127.33 ms per token,     7.85 tokens per second)\n",
      "llama_perf_context_print:       total time =     893.31 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  10%| | 2/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1146.32 ms /     9 runs   (  127.37 ms per token,     7.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1149.18 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  15%|▏| 3/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1349.89 ms /    11 runs   (  122.72 ms per token,     8.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1352.43 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  20%|▏| 4/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1269.97 ms /    10 runs   (  127.00 ms per token,     7.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1272.21 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  25%|▎| 5/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1262.13 ms /    10 runs   (  126.21 ms per token,     7.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1265.13 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  30%|▎| 6/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1704.90 ms /    14 runs   (  121.78 ms per token,     8.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1708.09 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  35%|▎| 7/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3386.71 ms /    27 runs   (  125.43 ms per token,     7.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    3392.66 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =         26\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  40%|▍| 8/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1513.91 ms /    12 runs   (  126.16 ms per token,     7.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1517.07 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  45%|▍| 9/20 [0Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3196.53 ms /    26 runs   (  122.94 ms per token,     8.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    3202.23 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =         25\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  50%|▌| 10/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1635.60 ms /    13 runs   (  125.82 ms per token,     7.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1638.47 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  55%|▌| 11/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1259.10 ms /    10 runs   (  125.91 ms per token,     7.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1261.55 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  60%|▌| 12/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1283.84 ms /    11 runs   (  116.71 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1286.33 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  65%|▋| 13/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1494.83 ms /    12 runs   (  124.57 ms per token,     8.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1497.93 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  70%|▋| 14/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1310.93 ms /    11 runs   (  119.18 ms per token,     8.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1313.50 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  75%|▊| 15/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     815.77 ms /     7 runs   (  116.54 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =     817.37 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  80%|▊| 16/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     739.53 ms /     6 runs   (  123.26 ms per token,     8.11 tokens per second)\n",
      "llama_perf_context_print:       total time =     740.93 ms /     7 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  85%|▊| 17/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1355.28 ms /    11 runs   (  123.21 ms per token,     8.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1357.71 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  90%|▉| 18/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1440.75 ms /    12 runs   (  120.06 ms per token,     8.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1443.31 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: artificial intelligence [MASK] modern [MASK]:  95%|▉| 19/20 [Llama.generate: 45 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1340.07 ms /    11 runs   (  121.82 ms per token,     8.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1342.47 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:   0%| | 0/2Llama.generate: 31 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     616.70 ms /    16 tokens (   38.54 ms per token,    25.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =     991.14 ms /     8 runs   (  123.89 ms per token,     8.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1609.89 ms /    24 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:   5%| | 1/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1359.23 ms /    11 runs   (  123.57 ms per token,     8.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1361.61 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  10%| | 2/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1260.41 ms /    10 runs   (  126.04 ms per token,     7.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1262.61 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  15%|▏| 3/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1010.92 ms /     8 runs   (  126.37 ms per token,     7.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1012.72 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  20%|▏| 4/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1016.88 ms /     8 runs   (  127.11 ms per token,     7.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1018.76 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  25%|▎| 5/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1322.84 ms /    11 runs   (  120.26 ms per token,     8.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1325.28 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  30%|▎| 6/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     876.57 ms /     7 runs   (  125.22 ms per token,     7.99 tokens per second)\n",
      "llama_perf_context_print:       total time =     878.15 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  35%|▎| 7/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1368.76 ms /    11 runs   (  124.43 ms per token,     8.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1371.33 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  40%|▍| 8/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1200.54 ms /    10 runs   (  120.05 ms per token,     8.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1202.82 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  45%|▍| 9/2Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1250.92 ms /    10 runs   (  125.09 ms per token,     7.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1253.06 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  50%|▌| 10/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1482.76 ms /    12 runs   (  123.56 ms per token,     8.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1485.54 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  55%|▌| 11/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1207.77 ms /    10 runs   (  120.78 ms per token,     8.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1210.09 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  60%|▌| 12/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1251.70 ms /    10 runs   (  125.17 ms per token,     7.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1253.91 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  65%|▋| 13/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1356.10 ms /    11 runs   (  123.28 ms per token,     8.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1358.52 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  70%|▋| 14/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1506.60 ms /    13 runs   (  115.89 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1509.39 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  75%|▊| 15/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1222.31 ms /    10 runs   (  122.23 ms per token,     8.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1224.56 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  80%|▊| 16/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1195.40 ms /    10 runs   (  119.54 ms per token,     8.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1197.76 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  85%|▊| 17/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1041.97 ms /     9 runs   (  115.77 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1044.07 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  90%|▉| 18/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1186.60 ms /    10 runs   (  118.66 ms per token,     8.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1188.96 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: artificial intelligence transforms [MASK] [MASK]:  95%|▉| 19/Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1191.62 ms /    10 runs   (  119.16 ms per token,     8.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1193.83 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial technology\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • artificial society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • intelligence society\n",
      " • advances society\n",
      " • exceeds traditional \n",
      " • drives [technology]\n",
      " • rapidly [mask] [mask]\n",
      " • has revolutionized technology\n",
      " • is revolutionizing technology\n",
      " • is increasingly [mask] in [mask]\n",
      " • instances of [mask] in the following sentence with one word each that make sense artificial intelligence applications era\n",
      " • rapidly [mask] [mask]\n",
      " • instances of [mask] in the following sentence with one word each that make sense artificial intelligence technology \n",
      " • is [transforming] [industries]\n",
      " • has revolutionized technology\n",
      " • can [mask] [mask]\n",
      " • dramatically [mask] [mask]\n",
      " • significantly [mask] [mask]\n",
      " • revolutionizes society\n",
      " • optimizes \n",
      " • dramatically [mask] [mask]\n",
      " • significantly [mask] [mask]\n",
      " • cannot [mask] [mask]\n",
      " • industries rapidly\n",
      " • industries and daily life\n",
      " • industries and society\n",
      " • society and economy\n",
      " • industries and society\n",
      " • industries and economies\n",
      " • the world\n",
      " • the world of technology\n",
      " • industries and processes\n",
      " • industries and economy\n",
      " • industries and workflows\n",
      " • society and industries\n",
      " • society and industries\n",
      " • industries rapidly and significantly\n",
      " • human communication and decisionmaking\n",
      " • industries and society\n",
      " • industries and society\n",
      " • human interaction\n",
      " • industries and lives\n",
      " • society and work\n",
      "\n",
      "Total generated: 80\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "artificial      PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "intelligence    PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "transforms      PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "modern          PMI=   -inf   P(x)=0.600  P(y)=0.000  P(xy)=0.000\n",
      "\n",
      "##############################################################################################################\n",
      "ANALYZING SENTENCE:\n",
      "   'children love sweet ice cream on warm summer days'\n",
      "##############################################################################################################\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 0   |   Anchor word: 'children'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:   0%| | 0/Llama.generate: 29 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     727.63 ms /    21 tokens (   34.65 ms per token,    28.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1477.51 ms /    12 runs   (  123.13 ms per token,     8.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    2208.32 ms /    33 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:   5%| | 1/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['she', 'savors', 'delightful', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1159.51 ms /    10 runs   (  115.95 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1161.72 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  10%| | 2/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1601.85 ms /    14 runs   (  114.42 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1605.21 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  15%|▏| 3/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['on', 'warm', 'summer', 'days', 'i', 'love', 'to', 'eat', 'delicious', 'ice', 'cream']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1314.31 ms /    12 runs   (  109.53 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1317.32 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  20%|▏| 4/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1342.86 ms /    12 runs   (  111.91 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1345.64 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  25%|▎| 5/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1448.60 ms /    13 runs   (  111.43 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1451.44 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  30%|▎| 6/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1090.91 ms /     9 runs   (  121.21 ms per token,     8.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1093.20 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  35%|▎| 7/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['treats', 'tastes', 'better', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1537.20 ms /    13 runs   (  118.25 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1540.35 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  40%|▍| 8/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['a', 'perfect', '[delicacy]', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1214.46 ms /    10 runs   (  121.45 ms per token,     8.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1217.28 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  45%|▍| 9/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1431.63 ms /    12 runs   (  119.30 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1434.78 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  50%|▌| 10Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1632.05 ms /    14 runs   (  116.58 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1635.48 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  55%|▌| 11Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['the', 'delicious', 'vanilla', 'ice', 'cream', 'melts', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2001.68 ms /    17 runs   (  117.75 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    2005.78 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  60%|▌| 12Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1642.48 ms /    14 runs   (  117.32 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1646.02 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  65%|▋| 13Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'to', 'eat', 'vanilla', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1420.85 ms /    12 runs   (  118.40 ms per token,     8.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1423.77 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  70%|▋| 14Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['she', 'enjoys', 'delicious', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1519.56 ms /    13 runs   (  116.89 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1522.75 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  75%|▊| 15Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['i', 'love', 'to', 'eat', 'delicious', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1490.96 ms /    13 runs   (  114.69 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1494.18 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  80%|▊| 16Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1357.21 ms /    12 runs   (  113.10 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1360.66 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  85%|▊| 17Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1580.16 ms /    13 runs   (  121.55 ms per token,     8.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1583.51 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  90%|▉| 18Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1465.81 ms /    12 runs   (  122.15 ms per token,     8.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1469.01 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  95%|▉| 19Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1696.09 ms /    15 runs   (  113.07 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1699.37 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:   0%| | 0/2Llama.generate: 32 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     503.99 ms /    18 tokens (   28.00 ms per token,    35.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =     468.73 ms /     4 runs   (  117.18 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =     974.05 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:   5%| | 1/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'to', 'eat']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1423.11 ms /    12 runs   (  118.59 ms per token,     8.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1426.27 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  10%| | 2/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1398.04 ms /    12 runs   (  116.50 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1400.94 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  15%|▏| 3/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1538.76 ms /    13 runs   (  118.37 ms per token,     8.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1541.84 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  20%|▏| 4/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1403.08 ms /    12 runs   (  116.92 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1406.09 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  25%|▎| 5/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1164.36 ms /    10 runs   (  116.44 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1166.71 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  30%|▎| 6/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1165.62 ms /    10 runs   (  116.56 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1168.00 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  35%|▎| 7/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1404.17 ms /    12 runs   (  117.01 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1406.99 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  40%|▍| 8/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1281.90 ms /    11 runs   (  116.54 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1284.55 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  45%|▍| 9/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1276.47 ms /    11 runs   (  116.04 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1279.30 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  50%|▌| 10/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'her', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1400.88 ms /    12 runs   (  116.74 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1403.67 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  55%|▌| 11/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1393.99 ms /    12 runs   (  116.17 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1396.87 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  60%|▌| 12/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'vanilla', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1283.53 ms /    11 runs   (  116.68 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1286.11 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  65%|▋| 13/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['john', 'loves', 'chocolate', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1314.18 ms /    11 runs   (  119.47 ms per token,     8.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1316.85 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  70%|▋| 14/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'vanilla', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     583.11 ms /     5 runs   (  116.62 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =     584.39 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  75%|▊| 15/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'to', 'eat']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1390.64 ms /    12 runs   (  115.89 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1393.45 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  80%|▊| 16/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1867.80 ms /    16 runs   (  116.74 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1871.87 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  85%|▊| 17/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     466.44 ms /     4 runs   (  116.61 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =     467.81 ms /     5 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  90%|▉| 18/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['i', 'love', 'eating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1423.60 ms /    12 runs   (  118.63 ms per token,     8.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1426.63 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  95%|▉| 19/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'vanilla', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1306.43 ms /    11 runs   (  118.77 ms per token,     8.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1309.16 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:   0%| | 0Llama.generate: 33 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     412.55 ms /    17 tokens (   24.27 ms per token,    41.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1238.78 ms /    11 runs   (  112.62 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1654.36 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:   5%| | 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1390.22 ms /    12 runs   (  115.85 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1393.70 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  10%| | 2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1348.70 ms /    12 runs   (  112.39 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1351.82 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  15%|▏| 3Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1504.12 ms /    13 runs   (  115.70 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1508.07 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  20%|▏| 4Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1496.99 ms /    13 runs   (  115.15 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1500.09 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  25%|▎| 5Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1482.38 ms /    13 runs   (  114.03 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1485.41 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  30%|▎| 6Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', '[mask]', 'cream', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'coconut', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1389.18 ms /    12 runs   (  115.77 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1391.93 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  35%|▎| 7Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', '[mask]', 'cream', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'strawberry', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1698.60 ms /    15 runs   (  113.24 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1702.09 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  40%|▍| 8Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1468.35 ms /    13 runs   (  112.95 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1471.42 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  45%|▍| 9Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1482.47 ms /    13 runs   (  114.04 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1485.52 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  50%|▌| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1535.97 ms /    13 runs   (  118.15 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1539.18 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  55%|▌| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1370.70 ms /    12 runs   (  114.23 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1373.72 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  60%|▌| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1378.79 ms /    12 runs   (  114.90 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1381.56 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  65%|▋| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1348.05 ms /    12 runs   (  112.34 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1350.76 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  70%|▋| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1293.23 ms /    12 runs   (  107.77 ms per token,     9.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1295.99 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  75%|▊| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', '[mask]', 'cream', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'strawberry', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1346.97 ms /    12 runs   (  112.25 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1350.21 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  80%|▊| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1307.08 ms /    12 runs   (  108.92 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1309.82 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  85%|▊| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1209.40 ms /    11 runs   (  109.95 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1211.89 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  90%|▉| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1470.89 ms /    13 runs   (  113.15 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1473.78 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  95%|▉| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1351.04 ms /    12 runs   (  112.59 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1353.78 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:   0%| | 0/2Llama.generate: 34 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     455.81 ms /    16 tokens (   28.49 ms per token,    35.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1088.83 ms /    10 runs   (  108.88 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1547.36 ms /    26 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:   5%| | 1/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', '[mask]', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1216.09 ms /    11 runs   (  110.55 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1219.18 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  10%| | 2/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1287.14 ms /    11 runs   (  117.01 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1289.64 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  15%|▏| 3/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1353.75 ms /    11 runs   (  123.07 ms per token,     8.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1356.16 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  20%|▏| 4/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', '[mask]', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'tea', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1242.55 ms /    11 runs   (  112.96 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1244.87 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  25%|▎| 5/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1227.80 ms /    11 runs   (  111.62 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1230.16 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  30%|▎| 6/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', '[mask]', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1177.70 ms /    11 runs   (  107.06 ms per token,     9.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1180.07 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  35%|▎| 7/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1211.12 ms /    11 runs   (  110.10 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1213.53 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  40%|▍| 8/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1201.98 ms /    11 runs   (  109.27 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1204.37 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  45%|▍| 9/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', '[mask]', 'on', 'warm', 'summer', 'days'] ['he', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1199.61 ms /    11 runs   (  109.06 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1202.07 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  50%|▌| 10/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1193.20 ms /    11 runs   (  108.47 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1195.62 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  55%|▌| 11/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1186.75 ms /    11 runs   (  107.89 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1189.14 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  60%|▌| 12/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1216.92 ms /    11 runs   (  110.63 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1219.35 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  65%|▋| 13/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1215.22 ms /    11 runs   (  110.47 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1217.92 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  70%|▋| 14/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', '[mask]', 'on', 'warm', 'summer', 'days'] ['he', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1152.92 ms /    10 runs   (  115.29 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1155.40 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  75%|▊| 15/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1231.91 ms /    11 runs   (  111.99 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1234.44 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  80%|▊| 16/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', '[mask]', 'on', 'warm', 'summer', 'days'] ['he', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1342.25 ms /    11 runs   (  122.02 ms per token,     8.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1344.95 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  85%|▊| 17/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', '[mask]', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1358.01 ms /    11 runs   (  123.46 ms per token,     8.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1360.92 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  90%|▉| 18/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1222.27 ms /    11 runs   (  111.12 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1225.34 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  95%|▉| 19/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1228.91 ms /    11 runs   (  111.72 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1231.78 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:   0%| | Llama.generate: 35 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     437.28 ms /    15 tokens (   29.15 ms per token,    34.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1230.85 ms /    10 runs   (  123.08 ms per token,     8.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1670.88 ms /    25 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:   5%| | Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', 'days'] ['he', 'loves', 'sweet', 'ice', 'cream', 'during', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1387.41 ms /    11 runs   (  126.13 ms per token,     7.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1390.17 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  10%| | Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1311.87 ms /    11 runs   (  119.26 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1316.12 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  15%|▏| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1247.79 ms /    11 runs   (  113.44 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1250.65 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  20%|▏| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1242.46 ms /    11 runs   (  112.95 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1244.94 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  25%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1172.68 ms /    10 runs   (  117.27 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1174.96 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  30%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1317.80 ms /    11 runs   (  119.80 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1320.31 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  35%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1265.31 ms /    11 runs   (  115.03 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1267.85 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  40%|▍| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1198.56 ms /    11 runs   (  108.96 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1201.24 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  45%|▍| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     775.79 ms /     7 runs   (  110.83 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =     777.76 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  50%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', 'days'] ['van', 'love', 'sweet', 'ice', 'cream', 'dogs']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1184.92 ms /    11 runs   (  107.72 ms per token,     9.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1187.39 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  55%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1210.02 ms /    11 runs   (  110.00 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1212.55 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  60%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1153.78 ms /    11 runs   (  104.89 ms per token,     9.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1156.26 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  65%|▋| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1203.89 ms /    11 runs   (  109.44 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1206.41 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  70%|▋| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1098.42 ms /    10 runs   (  109.84 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1100.74 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  75%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1273.95 ms /    11 runs   (  115.81 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1276.42 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  80%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1350.33 ms /    11 runs   (  122.76 ms per token,     8.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1353.25 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  85%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', 'days'] ['van', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1344.57 ms /    11 runs   (  122.23 ms per token,     8.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1347.45 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  90%|▉| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', 'days'] ['he', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1334.16 ms /    11 runs   (  121.29 ms per token,     8.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1336.66 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  95%|▉| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1384.57 ms /    11 runs   (  125.87 ms per token,     7.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1386.96 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:   0%| | 0/Llama.generate: 36 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     725.72 ms /    14 tokens (   51.84 ms per token,    19.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1168.00 ms /    10 runs   (  116.80 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1896.41 ms /    24 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:   5%| | 1/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1297.80 ms /    11 runs   (  117.98 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1300.80 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  10%| | 2/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', '[mask]', 'summer', 'days'] ['he', 'loves', 'sweet', 'ice', 'cream', 'on', 'hot', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1342.74 ms /    11 runs   (  122.07 ms per token,     8.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1345.37 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  15%|▏| 3/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1417.02 ms /    11 runs   (  128.82 ms per token,     7.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1419.52 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  20%|▏| 4/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', '[mask]', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'hot', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1386.80 ms /    11 runs   (  126.07 ms per token,     7.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1389.45 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  25%|▎| 5/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1322.59 ms /    11 runs   (  120.24 ms per token,     8.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1325.30 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  30%|▎| 6/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1318.02 ms /    11 runs   (  119.82 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1320.51 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  35%|▎| 7/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1281.49 ms /    11 runs   (  116.50 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1284.18 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  40%|▍| 8/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1429.14 ms /    11 runs   (  129.92 ms per token,     7.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1432.15 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  45%|▍| 9/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1639.31 ms /    11 runs   (  149.03 ms per token,     6.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1642.09 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  50%|▌| 10Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', '[mask]', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'hot', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1450.51 ms /    11 runs   (  131.86 ms per token,     7.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1453.33 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  55%|▌| 11Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', '[mask]', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1384.88 ms /    11 runs   (  125.90 ms per token,     7.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1389.40 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  60%|▌| 12Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1329.44 ms /    11 runs   (  120.86 ms per token,     8.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1332.24 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  65%|▋| 13Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1321.83 ms /    11 runs   (  120.17 ms per token,     8.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1324.28 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  70%|▋| 14Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1284.95 ms /    11 runs   (  116.81 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1287.41 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  75%|▊| 15Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', '[mask]', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'hot', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1310.25 ms /    11 runs   (  119.11 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1313.25 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  80%|▊| 16Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1194.78 ms /    10 runs   (  119.48 ms per token,     8.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1197.56 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  85%|▊| 17Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1349.47 ms /    11 runs   (  122.68 ms per token,     8.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1352.26 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  90%|▉| 18Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1317.33 ms /    11 runs   (  119.76 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1320.81 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  95%|▉| 19Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', '[mask]', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'hot', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1250.61 ms /    11 runs   (  113.69 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1254.33 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:   0%| | 0/20Llama.generate: 37 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     391.19 ms /    13 tokens (   30.09 ms per token,    33.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =     134.59 ms /     1 runs   (  134.59 ms per token,     7.43 tokens per second)\n",
      "llama_perf_context_print:       total time =     526.53 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =          0\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:   5%| | 1/20Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['summer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1452.35 ms /    11 runs   (  132.03 ms per token,     7.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1454.95 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  10%| | 2/20Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1366.63 ms /    11 runs   (  124.24 ms per token,     8.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1369.53 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  15%|▏| 3/20Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     369.82 ms /     3 runs   (  123.27 ms per token,     8.11 tokens per second)\n",
      "llama_perf_context_print:       total time =     370.83 ms /     4 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  20%|▏| 4/20Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     249.25 ms /     2 runs   (  124.63 ms per token,     8.02 tokens per second)\n",
      "llama_perf_context_print:       total time =     250.15 ms /     3 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  25%|▎| 5/20Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['summer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1302.03 ms /    11 runs   (  118.37 ms per token,     8.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1304.57 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  30%|▎| 6/20Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['he', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1287.71 ms /    11 runs   (  117.06 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1290.22 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  35%|▎| 7/20Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     234.91 ms /     2 runs   (  117.46 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =     235.54 ms /     3 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  40%|▍| 8/20Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['summer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     239.87 ms /     2 runs   (  119.94 ms per token,     8.34 tokens per second)\n",
      "llama_perf_context_print:       total time =     240.50 ms /     3 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  45%|▍| 9/20Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['summer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1257.54 ms /    11 runs   (  114.32 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1260.02 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  50%|▌| 10/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1231.60 ms /    11 runs   (  111.96 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1234.11 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  55%|▌| 11/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     224.21 ms /     2 runs   (  112.10 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =     225.08 ms /     3 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  60%|▌| 12/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['summer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1451.85 ms /    11 runs   (  131.99 ms per token,     7.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1454.52 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  65%|▋| 13/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     352.24 ms /     3 runs   (  117.41 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =     353.19 ms /     4 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  70%|▋| 14/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     236.49 ms /     2 runs   (  118.24 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =     237.15 ms /     3 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  75%|▊| 15/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['summer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     237.49 ms /     2 runs   (  118.74 ms per token,     8.42 tokens per second)\n",
      "llama_perf_context_print:       total time =     238.11 ms /     3 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  80%|▊| 16/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['summer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     238.94 ms /     2 runs   (  119.47 ms per token,     8.37 tokens per second)\n",
      "llama_perf_context_print:       total time =     239.57 ms /     3 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  85%|▊| 17/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['summer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     236.60 ms /     2 runs   (  118.30 ms per token,     8.45 tokens per second)\n",
      "llama_perf_context_print:       total time =     237.25 ms /     3 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  90%|▉| 18/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['summer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1276.67 ms /    11 runs   (  116.06 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1279.30 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  95%|▉| 19/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1245.58 ms /    11 runs   (  113.23 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1248.06 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:   0%| | 0/Llama.generate: 38 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     366.05 ms /    12 tokens (   30.50 ms per token,    32.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1196.62 ms /    10 runs   (  119.66 ms per token,     8.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1565.23 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:   5%| | 1/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1364.22 ms /    11 runs   (  124.02 ms per token,     8.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1366.89 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  10%| | 2/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1056.67 ms /     9 runs   (  117.41 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1058.82 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  15%|▏| 3/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['my', 'love', 'sweetens', 'our', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     374.26 ms /     3 runs   (  124.75 ms per token,     8.02 tokens per second)\n",
      "llama_perf_context_print:       total time =     375.08 ms /     4 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  20%|▏| 4/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1354.76 ms /    11 runs   (  123.16 ms per token,     8.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1357.38 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  25%|▎| 5/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1771.07 ms /    13 runs   (  136.24 ms per token,     7.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1774.14 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  30%|▎| 6/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['he', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'afternoons']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1487.13 ms /    12 runs   (  123.93 ms per token,     8.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1489.70 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  35%|▎| 7/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2251.15 ms /    19 runs   (  118.48 ms per token,     8.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    2255.61 ms /    20 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  40%|▍| 8/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['summer', 'days', 'bring', 'a', 'delightful', 'treat', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'afternoons']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1312.19 ms /    12 runs   (  109.35 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1315.51 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  45%|▍| 9/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['summer', 'evenings', 'are', 'perfect', 'for', 'enjoying', 'sweet', 'ice', 'cream']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1506.43 ms /    13 runs   (  115.88 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1509.35 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  50%|▌| 10Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['he', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'afternoons']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1254.69 ms /    11 runs   (  114.06 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1257.32 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  55%|▌| 11Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['he', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'nights']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1232.34 ms /    11 runs   (  112.03 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1234.91 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  60%|▌| 12Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1266.28 ms /    11 runs   (  115.12 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1268.71 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  65%|▋| 13Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['he', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1369.89 ms /    12 runs   (  114.16 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1372.74 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  70%|▋| 14Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'evenings']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1203.16 ms /    11 runs   (  109.38 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1205.72 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  75%|▊| 15Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1254.97 ms /    11 runs   (  114.09 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1257.40 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  80%|▊| 16Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1716.19 ms /    15 runs   (  114.41 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1719.72 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  85%|▊| 17Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['summer', 'evenings', 'are', 'perfect', 'for', 'enjoying', 'a', 'bowl', 'of', 'sweet', 'ice', 'cream']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1262.19 ms /    11 runs   (  114.74 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1264.71 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  90%|▉| 18Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1378.15 ms /    12 runs   (  114.85 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1380.86 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  95%|▉| 19Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['he', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'evenings']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1507.87 ms /    13 runs   (  115.99 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1510.76 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " •  \n",
      " • vanilla \n",
      " •  \n",
      " • vanilla vanilla\n",
      " • vanilla vanilla\n",
      " • they enjoy delightful\n",
      " •  \n",
      " •  \n",
      " • vanilla \n",
      " • i love eating\n",
      " •  \n",
      " • mmm vanilla ice cream is\n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • she enjoys the\n",
      " • vanilla vanilla\n",
      " • she enjoys vanilla\n",
      " • vanilla ice cream\n",
      " • scoop scrumptious\n",
      " •  \n",
      " • i vanilla\n",
      " • i to eat\n",
      " • my for\n",
      " • i my dog\n",
      " • i \n",
      " • i \n",
      " • i vanilla\n",
      " • i chocolate\n",
      " •  \n",
      " • i to eat\n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • i to eat\n",
      " • my for vanilla\n",
      " •  \n",
      " •  \n",
      " • i chocolate\n",
      " • i vanilla\n",
      " • i mango\n",
      " • i strawberry\n",
      " • i coconut\n",
      " • i coconut\n",
      " •  \n",
      " •  \n",
      " • honeybees clover\n",
      " • i coconut\n",
      " • i coconut\n",
      " • i coconut\n",
      " • i mango\n",
      " • i mango\n",
      " • i strawberry\n",
      " •  \n",
      " • i vanilla\n",
      " • i strawberry\n",
      " • i chocolate\n",
      " • i coconut\n",
      " • i vanilla\n",
      " •  \n",
      " • i cream\n",
      " • they cream\n",
      " •  \n",
      " • i cream\n",
      " •  \n",
      " • i cream\n",
      " • i cream\n",
      " •  \n",
      " • i cream\n",
      " • i cream\n",
      " • i cream\n",
      " • i cream\n",
      " •  \n",
      " • i cream\n",
      " •  \n",
      " •  \n",
      " • i cream\n",
      " • i cream\n",
      " • i cream\n",
      " •  \n",
      " • vanilla during\n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • i on\n",
      " •  \n",
      " • i on\n",
      " • i on\n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • i during\n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • i hot\n",
      " •  \n",
      " • i hot\n",
      " •  \n",
      " • i hot\n",
      " • i hot\n",
      " • i hot\n",
      " • i hot\n",
      " • i warm\n",
      " •  \n",
      " •  \n",
      " • i warm\n",
      " • i hot\n",
      " • i hot\n",
      " •  \n",
      " • i hot\n",
      " • i hot\n",
      " • i hot\n",
      " •  \n",
      " • i warm\n",
      " •  \n",
      " • i summer\n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • i summer\n",
      " •  \n",
      " • i summer\n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • i summer\n",
      " • i summer\n",
      " •  \n",
      " • they days\n",
      " •  \n",
      " •  \n",
      " • i days\n",
      " •  \n",
      " • i evenings\n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • i days\n",
      " •  \n",
      " • i days\n",
      " •  \n",
      " • i afternoons\n",
      "\n",
      "Total generated: 160\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "love            PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "sweet           PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "ice             PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "cream           PMI=   -inf   P(x)=0.000  P(y)=0.650  P(xy)=0.000\n",
      "on              PMI=   -inf   P(x)=0.000  P(y)=0.150  P(xy)=0.000\n",
      "warm            PMI=   -inf   P(x)=0.000  P(y)=0.150  P(xy)=0.000\n",
      "summer          PMI=   -inf   P(x)=0.000  P(y)=0.250  P(xy)=0.000\n",
      "days            PMI=   -inf   P(x)=0.000  P(y)=0.200  P(xy)=0.000\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 1   |   Anchor word: 'love'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:   0%| | 0/Llama.generate: 32 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     531.01 ms /    18 tokens (   29.50 ms per token,    33.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2306.20 ms /    20 runs   (  115.31 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    2842.36 ms /    38 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:   5%| | 1/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['on', 'warm', 'summer', 'days', 'the', 'sun', 'creates', 'the', 'perfect', 'atmosphere', 'for', 'enjoying', 'scrumptious', 'ice', 'cream']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1950.56 ms /    15 runs   (  130.04 ms per token,     7.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1954.35 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  10%| | 2/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1490.36 ms /    13 runs   (  114.64 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1493.23 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  15%|▏| 3/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'to', 'eat', 'delicious', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1358.26 ms /    12 runs   (  113.19 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1360.91 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  20%|▏| 4/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['we', 'enjoy', 'eating', 'delicious', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1527.37 ms /    14 runs   (  109.10 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1530.43 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  25%|▎| 5/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1343.64 ms /    12 runs   (  111.97 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1346.32 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  30%|▎| 6/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1321.62 ms /    12 runs   (  110.13 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1324.23 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  35%|▎| 7/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1643.02 ms /    15 runs   (  109.53 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1646.34 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  40%|▍| 8/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1306.55 ms /    12 runs   (  108.88 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1309.25 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  45%|▍| 9/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    4367.89 ms /    40 runs   (  109.20 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    4377.55 ms /    41 tokens\n",
      "llama_perf_context_print:    graphs reused =         39\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  50%|▌| 10Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3448.55 ms /    31 runs   (  111.24 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    3456.06 ms /    32 tokens\n",
      "llama_perf_context_print:    graphs reused =         29\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  55%|▌| 11Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1328.14 ms /    12 runs   (  110.68 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1331.16 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  60%|▌| 12Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1601.11 ms /    14 runs   (  114.37 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1604.31 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  65%|▋| 13Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1511.35 ms /    13 runs   (  116.26 ms per token,     8.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1514.21 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  70%|▋| 14Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'to', 'eat', 'delicious', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1200.94 ms /    11 runs   (  109.18 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1203.39 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  75%|▊| 15Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1287.69 ms /    11 runs   (  117.06 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1290.14 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  80%|▊| 16Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1144.00 ms /    10 runs   (  114.40 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1146.45 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  85%|▊| 17Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['vanilla', 'rich', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1218.68 ms /    11 runs   (  110.79 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1221.18 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  90%|▉| 18Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1370.46 ms /    12 runs   (  114.20 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1373.26 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] [MASK] sweet ice cream on warm summer days:  95%|▉| 19Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1385.00 ms /    12 runs   (  115.42 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1387.92 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:   0%| |Llama.generate: 29 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     544.92 ms /    21 tokens (   25.95 ms per token,    38.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1204.91 ms /     9 runs   (  133.88 ms per token,     7.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1752.43 ms /    30 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:   5%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1223.75 ms /     9 runs   (  135.97 ms per token,     7.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1225.92 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  10%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1491.34 ms /    11 runs   (  135.58 ms per token,     7.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1493.93 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  15%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1579.70 ms /    12 runs   (  131.64 ms per token,     7.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1582.39 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  20%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1726.32 ms /    13 runs   (  132.79 ms per token,     7.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1729.89 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  25%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1820.09 ms /    14 runs   (  130.01 ms per token,     7.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1823.27 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  30%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1303.27 ms /    10 runs   (  130.33 ms per token,     7.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1305.46 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  35%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1960.83 ms /    15 runs   (  130.72 ms per token,     7.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1964.15 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  40%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1336.18 ms /    11 runs   (  121.47 ms per token,     8.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1338.61 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  45%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1368.78 ms /    11 runs   (  124.43 ms per token,     8.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1371.23 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  50%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1278.20 ms /    11 runs   (  116.20 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1280.85 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  55%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', '[mask]', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['children', 'enjoy', '[ice', 'cream]', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1083.64 ms /    10 runs   (  108.36 ms per token,     9.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1085.98 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  60%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1011.84 ms /     9 runs   (  112.43 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1014.11 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  65%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     999.61 ms /     9 runs   (  111.07 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1001.64 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  70%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1128.07 ms /    10 runs   (  112.81 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1130.33 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  75%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1227.78 ms /    11 runs   (  111.62 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1230.23 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  80%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1592.49 ms /    14 runs   (  113.75 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1595.68 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  85%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1317.27 ms /    12 runs   (  109.77 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1320.02 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  90%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', '[mask]', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['children', 'enjoy', '[ice', 'cream]', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1514.27 ms /    14 runs   (  108.16 ms per token,     9.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1517.87 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  95%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     988.51 ms /     9 runs   (  109.83 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =     990.53 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:   0%|Llama.generate: 33 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     458.35 ms /    17 tokens (   26.96 ms per token,    37.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1756.38 ms /    14 runs   (  125.46 ms per token,     7.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    2218.18 ms /    31 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:   5%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1154.71 ms /    10 runs   (  115.47 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1157.08 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  10%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1237.42 ms /    11 runs   (  112.49 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1240.32 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  15%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     995.22 ms /     9 runs   (  110.58 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =     997.57 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  20%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1218.89 ms /    11 runs   (  110.81 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1221.35 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  25%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1126.04 ms /    10 runs   (  112.60 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1128.39 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  30%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1040.86 ms /     9 runs   (  115.65 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1042.90 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  35%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1212.65 ms /    11 runs   (  110.24 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1215.08 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  40%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1277.24 ms /    11 runs   (  116.11 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1279.80 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  45%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1143.85 ms /    10 runs   (  114.38 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1146.08 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  50%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1198.46 ms /    11 runs   (  108.95 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1200.88 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  55%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1367.62 ms /    12 runs   (  113.97 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1370.52 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  60%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1136.69 ms /    10 runs   (  113.67 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1139.02 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  65%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     998.33 ms /     9 runs   (  110.93 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1000.36 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  70%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1130.30 ms /    10 runs   (  113.03 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1132.54 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  75%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1131.19 ms /    10 runs   (  113.12 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1133.38 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  80%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1245.26 ms /    11 runs   (  113.21 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1247.69 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  85%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1603.63 ms /    15 runs   (  106.91 ms per token,     9.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1606.92 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  90%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1132.07 ms /    10 runs   (  113.21 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1134.34 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  95%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1241.19 ms /    11 runs   (  112.84 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1243.66 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:   0%| |Llama.generate: 34 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     440.06 ms /    16 tokens (   27.50 ms per token,    36.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =     979.33 ms /     9 runs   (  108.81 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1421.67 ms /    25 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:   5%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1080.38 ms /    10 runs   (  108.04 ms per token,     9.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1082.66 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  10%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     974.84 ms /     9 runs   (  108.32 ms per token,     9.23 tokens per second)\n",
      "llama_perf_context_print:       total time =     976.82 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  15%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1103.47 ms /    10 runs   (  110.35 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1105.71 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  20%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1087.15 ms /    10 runs   (  108.71 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1089.36 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  25%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1180.73 ms /    11 runs   (  107.34 ms per token,     9.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1183.20 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  30%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     983.65 ms /     9 runs   (  109.29 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =     985.66 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  35%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1100.12 ms /    10 runs   (  110.01 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1102.32 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  40%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     988.93 ms /     9 runs   (  109.88 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =     990.96 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  45%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1086.78 ms /    10 runs   (  108.68 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1089.33 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  50%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     967.55 ms /     9 runs   (  107.51 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =     970.01 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  55%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1068.62 ms /    10 runs   (  106.86 ms per token,     9.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1070.84 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  60%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1186.99 ms /    11 runs   (  107.91 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1189.56 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  65%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1121.11 ms /    10 runs   (  112.11 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1123.39 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  70%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1100.25 ms /    10 runs   (  110.02 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1102.55 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  75%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1094.96 ms /    10 runs   (  109.50 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1097.17 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  80%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1116.16 ms /    10 runs   (  111.62 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1118.32 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  85%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1127.85 ms /    10 runs   (  112.78 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1130.26 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  90%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1079.68 ms /    10 runs   (  107.97 ms per token,     9.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1081.99 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  95%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1131.21 ms /    10 runs   (  113.12 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1133.48 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:   0%Llama.generate: 35 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     550.10 ms /    15 tokens (   36.67 ms per token,    27.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1151.42 ms /     9 runs   (  127.94 ms per token,     7.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1703.89 ms /    24 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:   5%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1123.32 ms /    10 runs   (  112.33 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1125.52 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  10%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1234.30 ms /    11 runs   (  112.21 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1236.88 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  15%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1219.81 ms /    10 runs   (  121.98 ms per token,     8.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1222.04 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  20%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1129.99 ms /    10 runs   (  113.00 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1132.40 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  25%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1151.41 ms /    10 runs   (  115.14 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1154.05 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  30%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1136.01 ms /    10 runs   (  113.60 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1138.30 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  35%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1233.80 ms /    11 runs   (  112.16 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1236.57 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  40%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1173.72 ms /    11 runs   (  106.70 ms per token,     9.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1176.14 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  45%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1391.77 ms /    12 runs   (  115.98 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1394.50 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  50%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1373.25 ms /    10 runs   (  137.33 ms per token,     7.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1375.49 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  55%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1193.74 ms /    10 runs   (  119.37 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1196.23 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  60%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1299.46 ms /    11 runs   (  118.13 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1302.18 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  65%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1197.83 ms /    10 runs   (  119.78 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1200.00 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  70%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1359.44 ms /    12 runs   (  113.29 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1362.07 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  75%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1245.36 ms /    11 runs   (  113.21 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1247.77 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  80%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1141.34 ms /    10 runs   (  114.13 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1143.52 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  85%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1119.57 ms /    10 runs   (  111.96 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1121.79 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  90%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1220.17 ms /    11 runs   (  110.92 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1222.60 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  95%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1225.38 ms /    11 runs   (  111.40 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1227.77 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:   0%| Llama.generate: 36 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     537.64 ms /    14 tokens (   38.40 ms per token,    26.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1211.15 ms /    11 runs   (  110.10 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1751.67 ms /    25 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:   5%| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1216.27 ms /    10 runs   (  121.63 ms per token,     8.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1218.78 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  10%| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1118.65 ms /    10 runs   (  111.87 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1120.91 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  15%|▏Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1214.49 ms /    11 runs   (  110.41 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1216.93 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  20%|▏Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1203.88 ms /    11 runs   (  109.44 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1206.40 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  25%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1398.28 ms /    13 runs   (  107.56 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1401.44 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  30%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1074.37 ms /    10 runs   (  107.44 ms per token,     9.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1076.74 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  35%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1179.35 ms /    11 runs   (  107.21 ms per token,     9.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1181.95 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  40%|▍Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1490.59 ms /    14 runs   (  106.47 ms per token,     9.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1493.67 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  45%|▍Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1279.22 ms /    12 runs   (  106.60 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1281.91 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  50%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1188.68 ms /    11 runs   (  108.06 ms per token,     9.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1191.12 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  55%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1197.78 ms /    11 runs   (  108.89 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1200.25 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  60%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1180.33 ms /    11 runs   (  107.30 ms per token,     9.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1182.80 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  65%|▋Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1613.94 ms /    15 runs   (  107.60 ms per token,     9.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1617.56 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  70%|▋Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     977.14 ms /     9 runs   (  108.57 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =     979.16 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  75%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1086.82 ms /    10 runs   (  108.68 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1089.26 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  80%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1291.39 ms /    12 runs   (  107.62 ms per token,     9.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1294.26 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  85%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1077.52 ms /    10 runs   (  107.75 ms per token,     9.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1079.95 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  90%|▉Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1334.56 ms /    12 runs   (  111.21 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1337.44 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  95%|▉Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1131.63 ms /    10 runs   (  113.16 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1133.86 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:   0%| | Llama.generate: 37 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     600.70 ms /    13 tokens (   46.21 ms per token,    21.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1260.40 ms /    12 runs   (  105.03 ms per token,     9.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1864.20 ms /    25 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:   5%| | Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1402.90 ms /    13 runs   (  107.92 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1405.73 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  10%| | Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1207.76 ms /    11 runs   (  109.80 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1210.19 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  15%|▏| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1227.28 ms /    11 runs   (  111.57 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1229.83 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  20%|▏| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1213.67 ms /    11 runs   (  110.33 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1216.09 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  25%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1223.38 ms /    11 runs   (  111.22 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1225.82 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  30%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1759.22 ms /    16 runs   (  109.95 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1763.22 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  35%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1210.74 ms /    11 runs   (  110.07 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1213.15 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  40%|▍| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1207.53 ms /    11 runs   (  109.78 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1209.98 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  45%|▍| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1200.36 ms /    11 runs   (  109.12 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1202.82 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  50%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1115.78 ms /    10 runs   (  111.58 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1118.00 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  55%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1238.01 ms /    11 runs   (  112.55 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1240.45 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  60%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1052.97 ms /    10 runs   (  105.30 ms per token,     9.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1055.38 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  65%|▋| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1242.07 ms /    11 runs   (  112.92 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1244.60 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  70%|▋| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1196.58 ms /    11 runs   (  108.78 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1198.99 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  75%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1186.81 ms /    11 runs   (  107.89 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1189.44 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  80%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1198.38 ms /    11 runs   (  108.94 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1200.86 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  85%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1214.50 ms /    11 runs   (  110.41 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1217.23 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  90%|▉| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1386.75 ms /    11 runs   (  126.07 ms per token,     7.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1389.32 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  95%|▉| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1139.40 ms /    10 runs   (  113.94 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1141.64 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:   0%| Llama.generate: 38 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     703.60 ms /    12 tokens (   58.63 ms per token,    17.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1222.13 ms /    10 runs   (  122.21 ms per token,     8.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1928.22 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:   5%| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1149.17 ms /    10 runs   (  114.92 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1151.38 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  10%| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1165.27 ms /    10 runs   (  116.53 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1167.71 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  15%|▏Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1622.84 ms /    14 runs   (  115.92 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1625.93 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  20%|▏Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1842.71 ms /    16 runs   (  115.17 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1846.21 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  25%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1193.16 ms /    10 runs   (  119.32 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1195.46 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  30%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1285.61 ms /    11 runs   (  116.87 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1288.05 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  35%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1248.25 ms /    11 runs   (  113.48 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1250.69 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  40%|▍Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1169.15 ms /    10 runs   (  116.91 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1171.38 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  45%|▍Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1145.46 ms /    10 runs   (  114.55 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1147.70 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  50%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1159.78 ms /    10 runs   (  115.98 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1161.97 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  55%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1169.41 ms /    10 runs   (  116.94 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1171.65 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  60%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1318.83 ms /    11 runs   (  119.89 ms per token,     8.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1321.25 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  65%|▋Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1274.89 ms /    11 runs   (  115.90 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1277.53 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  70%|▋Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2394.51 ms /    21 runs   (  114.02 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    2399.48 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =         20\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  75%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1145.71 ms /    10 runs   (  114.57 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1147.97 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  80%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1437.89 ms /    13 runs   (  110.61 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1440.77 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  85%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1281.56 ms /    11 runs   (  116.51 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1284.42 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  90%|▉Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1113.99 ms /    10 runs   (  111.40 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1116.35 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  95%|▉Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1265.53 ms /    12 runs   (  105.46 ms per token,     9.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1268.20 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " •  \n",
      " • scoop scrumptious\n",
      " •  \n",
      " •  \n",
      " • mouthwatering mild\n",
      " • i savor\n",
      " • she enjoys\n",
      " • my i absolutely adore\n",
      " • vanilla vanilla\n",
      " • the word best and scoop can replace the [mask] in the sentence the sentence will be the best scoop of\n",
      " • the word yummy can be used to replace the [mask] in the sentence yummy\n",
      " • delicious vanilla\n",
      " • tasty scrumptious\n",
      " •  \n",
      " • she savors\n",
      " • vanilla delicious\n",
      " •  \n",
      " • taste delicious\n",
      " • i love eating\n",
      " • vanilla vanilla\n",
      " • love eating\n",
      " • enjoy \n",
      " • love eating\n",
      " • love to enjoy\n",
      " • relish [mask]\n",
      " • adore indulging in\n",
      " • love enjoying\n",
      " • relish [enjoy]\n",
      " • devour \n",
      " • love eating\n",
      " •  \n",
      " • love eating\n",
      " • love \n",
      " • love \n",
      " • enjoy eating\n",
      " • love enjoying\n",
      " • love [eating]\n",
      " •  \n",
      " • enjoy [enjoy]\n",
      " • enjoy \n",
      " • love vanilla\n",
      " • adore \n",
      " • savor \n",
      " • enjoy \n",
      " • enjoy ice\n",
      " • devour \n",
      " • love \n",
      " • enjoy vanilla\n",
      " • eat delicious \n",
      " • relish \n",
      " • eat delicious \n",
      " • enjoy mango\n",
      " • enjoy \n",
      " • enjoy \n",
      " • adore \n",
      " • adore \n",
      " • love vanilla\n",
      " • love vanilla\n",
      " • adore \n",
      " • enjoy ice\n",
      " • enjoy \n",
      " • love cream\n",
      " • love \n",
      " • love cream\n",
      " • enjoy cream\n",
      " • enjoy cream\n",
      " • eat \n",
      " • enjoy cream\n",
      " • eat \n",
      " • adore \n",
      " • love \n",
      " • love cream\n",
      " • eat cream\n",
      " • enjoy cream\n",
      " • love cream\n",
      " • relish \n",
      " • enjoy \n",
      " • crave \n",
      " • enjoy cream\n",
      " • love cream\n",
      " • love during\n",
      " • love during\n",
      " • adore on\n",
      " • love during\n",
      " • love on\n",
      " • love during\n",
      " • love on\n",
      " • enjoy during\n",
      " • adore during\n",
      " • adore during\n",
      " • enjoy on\n",
      " • love on\n",
      " • love during\n",
      " • love during\n",
      " • relish during\n",
      " • love on\n",
      " • love on\n",
      " • enjoy during\n",
      " • adore during\n",
      " • adore on\n",
      " • adore sunny\n",
      " • love hot\n",
      " • love warm\n",
      " • love sunny\n",
      " • enjoy sunny\n",
      " • relish sunny\n",
      " • enjoy hot\n",
      " • love sunny\n",
      " • love hot\n",
      " • enjoy sunny\n",
      " • love sunny\n",
      " • love sunny\n",
      " • adore hot\n",
      " • enjoy hot\n",
      " • love \n",
      " • enjoy warm\n",
      " • devour sunny\n",
      " • love hot\n",
      " • adore sunny\n",
      " • love hot\n",
      " • eagerly enjoy sunny\n",
      " • eagerly enjoy sunny\n",
      " • love sunny\n",
      " • love sunny\n",
      " • love sunny\n",
      " • enjoy summer\n",
      " • enjoy summer\n",
      " • adore summer\n",
      " • love sunny\n",
      " • enjoy sunny\n",
      " • love summer\n",
      " • enjoy sunny\n",
      " • love summer\n",
      " • love sunny\n",
      " • love sunny\n",
      " • love sunny\n",
      " • love summer\n",
      " • love sunny\n",
      " • love sunny\n",
      " • enjoy summer\n",
      " • happily enjoy days\n",
      " • love days\n",
      " • love days\n",
      " • love days\n",
      " • enjoy evenings\n",
      " • love days\n",
      " • enjoy days\n",
      " • adore days\n",
      " • enjoy days\n",
      " • enjoy days\n",
      " • love days\n",
      " • love days\n",
      " • adore days\n",
      " • adore days\n",
      " • sentence with appropriate words is children eagerly savor days\n",
      " • eat days\n",
      " • eagerly enjoy days\n",
      " • happily enjoy days\n",
      " • love days\n",
      " • happily enjoy days\n",
      "\n",
      "Total generated: 160\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "children        PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "sweet           PMI=   -inf   P(x)=0.500  P(y)=0.000  P(xy)=0.000\n",
      "ice             PMI=   -inf   P(x)=0.200  P(y)=0.100  P(xy)=0.000\n",
      "cream           PMI=  0.377   P(x)=0.350  P(y)=0.550  P(xy)=0.250\n",
      "on              PMI=  0.184   P(x)=0.550  P(y)=0.400  P(xy)=0.250\n",
      "warm            PMI=  0.000   P(x)=0.500  P(y)=0.100  P(xy)=0.050\n",
      "summer          PMI= -0.485   P(x)=0.600  P(y)=0.350  P(xy)=0.150\n",
      "days            PMI=  0.074   P(x)=0.350  P(y)=0.950  P(xy)=0.350\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 2   |   Anchor word: 'sweet'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:   0%| | 0/2Llama.generate: 29 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     789.30 ms /    21 tokens (   37.59 ms per token,    26.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1221.98 ms /    10 runs   (  122.20 ms per token,     8.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    2014.38 ms /    31 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:   5%| | 1/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['john', 'loves', 'chocolate', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     644.51 ms /     5 runs   (  128.90 ms per token,     7.76 tokens per second)\n",
      "llama_perf_context_print:       total time =     646.03 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  10%| | 2/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['he', 'loves', 'to', 'eat']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1396.49 ms /    12 runs   (  116.37 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1399.41 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  15%|▏| 3/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1349.82 ms /    12 runs   (  112.49 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1353.01 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  20%|▏| 4/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['john', 'loves', 'vanilla', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1022.90 ms /     9 runs   (  113.66 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1026.00 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  25%|▎| 5/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['i', 'love', 'he', 'loves', 'she', 'likes', 'they', 'enjoy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1350.21 ms /    12 runs   (  112.52 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1353.06 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  30%|▎| 6/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['john', 'loves', 'vanilla', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1125.58 ms /    10 runs   (  112.56 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1128.00 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  35%|▎| 7/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1263.97 ms /    11 runs   (  114.91 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1266.76 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  40%|▍| 8/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1242.80 ms /    11 runs   (  112.98 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1245.33 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  45%|▍| 9/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1153.53 ms /    10 runs   (  115.35 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1155.86 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  50%|▌| 10/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1274.82 ms /    11 runs   (  115.89 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1277.73 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  55%|▌| 11/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1125.88 ms /    10 runs   (  112.59 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1128.35 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  60%|▌| 12/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'eating', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1102.41 ms /    10 runs   (  110.24 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1104.84 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  65%|▋| 13/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['he', 'loves', 'her', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1427.64 ms /    13 runs   (  109.82 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1430.66 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  70%|▋| 14/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1216.73 ms /    11 runs   (  110.61 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1219.16 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  75%|▊| 15/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     547.84 ms /     5 runs   (  109.57 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =     549.11 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  80%|▊| 16/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'to', 'eat']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1337.08 ms /    12 runs   (  111.42 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1340.19 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  85%|▊| 17/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'vanilla', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1329.82 ms /    12 runs   (  110.82 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1332.48 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  90%|▉| 18/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['he', 'loves', 'vanilla', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     576.54 ms /     5 runs   (  115.31 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =     577.80 ms /     6 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Processing prompt: [MASK] love [MASK] ice cream on warm summer days:  95%|▉| 19/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['i', 'love', 'to', 'eat']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1214.91 ms /    11 runs   (  110.45 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1217.37 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['john', 'loves', 'chocolate', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:   0%| |Llama.generate: 29 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     451.81 ms /    21 tokens (   21.51 ms per token,    46.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1218.55 ms /    10 runs   (  121.85 ms per token,     8.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1673.11 ms /    31 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:   5%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1197.09 ms /    10 runs   (  119.71 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1199.51 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  10%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1061.63 ms /     9 runs   (  117.96 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1063.91 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  15%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1310.09 ms /    11 runs   (  119.10 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1312.51 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  20%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1432.12 ms /    10 runs   (  143.21 ms per token,     6.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1434.31 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  25%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1044.67 ms /     9 runs   (  116.07 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1046.65 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  30%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1286.12 ms /    11 runs   (  116.92 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1288.52 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  35%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1438.14 ms /    12 runs   (  119.85 ms per token,     8.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1440.74 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  40%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', '[mask]', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['children', 'enjoy', '[ice', 'cream]', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1178.41 ms /    10 runs   (  117.84 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1180.56 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  45%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1196.10 ms /    10 runs   (  119.61 ms per token,     8.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1198.29 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  50%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1413.82 ms /    12 runs   (  117.82 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1416.42 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  55%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1153.04 ms /    10 runs   (  115.30 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1155.25 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  60%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1266.13 ms /    11 runs   (  115.10 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1268.77 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  65%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', '[mask]', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['children', 'enjoy', '[ice', 'cream]', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1152.29 ms /    10 runs   (  115.23 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1154.86 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  70%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1368.81 ms /    12 runs   (  114.07 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1371.43 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  75%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1396.19 ms /    12 runs   (  116.35 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1398.80 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  80%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', '[mask]', '[mask]', 'ice', 'cream', 'on', 'warm', 'summer', 'days'] ['children', 'enjoy', '[ice', 'cream]', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1123.24 ms /    10 runs   (  112.32 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1125.50 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  85%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1125.66 ms /    10 runs   (  112.57 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1127.84 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  90%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1190.96 ms /    11 runs   (  108.27 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1193.44 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] [MASK] ice cream on warm summer days:  95%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2225.64 ms /    20 runs   (  111.28 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    2230.07 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =         19\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:   0%| Llama.generate: 30 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     527.10 ms /    20 tokens (   26.35 ms per token,    37.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1185.38 ms /     9 runs   (  131.71 ms per token,     7.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1715.20 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:   5%| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1250.26 ms /    10 runs   (  125.03 ms per token,     8.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1252.50 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  10%| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1500.99 ms /    12 runs   (  125.08 ms per token,     7.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1503.96 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  15%|▏Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1278.44 ms /    10 runs   (  127.84 ms per token,     7.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1280.76 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  20%|▏Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1370.27 ms /    11 runs   (  124.57 ms per token,     8.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1372.77 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  25%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1303.58 ms /    10 runs   (  130.36 ms per token,     7.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1305.77 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  30%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1216.85 ms /    10 runs   (  121.68 ms per token,     8.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1219.48 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  35%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1405.46 ms /    12 runs   (  117.12 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1408.41 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  40%|▍Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1204.68 ms /    10 runs   (  120.47 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1206.94 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  45%|▍Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1174.41 ms /    10 runs   (  117.44 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1176.64 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  50%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1193.87 ms /    10 runs   (  119.39 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1196.14 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  55%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1185.55 ms /    10 runs   (  118.56 ms per token,     8.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1187.81 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  60%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1175.33 ms /    10 runs   (  117.53 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1177.54 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  65%|▋Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1285.75 ms /    11 runs   (  116.89 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1288.22 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  70%|▋Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1154.39 ms /    10 runs   (  115.44 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1156.77 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  75%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1172.00 ms /    10 runs   (  117.20 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1174.29 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  80%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1506.11 ms /    13 runs   (  115.85 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1509.16 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  85%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1178.35 ms /    10 runs   (  117.84 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1180.55 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  90%|▉Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1156.84 ms /    10 runs   (  115.68 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1159.06 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  95%|▉Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1085.42 ms /    10 runs   (  108.54 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1087.63 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:   0%| | Llama.generate: 34 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     427.75 ms /    16 tokens (   26.73 ms per token,    37.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1562.63 ms /    11 runs   (  142.06 ms per token,     7.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1993.48 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:   5%| | Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1515.39 ms /    12 runs   (  126.28 ms per token,     7.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1518.37 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  10%| | Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1515.33 ms /    12 runs   (  126.28 ms per token,     7.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1518.17 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  15%|▏| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1527.79 ms /    12 runs   (  127.32 ms per token,     7.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1530.68 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  20%|▏| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1458.06 ms /    12 runs   (  121.51 ms per token,     8.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1460.61 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  25%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1570.59 ms /    12 runs   (  130.88 ms per token,     7.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1573.66 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  30%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1406.16 ms /    11 runs   (  127.83 ms per token,     7.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1408.53 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  35%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1467.79 ms /    12 runs   (  122.32 ms per token,     8.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1470.37 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  40%|▍| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1467.76 ms /    12 runs   (  122.31 ms per token,     8.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1470.55 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  45%|▍| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1649.17 ms /    11 runs   (  149.92 ms per token,     6.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1652.43 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  50%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1475.16 ms /    11 runs   (  134.11 ms per token,     7.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1477.78 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  55%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1448.73 ms /    11 runs   (  131.70 ms per token,     7.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1452.08 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  60%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1510.27 ms /    12 runs   (  125.86 ms per token,     7.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1513.89 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  65%|▋| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1510.43 ms /    12 runs   (  125.87 ms per token,     7.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1513.25 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  70%|▋| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1467.52 ms /    12 runs   (  122.29 ms per token,     8.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1470.24 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  75%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1291.75 ms /    11 runs   (  117.43 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1294.23 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  80%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1404.32 ms /    12 runs   (  117.03 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1407.22 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  85%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1494.75 ms /    12 runs   (  124.56 ms per token,     8.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1497.68 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  90%|▉| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1531.28 ms /    12 runs   (  127.61 ms per token,     7.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1533.97 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  95%|▉| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1531.82 ms /    12 runs   (  127.65 ms per token,     7.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1534.44 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:   0%|Llama.generate: 35 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     607.81 ms /    15 tokens (   40.52 ms per token,    24.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1198.51 ms /    10 runs   (  119.85 ms per token,     8.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1808.85 ms /    25 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:   5%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1405.28 ms /    11 runs   (  127.75 ms per token,     7.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1408.05 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  10%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1422.75 ms /    11 runs   (  129.34 ms per token,     7.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1425.56 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  15%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1439.87 ms /    12 runs   (  119.99 ms per token,     8.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1442.54 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  20%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1509.88 ms /    12 runs   (  125.82 ms per token,     7.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1512.60 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  25%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1448.53 ms /    12 runs   (  120.71 ms per token,     8.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1451.26 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  30%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1320.78 ms /    11 runs   (  120.07 ms per token,     8.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1323.53 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  35%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1327.37 ms /    11 runs   (  120.67 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1330.01 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  40%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1401.82 ms /    12 runs   (  116.82 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1404.50 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  45%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1380.76 ms /    12 runs   (  115.06 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1383.64 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  50%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1244.71 ms /    11 runs   (  113.16 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1247.20 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  55%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1358.81 ms /    12 runs   (  113.23 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1361.46 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  60%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1256.29 ms /    11 runs   (  114.21 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1258.88 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  65%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1351.23 ms /    12 runs   (  112.60 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1353.91 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  70%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1346.26 ms /    12 runs   (  112.19 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1348.92 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  75%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1244.42 ms /    11 runs   (  113.13 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1246.84 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  80%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1386.05 ms /    12 runs   (  115.50 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1388.71 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  85%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1378.33 ms /    12 runs   (  114.86 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1381.01 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  90%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1371.49 ms /    12 runs   (  114.29 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1374.11 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  95%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1258.88 ms /    11 runs   (  114.44 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1261.33 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:   0%| |Llama.generate: 36 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     601.67 ms /    14 tokens (   42.98 ms per token,    23.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1560.17 ms /    14 runs   (  111.44 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    2165.14 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:   5%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1399.64 ms /    12 runs   (  116.64 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1402.28 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  10%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1393.21 ms /    12 runs   (  116.10 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1395.98 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  15%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1519.58 ms /    13 runs   (  116.89 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1522.72 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  20%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1283.88 ms /    11 runs   (  116.72 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1286.29 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  25%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1354.27 ms /    12 runs   (  112.86 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1356.88 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  30%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1587.77 ms /    14 runs   (  113.41 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1590.84 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  35%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1478.19 ms /    13 runs   (  113.71 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1481.05 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  40%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1449.87 ms /    13 runs   (  111.53 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1452.70 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  45%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1362.98 ms /    12 runs   (  113.58 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1365.62 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  50%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1477.21 ms /    13 runs   (  113.63 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1480.04 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  55%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1586.04 ms /    12 runs   (  132.17 ms per token,     7.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1588.70 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  60%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1126.82 ms /    10 runs   (  112.68 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1129.11 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  65%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1483.64 ms /    13 runs   (  114.13 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1486.48 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  70%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1476.91 ms /    13 runs   (  113.61 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1479.77 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  75%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1465.46 ms /    13 runs   (  112.73 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1468.32 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  80%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1451.11 ms /    13 runs   (  111.62 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1454.16 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  85%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1372.00 ms /    12 runs   (  114.33 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1374.93 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  90%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1460.38 ms /    13 runs   (  112.34 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1463.23 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  95%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1457.45 ms /    13 runs   (  112.11 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1460.27 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:   0%| | 0Llama.generate: 37 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     603.57 ms /    13 tokens (   46.43 ms per token,    21.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1220.56 ms /    11 runs   (  110.96 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1826.84 ms /    24 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:   5%| | 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1368.32 ms /    12 runs   (  114.03 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1371.02 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  10%| | 2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1328.84 ms /    12 runs   (  110.74 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1331.72 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  15%|▏| 3Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1446.42 ms /    13 runs   (  111.26 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1449.33 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  20%|▏| 4Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1373.64 ms /    12 runs   (  114.47 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1376.29 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  25%|▎| 5Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1388.12 ms /    12 runs   (  115.68 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1391.10 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  30%|▎| 6Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1374.80 ms /    12 runs   (  114.57 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1377.43 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  35%|▎| 7Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1480.25 ms /    13 runs   (  113.87 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1483.39 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  40%|▍| 8Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1482.02 ms /    13 runs   (  114.00 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1485.01 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  45%|▍| 9Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1484.05 ms /    13 runs   (  114.16 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1486.94 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  50%|▌| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1236.61 ms /    11 runs   (  112.42 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1239.06 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  55%|▌| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1370.71 ms /    12 runs   (  114.23 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1373.35 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  60%|▌| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1420.52 ms /    11 runs   (  129.14 ms per token,     7.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1422.99 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  65%|▋| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1344.91 ms /    12 runs   (  112.08 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1347.70 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  70%|▋| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1473.25 ms /    13 runs   (  113.33 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1476.14 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  75%|▊| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1461.40 ms /    13 runs   (  112.42 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1464.22 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  80%|▊| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1430.78 ms /    13 runs   (  110.06 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1433.59 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  85%|▊| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1356.71 ms /    12 runs   (  113.06 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1359.46 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  90%|▉| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1331.23 ms /    12 runs   (  110.94 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1333.85 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  95%|▉| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1242.11 ms /    11 runs   (  112.92 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1244.49 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:   0%| |Llama.generate: 38 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     743.59 ms /    12 tokens (   61.97 ms per token,    16.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1256.90 ms /    11 runs   (  114.26 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    2003.45 ms /    23 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:   5%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1376.29 ms /    12 runs   (  114.69 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1379.27 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  10%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1414.85 ms /    12 runs   (  117.90 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1417.54 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  15%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1399.26 ms /    12 runs   (  116.60 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1401.91 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  20%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1371.98 ms /    12 runs   (  114.33 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1374.64 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  25%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1378.22 ms /    12 runs   (  114.85 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1380.85 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  30%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1404.30 ms /    12 runs   (  117.03 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1407.03 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  35%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1457.71 ms /    12 runs   (  121.48 ms per token,     8.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1460.69 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  40%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1237.21 ms /    11 runs   (  112.47 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1239.65 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  45%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1371.27 ms /    12 runs   (  114.27 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1373.90 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  50%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1240.67 ms /    11 runs   (  112.79 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1243.10 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  55%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1256.96 ms /    11 runs   (  114.27 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1259.38 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  60%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1259.37 ms /    11 runs   (  114.49 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1261.98 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  65%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1328.09 ms /    12 runs   (  110.67 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1330.74 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  70%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1499.57 ms /    13 runs   (  115.35 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1502.81 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  75%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1390.45 ms /    12 runs   (  115.87 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1393.23 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  80%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1240.30 ms /    11 runs   (  112.75 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1242.76 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  85%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1372.23 ms /    12 runs   (  114.35 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1374.92 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  90%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1254.43 ms /    11 runs   (  114.04 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1256.91 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  95%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1233.19 ms /    11 runs   (  112.11 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1236.15 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " •  \n",
      " •  \n",
      " • i vanilla\n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • i \n",
      " • i chocolate\n",
      " • i chocolate\n",
      " • i \n",
      " • i him\n",
      " •  \n",
      " •  \n",
      " • my for\n",
      " • i her\n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • relish \n",
      " • enjoy \n",
      " • enjoy \n",
      " • love eating\n",
      " • love \n",
      " • enjoy \n",
      " • love eating\n",
      " •  \n",
      " • love \n",
      " • love eating\n",
      " • love to enjoy\n",
      " • love love\n",
      " •  \n",
      " • love eating\n",
      " • adore eating\n",
      " •  \n",
      " • love eat\n",
      " • love eating\n",
      " • relish \n",
      " • love [eating] [enjoying]\n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • chocolate ice\n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " •  cream cones\n",
      " •  cream ice cream\n",
      " • popsicle \n",
      " •  cream cones\n",
      " • popsicle \n",
      " • popsicle \n",
      " • lemon cream\n",
      " • popsicle \n",
      " • popsicle \n",
      " • popsicle \n",
      " • lemon cream\n",
      " • lemon cream\n",
      " • popsicle \n",
      " • lemon pops\n",
      " •  cream ice cream\n",
      " •  cream ice\n",
      " • popsicle \n",
      " • lemon pops\n",
      " • lemon pops\n",
      " • popsicle \n",
      " • chocolate on\n",
      " • chocolate on\n",
      " • chocolate on\n",
      " • vanilla on\n",
      " • vanilla on\n",
      " • vanilla on\n",
      " • chocolate on\n",
      " • chocolate on\n",
      " • vanilla on\n",
      " • vanilla on\n",
      " • chocolate on\n",
      " • vanilla on\n",
      " • chocolate on\n",
      " • vanilla on\n",
      " • vanilla on\n",
      " • chocolate on\n",
      " • vanilla on\n",
      " • vanilla on\n",
      " • vanilla on\n",
      " • chocolate on\n",
      " • vanilla sweltering\n",
      " • chocolate sunny\n",
      " • chocolate sunny\n",
      " • vanilla sunny\n",
      " • chocolate sunny\n",
      " • vanilla hot\n",
      " • vanilla scorching\n",
      " • vanilla sunny\n",
      " • vanilla sunny\n",
      " • vanilla hot\n",
      " • vanilla sunny\n",
      " • chocolate sunny\n",
      " • chocolate hot\n",
      " • vanilla sunny\n",
      " • vanilla sunny\n",
      " • vanilla sunny\n",
      " • vanilla sunny\n",
      " • chocolate sunny\n",
      " • vanilla sunny\n",
      " • vanilla sunny\n",
      " • chocolate sunny\n",
      " • chocolate sunny\n",
      " • chocolate sunny\n",
      " • vanilla sunny\n",
      " • chocolate sunny\n",
      " • chocolate sunny\n",
      " • vanilla sunny\n",
      " • vanilla sunny\n",
      " • vanilla sunny\n",
      " • vanilla sunny\n",
      " • chocolate summer\n",
      " • chocolate sunny\n",
      " • chocolate sunny\n",
      " • chocolate sunny\n",
      " • vanilla sunny\n",
      " • vanilla sunny\n",
      " • vanilla sunny\n",
      " • chocolate sunny\n",
      " • chocolate sunny\n",
      " • chocolate sunny\n",
      " • vanilla days\n",
      " • vanilla days\n",
      " • vanilla days\n",
      " • vanilla days\n",
      " • vanilla days\n",
      " • vanilla days\n",
      " • vanilla days\n",
      " • vanilla days\n",
      " • chocolate days\n",
      " • vanilla days\n",
      " • chocolate days\n",
      " • chocolate days\n",
      " • chocolate days\n",
      " • chocolate evenings\n",
      " • vanilla evenings\n",
      " • chocolate evenings\n",
      " • chocolate days\n",
      " • vanilla days\n",
      " • chocolate days\n",
      " • chocolate days\n",
      "\n",
      "Total generated: 160\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "children        PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "love            PMI=   -inf   P(x)=0.000  P(y)=0.050  P(xy)=0.000\n",
      "ice             PMI=   -inf   P(x)=0.000  P(y)=0.050  P(xy)=0.000\n",
      "cream           PMI=   -inf   P(x)=0.000  P(y)=0.150  P(xy)=0.000\n",
      "on              PMI=   -inf   P(x)=0.000  P(y)=1.000  P(xy)=0.000\n",
      "warm            PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "summer          PMI=   -inf   P(x)=0.000  P(y)=0.050  P(xy)=0.000\n",
      "days            PMI=   -inf   P(x)=0.000  P(y)=0.850  P(xy)=0.000\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 3   |   Anchor word: 'ice'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:   0%| | 0Llama.generate: 29 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     790.92 ms /    21 tokens (   37.66 ms per token,    26.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1514.46 ms /    12 runs   (  126.21 ms per token,     7.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    2308.61 ms /    33 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:   5%| | 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1623.56 ms /    13 runs   (  124.89 ms per token,     8.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1626.79 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  10%| | 2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1479.92 ms /    12 runs   (  123.33 ms per token,     8.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1482.70 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  15%|▏| 3Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1417.12 ms /    12 runs   (  118.09 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1419.84 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  20%|▏| 4Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', '[mask]', 'cream', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'vanilla', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1415.36 ms /    12 runs   (  117.95 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1418.39 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  25%|▎| 5Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', '[mask]', 'cream', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'strawberry', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1441.58 ms /    12 runs   (  120.13 ms per token,     8.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1444.33 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  30%|▎| 6Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1531.05 ms /    13 runs   (  117.77 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1534.04 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  35%|▎| 7Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1407.05 ms /    12 runs   (  117.25 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1409.80 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  40%|▍| 8Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1433.13 ms /    12 runs   (  119.43 ms per token,     8.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1435.91 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  45%|▍| 9Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', '[mask]', 'cream', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'strawberry', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1410.12 ms /    12 runs   (  117.51 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1412.90 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  50%|▌| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', '[mask]', 'cream', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'mango', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1682.54 ms /    14 runs   (  120.18 ms per token,     8.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1685.73 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  55%|▌| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', '[mask]', 'cream', 'on', 'warm', 'summer', 'days'] ['vanessa', 'loves', 'sweet', 'coconut', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1541.82 ms /    13 runs   (  118.60 ms per token,     8.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1544.82 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  60%|▌| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', '[mask]', 'cream', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'coconut', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1418.50 ms /    12 runs   (  118.21 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1421.22 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  65%|▋| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1524.54 ms /    13 runs   (  117.27 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1527.53 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  70%|▋| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1433.12 ms /    12 runs   (  119.43 ms per token,     8.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1435.86 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  75%|▊| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', '[mask]', 'cream', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'strawberry', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1523.41 ms /    13 runs   (  117.19 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1526.35 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  80%|▊| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1554.22 ms /    13 runs   (  119.56 ms per token,     8.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1557.20 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  85%|▊| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1641.43 ms /    14 runs   (  117.25 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1644.61 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  90%|▉| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', '[mask]', 'cream', 'on', 'warm', 'summer', 'days'] ['vanilla', 'loves', 'sweet', 'caramel', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1412.02 ms /    12 runs   (  117.67 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1414.74 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet [MASK] cream on warm summer days:  95%|▉| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1533.19 ms /    13 runs   (  117.94 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1536.15 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:   0%|Llama.generate: 29 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     569.49 ms /    21 tokens (   27.12 ms per token,    36.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =     970.82 ms /     8 runs   (  121.35 ms per token,     8.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1542.44 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:   5%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1254.80 ms /    10 runs   (  125.48 ms per token,     7.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1257.05 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  10%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1254.31 ms /    10 runs   (  125.43 ms per token,     7.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1256.52 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  15%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1092.51 ms /     9 runs   (  121.39 ms per token,     8.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1094.52 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  20%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1390.68 ms /    11 runs   (  126.43 ms per token,     7.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1393.09 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  25%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1130.24 ms /     9 runs   (  125.58 ms per token,     7.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1132.27 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  30%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1825.90 ms /    15 runs   (  121.73 ms per token,     8.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1829.18 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  35%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1891.83 ms /    15 runs   (  126.12 ms per token,     7.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1895.12 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  40%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', '[mask]', 'sweet', '[mask]', 'cream', 'on', 'warm', 'summer', 'days'] ['children', 'love', 'sweet', 'caramel', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1076.04 ms /     9 runs   (  119.56 ms per token,     8.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1078.05 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  45%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1293.05 ms /    11 runs   (  117.55 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1295.48 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  50%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1308.47 ms /    11 runs   (  118.95 ms per token,     8.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1310.92 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  55%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', '[mask]', 'sweet', '[mask]', 'cream', 'on', 'warm', 'summer', 'days'] ['children', 'enjoy', 'sweet', 'caramel', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1074.40 ms /     9 runs   (  119.38 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1076.63 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  60%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1180.23 ms /    10 runs   (  118.02 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1182.43 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  65%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1190.94 ms /    10 runs   (  119.09 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1193.17 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  70%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1066.69 ms /     9 runs   (  118.52 ms per token,     8.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1068.70 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  75%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1068.49 ms /     9 runs   (  118.72 ms per token,     8.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1070.53 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  80%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1773.34 ms /    15 runs   (  118.22 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1776.62 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  85%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1185.48 ms /    10 runs   (  118.55 ms per token,     8.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1187.72 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  90%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1071.74 ms /     9 runs   (  119.08 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1073.74 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet [MASK] cream on warm summer days:  95%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1890.55 ms /    16 runs   (  118.16 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1894.10 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:   0%| Llama.generate: 30 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     533.64 ms /    20 tokens (   26.68 ms per token,    37.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1124.63 ms /     9 runs   (  124.96 ms per token,     8.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1660.72 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:   5%| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1183.91 ms /    10 runs   (  118.39 ms per token,     8.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1186.14 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  10%| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1217.90 ms /    10 runs   (  121.79 ms per token,     8.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1220.11 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  15%|▏Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1451.76 ms /    12 runs   (  120.98 ms per token,     8.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1454.84 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  20%|▏Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1181.92 ms /    10 runs   (  118.19 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1184.17 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  25%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1209.35 ms /    10 runs   (  120.93 ms per token,     8.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1211.60 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  30%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1219.21 ms /    10 runs   (  121.92 ms per token,     8.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1221.44 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  35%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1445.42 ms /    12 runs   (  120.45 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1448.28 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  40%|▍Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1190.61 ms /    10 runs   (  119.06 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1193.36 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  45%|▍Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1190.70 ms /    10 runs   (  119.07 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1192.96 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  50%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1177.08 ms /    10 runs   (  117.71 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1179.27 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  55%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1534.40 ms /    13 runs   (  118.03 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1537.23 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  60%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1233.35 ms /    10 runs   (  123.33 ms per token,     8.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1235.56 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  65%|▋Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1171.96 ms /    10 runs   (  117.20 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1174.18 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  70%|▋Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1177.57 ms /    10 runs   (  117.76 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1179.81 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  75%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1191.97 ms /    10 runs   (  119.20 ms per token,     8.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1194.13 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  80%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1185.77 ms /    10 runs   (  118.58 ms per token,     8.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1187.98 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  85%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1440.16 ms /    12 runs   (  120.01 ms per token,     8.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1442.76 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  90%|▉Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1193.69 ms /    10 runs   (  119.37 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1195.93 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] [MASK] cream on warm summer days:  95%|▉Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1172.15 ms /    10 runs   (  117.21 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1174.39 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:   0%| Llama.generate: 31 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     502.12 ms /    19 tokens (   26.43 ms per token,    37.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1234.42 ms /    11 runs   (  112.22 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1739.27 ms /    30 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:   5%| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1477.45 ms /    13 runs   (  113.65 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1480.52 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  10%| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1438.23 ms /    13 runs   (  110.63 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1442.02 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  15%|▏Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1335.43 ms /    12 runs   (  111.29 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1338.16 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  20%|▏Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1466.99 ms /    13 runs   (  112.85 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1469.87 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  25%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1287.13 ms /    11 runs   (  117.01 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1289.57 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  30%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1481.73 ms /    13 runs   (  113.98 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1484.62 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  35%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1376.70 ms /    12 runs   (  114.73 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1379.34 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  40%|▍Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1322.76 ms /    12 runs   (  110.23 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1325.42 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  45%|▍Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1494.59 ms /    13 runs   (  114.97 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1497.48 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  50%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1371.36 ms /    12 runs   (  114.28 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1374.02 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  55%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1531.24 ms /    14 runs   (  109.37 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1534.31 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  60%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1380.18 ms /    12 runs   (  115.02 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1382.91 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  65%|▋Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1369.37 ms /    12 runs   (  114.11 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1372.18 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  70%|▋Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1281.38 ms /    12 runs   (  106.78 ms per token,     9.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1284.13 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  75%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1360.54 ms /    12 runs   (  113.38 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1363.23 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  80%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1368.19 ms /    12 runs   (  114.02 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1371.08 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  85%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1291.43 ms /    12 runs   (  107.62 ms per token,     9.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1294.11 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  90%|▉Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1344.55 ms /    12 runs   (  112.05 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1347.22 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  95%|▉Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1559.07 ms /    12 runs   (  129.92 ms per token,     7.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1561.81 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:   0Llama.generate: 35 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     581.56 ms /    15 tokens (   38.77 ms per token,    25.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1399.26 ms /    12 runs   (  116.60 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1983.86 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:   5Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1278.24 ms /    11 runs   (  116.20 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1280.88 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  10Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1205.99 ms /    11 runs   (  109.64 ms per token,     9.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1208.63 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  15Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1213.08 ms /    11 runs   (  110.28 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1215.72 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  20Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1244.56 ms /    11 runs   (  113.14 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1246.99 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  25Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1320.05 ms /    12 runs   (  110.00 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1322.68 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  30Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1460.66 ms /    13 runs   (  112.36 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1463.46 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  35Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1474.08 ms /    13 runs   (  113.39 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1476.88 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  40Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1204.53 ms /    11 runs   (  109.50 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1206.96 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  45Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1363.79 ms /    12 runs   (  113.65 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1366.44 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  50Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1243.66 ms /    11 runs   (  113.06 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1246.31 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  55Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1211.57 ms /    11 runs   (  110.14 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1214.21 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  60Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1480.59 ms /    13 runs   (  113.89 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1483.45 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  65Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1237.67 ms /    11 runs   (  112.52 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1240.10 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  70Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1179.83 ms /    11 runs   (  107.26 ms per token,     9.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1182.24 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  75Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1229.26 ms /    11 runs   (  111.75 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1231.66 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  80Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1225.86 ms /    11 runs   (  111.44 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1228.34 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  85Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1205.10 ms /    11 runs   (  109.55 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1207.52 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  90Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1163.57 ms /    11 runs   (  105.78 ms per token,     9.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1165.99 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  95Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1371.86 ms /    12 runs   (  114.32 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1374.49 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:   0%|Llama.generate: 36 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     591.71 ms /    14 tokens (   42.26 ms per token,    23.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1162.47 ms /    10 runs   (  116.25 ms per token,     8.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1756.89 ms /    24 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:   5%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1431.61 ms /    12 runs   (  119.30 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1434.53 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  10%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1680.91 ms /    14 runs   (  120.06 ms per token,     8.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1683.96 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  15%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1360.21 ms /    12 runs   (  113.35 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1362.87 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  20%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1438.33 ms /    13 runs   (  110.64 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1441.14 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  25%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1345.41 ms /    12 runs   (  112.12 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1348.27 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  30%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1430.71 ms /    13 runs   (  110.05 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1433.71 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  35%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1420.44 ms /    13 runs   (  109.26 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1423.29 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  40%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1447.51 ms /    13 runs   (  111.35 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1450.35 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  45%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1340.69 ms /    11 runs   (  121.88 ms per token,     8.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1343.46 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  50%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1453.44 ms /    12 runs   (  121.12 ms per token,     8.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1456.34 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  55%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1459.34 ms /    12 runs   (  121.61 ms per token,     8.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1462.26 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  60%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1347.27 ms /    11 runs   (  122.48 ms per token,     8.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1349.99 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  65%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1446.70 ms /    12 runs   (  120.56 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1449.66 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  70%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1527.28 ms /    13 runs   (  117.48 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1530.44 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  75%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1616.30 ms /    14 runs   (  115.45 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1619.91 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  80%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1353.94 ms /    12 runs   (  112.83 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1357.07 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  85%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1468.62 ms /    13 runs   (  112.97 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1471.75 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  90%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1506.87 ms /    13 runs   (  115.91 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1510.02 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  95%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1378.12 ms /    12 runs   (  114.84 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1381.00 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:   0%| |Llama.generate: 37 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     650.40 ms /    13 tokens (   50.03 ms per token,    19.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1318.55 ms /    11 runs   (  119.87 ms per token,     8.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1971.97 ms /    24 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:   5%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1336.07 ms /    11 runs   (  121.46 ms per token,     8.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1338.69 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  10%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1638.32 ms /    12 runs   (  136.53 ms per token,     7.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1641.17 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  15%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1373.32 ms /    12 runs   (  114.44 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1376.19 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  20%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1321.96 ms /    11 runs   (  120.18 ms per token,     8.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1324.54 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  25%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1527.97 ms /    13 runs   (  117.54 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1531.77 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  30%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1418.11 ms /    12 runs   (  118.18 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1421.03 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  35%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1462.53 ms /    12 runs   (  121.88 ms per token,     8.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1465.42 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  40%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1456.76 ms /    12 runs   (  121.40 ms per token,     8.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1459.73 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  45%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1451.69 ms /    12 runs   (  120.97 ms per token,     8.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1455.12 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  50%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1307.73 ms /    11 runs   (  118.88 ms per token,     8.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1310.96 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  55%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1296.92 ms /    11 runs   (  117.90 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1299.28 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  60%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1417.80 ms /    12 runs   (  118.15 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1420.36 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  65%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1296.94 ms /    11 runs   (  117.90 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1299.27 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  70%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1417.59 ms /    12 runs   (  118.13 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1420.17 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  75%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1304.36 ms /    11 runs   (  118.58 ms per token,     8.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1306.73 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  80%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1690.21 ms /    14 runs   (  120.73 ms per token,     8.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1693.27 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  85%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1324.36 ms /    11 runs   (  120.40 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1326.95 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  90%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1455.10 ms /    12 runs   (  121.26 ms per token,     8.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1457.89 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  95%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1411.84 ms /    12 runs   (  117.65 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1414.58 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:   0%|Llama.generate: 38 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     685.91 ms /    12 tokens (   57.16 ms per token,    17.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1379.50 ms /    11 runs   (  125.41 ms per token,     7.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    2068.18 ms /    23 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:   5%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1301.87 ms /    11 runs   (  118.35 ms per token,     8.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1304.37 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  10%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1424.73 ms /    12 runs   (  118.73 ms per token,     8.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1427.47 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  15%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1313.51 ms /    11 runs   (  119.41 ms per token,     8.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1316.03 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  20%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1297.97 ms /    11 runs   (  118.00 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1300.69 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  25%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1420.67 ms /    12 runs   (  118.39 ms per token,     8.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1423.39 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  30%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1324.23 ms /    11 runs   (  120.38 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1326.74 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  35%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1383.00 ms /    12 runs   (  115.25 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1385.69 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  40%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1221.25 ms /    11 runs   (  111.02 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1223.78 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  45%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1180.30 ms /    11 runs   (  107.30 ms per token,     9.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1183.05 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  50%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1156.77 ms /    11 runs   (  105.16 ms per token,     9.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1159.38 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  55%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1378.15 ms /    13 runs   (  106.01 ms per token,     9.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1381.07 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  60%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1283.33 ms /    12 runs   (  106.94 ms per token,     9.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1286.24 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  65%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1174.22 ms /    11 runs   (  106.75 ms per token,     9.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1176.85 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  70%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1331.15 ms /    13 runs   (  102.40 ms per token,     9.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1334.08 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  75%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1280.84 ms /    12 runs   (  106.74 ms per token,     9.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1283.51 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  80%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1174.16 ms /    11 runs   (  106.74 ms per token,     9.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1176.65 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  85%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1225.34 ms /    12 runs   (  102.11 ms per token,     9.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1228.03 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  90%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1268.40 ms /    12 runs   (  105.70 ms per token,     9.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1271.16 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  95%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1388.74 ms /    13 runs   (  106.83 ms per token,     9.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1391.75 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • i coconut\n",
      " • i vanilla ice\n",
      " • i mango\n",
      " •  \n",
      " •  \n",
      " • i vanilla\n",
      " • i coconut\n",
      " • i vanilla\n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • i mango\n",
      " • i vanilla ice\n",
      " •  \n",
      " • i vanilla ice\n",
      " • honey vanilla\n",
      " •  \n",
      " • i mango\n",
      " • i coconut\n",
      " • love \n",
      " • enjoy \n",
      " • love \n",
      " • love \n",
      " • relish \n",
      " • enjoy \n",
      " • love vanilla\n",
      " •  \n",
      " • love \n",
      " • savor \n",
      " •  \n",
      " • enjoy \n",
      " • enjoy \n",
      " • love \n",
      " • love \n",
      " • enjoy \n",
      " • love vanilla\n",
      " • eat delicious \n",
      " • enjoy \n",
      " • love vanilla\n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • ice \n",
      " • popsicles \n",
      " • ice cream cones\n",
      " • ice cream cones\n",
      " • popsicles \n",
      " • ice cream cones\n",
      " • ice cream\n",
      " • ice cream cones\n",
      " • popsicles \n",
      " • popsicles \n",
      " • ice cream cones\n",
      " • popsicles \n",
      " • lemonade popsicles\n",
      " • popsicles \n",
      " • popsicles \n",
      " • popsicles \n",
      " • popsicles \n",
      " • popsicles \n",
      " • popsicles \n",
      " • popsicles \n",
      " • candy apples\n",
      " • coconut on\n",
      " • chocolate on\n",
      " • ice on\n",
      " • ice on\n",
      " • ice during\n",
      " • vanilla on\n",
      " • coconut on\n",
      " • coconut on\n",
      " • ice on\n",
      " • vanilla on\n",
      " • ice on\n",
      " • chocolate on\n",
      " • coconut on\n",
      " • ice on\n",
      " • ice on\n",
      " • ice on\n",
      " • ice on\n",
      " • ice on\n",
      " • chocolate on\n",
      " • vanilla on\n",
      " • chocolate hot\n",
      " • vanilla warm\n",
      " • cotton sweltering\n",
      " • strawberry warm\n",
      " • strawberry sunny\n",
      " • strawberry warm\n",
      " • vanilla sunny\n",
      " • strawberry sunny\n",
      " • vanilla sunny\n",
      " • ice hot\n",
      " • strawberry hot\n",
      " • strawberry warm\n",
      " • ice hot\n",
      " • fruit refreshing\n",
      " • mango sunny\n",
      " • dessert scorching\n",
      " • vanilla warm\n",
      " • vanilla sunny\n",
      " • strawberry sunny\n",
      " • vanilla hot\n",
      " • chocolate sunny\n",
      " • chocolate summer\n",
      " • vanilla sunny\n",
      " • chocolate sunny\n",
      " • chocolate summer\n",
      " • vanilla sunny\n",
      " • vanilla summer\n",
      " • dessert summer\n",
      " • vanilla summer\n",
      " • vanilla summer\n",
      " • chocolate summer\n",
      " • chocolate summer\n",
      " • vanilla summer\n",
      " • chocolate summer\n",
      " • vanilla summer\n",
      " • chocolate summer\n",
      " • coconut sunny\n",
      " • chocolate summer\n",
      " • chocolate sunny\n",
      " • vanilla summer\n",
      " • dessert days\n",
      " • chocolate days\n",
      " • strawberry days\n",
      " • chocolate days\n",
      " • chocolate days\n",
      " • vanilla days\n",
      " • chocolate days\n",
      " • vanilla days\n",
      " • chocolate days\n",
      " • chocolate days\n",
      " • chocolate nights\n",
      " • coconut days\n",
      " • dessert days\n",
      " • chocolate days\n",
      " • coconut days\n",
      " • strawberry days\n",
      " • chocolate days\n",
      " • vanilla days\n",
      " • strawberry days\n",
      " • dessert evenings\n",
      "\n",
      "Total generated: 160\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "children        PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "love            PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "sweet           PMI=   -inf   P(x)=1.000  P(y)=0.000  P(xy)=0.000\n",
      "cream           PMI=  1.737   P(x)=0.300  P(y)=0.050  P(xy)=0.050\n",
      "on              PMI= -0.078   P(x)=0.500  P(y)=0.950  P(xy)=0.450\n",
      "warm            PMI=   -inf   P(x)=0.100  P(y)=0.250  P(xy)=0.000\n",
      "summer          PMI=   -inf   P(x)=0.000  P(y)=0.700  P(xy)=0.000\n",
      "days            PMI=   -inf   P(x)=0.000  P(y)=0.900  P(xy)=0.000\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 4   |   Anchor word: 'cream'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:   0%| | 0/2Llama.generate: 29 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     897.80 ms /    21 tokens (   42.75 ms per token,    23.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1430.57 ms /    10 runs   (  143.06 ms per token,     6.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    2331.12 ms /    31 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:   5%| | 1/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1460.09 ms /    11 runs   (  132.74 ms per token,     7.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1462.66 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  10%| | 2/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1247.88 ms /    10 runs   (  124.79 ms per token,     8.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1250.11 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  15%|▏| 3/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1430.28 ms /    11 runs   (  130.03 ms per token,     7.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1432.92 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  20%|▏| 4/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1429.97 ms /    11 runs   (  130.00 ms per token,     7.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1432.38 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  25%|▎| 5/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', '[mask]', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1606.86 ms /    11 runs   (  146.08 ms per token,     6.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1609.43 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  30%|▎| 6/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', '[mask]', 'on', 'warm', 'summer', 'days'] ['he', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1321.96 ms /    11 runs   (  120.18 ms per token,     8.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1324.61 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  35%|▎| 7/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', '[mask]', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1306.67 ms /    11 runs   (  118.79 ms per token,     8.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1309.34 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  40%|▍| 8/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', '[mask]', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1336.38 ms /    11 runs   (  121.49 ms per token,     8.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1339.06 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  45%|▍| 9/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1358.76 ms /    11 runs   (  123.52 ms per token,     8.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1361.70 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  50%|▌| 10/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', '[mask]', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1387.35 ms /    11 runs   (  126.12 ms per token,     7.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1390.35 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  55%|▌| 11/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', '[mask]', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1308.77 ms /    11 runs   (  118.98 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1311.41 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  60%|▌| 12/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', '[mask]', 'on', 'warm', 'summer', 'days'] ['he', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1306.99 ms /    11 runs   (  118.82 ms per token,     8.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1309.56 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  65%|▋| 13/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1320.03 ms /    11 runs   (  120.00 ms per token,     8.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1322.58 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  70%|▋| 14/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', '[mask]', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'tea', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1311.89 ms /    11 runs   (  119.26 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1314.52 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  75%|▊| 15/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1334.06 ms /    11 runs   (  121.28 ms per token,     8.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1336.74 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  80%|▊| 16/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', '[mask]', 'on', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1284.10 ms /    11 runs   (  116.74 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1286.79 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  85%|▊| 17/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1216.20 ms /    11 runs   (  110.56 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1218.89 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  90%|▉| 18/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1171.12 ms /    11 runs   (  106.47 ms per token,     9.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1174.45 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice [MASK] on warm summer days:  95%|▉| 19/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', '[mask]', 'on', 'warm', 'summer', 'days'] ['he', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1178.51 ms /    11 runs   (  107.14 ms per token,     9.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1182.29 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:   0%| |Llama.generate: 29 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     477.29 ms /    21 tokens (   22.73 ms per token,    44.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1116.87 ms /    10 runs   (  111.69 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1596.83 ms /    31 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:   5%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1106.26 ms /    10 runs   (  110.63 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1108.48 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  10%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1108.85 ms /    10 runs   (  110.88 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1111.05 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  15%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     997.42 ms /     9 runs   (  110.82 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =     999.41 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  20%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1225.89 ms /    11 runs   (  111.44 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1228.36 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  25%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1804.20 ms /    15 runs   (  120.28 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1807.69 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  30%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1032.87 ms /     9 runs   (  114.76 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1035.17 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  35%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1083.27 ms /     9 runs   (  120.36 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1085.36 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  40%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1203.67 ms /    10 runs   (  120.37 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1206.26 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  45%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1048.74 ms /     9 runs   (  116.53 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1051.10 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  50%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1246.29 ms /    11 runs   (  113.30 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1248.72 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  55%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1183.62 ms /    10 runs   (  118.36 ms per token,     8.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1185.86 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  60%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1372.12 ms /    12 runs   (  114.34 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1375.04 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  65%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1144.03 ms /    10 runs   (  114.40 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1146.22 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  70%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1182.54 ms /    10 runs   (  118.25 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1184.76 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  75%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1202.24 ms /    10 runs   (  120.22 ms per token,     8.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1204.65 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  80%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1062.41 ms /     9 runs   (  118.05 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1064.48 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  85%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1741.45 ms /    15 runs   (  116.10 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1744.79 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  90%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1190.64 ms /    10 runs   (  119.06 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1192.90 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice [MASK] on warm summer days:  95%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1152.74 ms /    10 runs   (  115.27 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1154.98 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:   0%| | Llama.generate: 30 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     503.40 ms /    20 tokens (   25.17 ms per token,    39.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =     941.32 ms /     8 runs   (  117.67 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1446.89 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:   5%| | Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1385.33 ms /    12 runs   (  115.44 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1388.80 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  10%| | Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1388.83 ms /    12 runs   (  115.74 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1391.73 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  15%|▏| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1296.59 ms /    11 runs   (  117.87 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1299.09 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  20%|▏| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1380.84 ms /    12 runs   (  115.07 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1383.63 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  25%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1377.37 ms /    12 runs   (  114.78 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1380.25 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  30%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1163.72 ms /    10 runs   (  116.37 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1166.23 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  35%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1381.41 ms /    12 runs   (  115.12 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1384.13 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  40%|▍| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1350.25 ms /    12 runs   (  112.52 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1352.95 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  45%|▍| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1379.32 ms /    12 runs   (  114.94 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1382.03 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  50%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1394.65 ms /    12 runs   (  116.22 ms per token,     8.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1397.35 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  55%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1516.65 ms /    11 runs   (  137.88 ms per token,     7.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1519.08 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  60%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1285.22 ms /    11 runs   (  116.84 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1287.84 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  65%|▋| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1408.38 ms /    12 runs   (  117.37 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1411.10 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  70%|▋| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1331.51 ms /    12 runs   (  110.96 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1334.08 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  75%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1418.34 ms /    12 runs   (  118.19 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1421.09 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  80%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1293.16 ms /    11 runs   (  117.56 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1295.55 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  85%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1333.96 ms /    12 runs   (  111.16 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1336.52 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  90%|▉| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1389.78 ms /    12 runs   (  115.81 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1392.65 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice [MASK] on warm summer days:  95%|▉| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1369.65 ms /    12 runs   (  114.14 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1372.36 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:   0%| Llama.generate: 31 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     567.54 ms /    19 tokens (   29.87 ms per token,    33.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1420.32 ms /    11 runs   (  129.12 ms per token,     7.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1990.97 ms /    30 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:   5%| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1541.26 ms /    12 runs   (  128.44 ms per token,     7.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1543.99 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  10%| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1480.17 ms /    12 runs   (  123.35 ms per token,     8.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1482.85 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  15%|▏Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1530.91 ms /    12 runs   (  127.58 ms per token,     7.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1533.71 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  20%|▏Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1535.12 ms /    12 runs   (  127.93 ms per token,     7.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1538.09 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  25%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1585.10 ms /    13 runs   (  121.93 ms per token,     8.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1587.95 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  30%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1367.33 ms /    12 runs   (  113.94 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1369.93 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  35%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1350.14 ms /    12 runs   (  112.51 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1352.91 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  40%|▍Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1290.93 ms /    12 runs   (  107.58 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1293.82 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  45%|▍Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1411.66 ms /    13 runs   (  108.59 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1414.52 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  50%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1299.21 ms /    12 runs   (  108.27 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1301.81 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  55%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1391.92 ms /    13 runs   (  107.07 ms per token,     9.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1394.79 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  60%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1331.69 ms /    12 runs   (  110.97 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1334.32 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  65%|▋Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1297.83 ms /    12 runs   (  108.15 ms per token,     9.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1300.42 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  70%|▋Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1344.80 ms /    12 runs   (  112.07 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1347.41 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  75%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1305.00 ms /    12 runs   (  108.75 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1307.70 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  80%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1341.02 ms /    12 runs   (  111.75 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1343.76 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  85%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1368.24 ms /    12 runs   (  114.02 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1371.06 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  90%|▉Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1329.50 ms /    12 runs   (  110.79 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1332.15 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] [MASK] on warm summer days:  95%|▉Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1343.78 ms /    12 runs   (  111.98 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1346.82 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:   0%|Llama.generate: 32 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     619.72 ms /    18 tokens (   34.43 ms per token,    29.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1232.25 ms /    10 runs   (  123.22 ms per token,     8.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1854.41 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:   5%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1305.28 ms /    11 runs   (  118.66 ms per token,     8.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1307.82 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  10%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1318.88 ms /    11 runs   (  119.90 ms per token,     8.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1321.41 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  15%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1369.77 ms /    11 runs   (  124.52 ms per token,     8.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1372.15 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  20%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1621.28 ms /    13 runs   (  124.71 ms per token,     8.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1624.07 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  25%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1605.10 ms /    13 runs   (  123.47 ms per token,     8.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1607.88 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  30%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1380.10 ms /    11 runs   (  125.46 ms per token,     7.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1382.54 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  35%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1383.39 ms /    11 runs   (  125.76 ms per token,     7.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1385.84 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  40%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1373.49 ms /    11 runs   (  124.86 ms per token,     8.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1375.83 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  45%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1390.40 ms /    11 runs   (  126.40 ms per token,     7.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1392.73 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  50%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1357.40 ms /    11 runs   (  123.40 ms per token,     8.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1359.81 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  55%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1387.02 ms /    11 runs   (  126.09 ms per token,     7.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1389.42 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  60%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1375.45 ms /    11 runs   (  125.04 ms per token,     8.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1377.81 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  65%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1582.12 ms /    11 runs   (  143.83 ms per token,     6.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1584.51 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  70%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1371.64 ms /    11 runs   (  124.69 ms per token,     8.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1374.22 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  75%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1288.71 ms /    11 runs   (  117.16 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1291.37 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  80%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1269.31 ms /    11 runs   (  115.39 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1271.75 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  85%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1254.32 ms /    11 runs   (  114.03 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1256.70 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  90%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1289.49 ms /    11 runs   (  117.23 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1291.92 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  95%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1255.17 ms /    11 runs   (  114.11 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1257.60 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:   0%| |Llama.generate: 36 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     587.00 ms /    14 tokens (   41.93 ms per token,    23.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1498.28 ms /    13 runs   (  115.25 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    2088.65 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:   5%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1248.56 ms /    11 runs   (  113.51 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1251.16 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  10%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1215.02 ms /    11 runs   (  110.46 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1217.56 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  15%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1199.46 ms /    11 runs   (  109.04 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1202.04 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  20%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1260.85 ms /    11 runs   (  114.62 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1263.59 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  25%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1215.93 ms /    11 runs   (  110.54 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1218.39 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  30%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1257.94 ms /    11 runs   (  114.36 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1260.40 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  35%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1259.73 ms /    11 runs   (  114.52 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1262.19 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  40%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1366.89 ms /    12 runs   (  113.91 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1369.59 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  45%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1226.04 ms /    11 runs   (  111.46 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1228.52 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  50%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1224.92 ms /    11 runs   (  111.36 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1227.50 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  55%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1238.18 ms /    11 runs   (  112.56 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1240.78 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  60%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1600.45 ms /    14 runs   (  114.32 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1603.57 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  65%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1261.11 ms /    11 runs   (  114.65 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1263.80 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  70%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1561.97 ms /    14 runs   (  111.57 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1565.15 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  75%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1353.28 ms /    12 runs   (  112.77 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1356.00 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  80%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1336.17 ms /    12 runs   (  111.35 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1338.87 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  85%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1209.30 ms /    11 runs   (  109.94 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1211.79 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  90%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1372.55 ms /    12 runs   (  114.38 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1375.29 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  95%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1256.77 ms /    11 runs   (  114.25 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1259.22 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:   0%| | 0Llama.generate: 37 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     655.47 ms /    13 tokens (   50.42 ms per token,    19.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1331.07 ms /    11 runs   (  121.01 ms per token,     8.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1989.57 ms /    24 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:   5%| | 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1336.12 ms /    11 runs   (  121.47 ms per token,     8.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1339.38 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  10%| | 2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1462.43 ms /    12 runs   (  121.87 ms per token,     8.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1465.16 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  15%|▏| 3Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1370.00 ms /    12 runs   (  114.17 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1373.01 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  20%|▏| 4Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1352.70 ms /    12 runs   (  112.73 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1355.61 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  25%|▎| 5Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1239.84 ms /    11 runs   (  112.71 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1242.37 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  30%|▎| 6Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1269.25 ms /    11 runs   (  115.39 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1272.85 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  35%|▎| 7Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1375.87 ms /    12 runs   (  114.66 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1378.64 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  40%|▍| 8Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1362.75 ms /    12 runs   (  113.56 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1365.43 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  45%|▍| 9Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1374.21 ms /    12 runs   (  114.52 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1377.25 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  50%|▌| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1256.42 ms /    11 runs   (  114.22 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1258.90 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  55%|▌| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1248.58 ms /    11 runs   (  113.51 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1251.25 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  60%|▌| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1356.38 ms /    12 runs   (  113.03 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1359.03 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  65%|▋| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1355.65 ms /    12 runs   (  112.97 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1358.22 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  70%|▋| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1252.10 ms /    11 runs   (  113.83 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1254.47 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  75%|▊| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1340.77 ms /    12 runs   (  111.73 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1343.44 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  80%|▊| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1343.20 ms /    12 runs   (  111.93 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1345.81 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  85%|▊| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1532.02 ms /    12 runs   (  127.67 ms per token,     7.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1534.72 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  90%|▉| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1443.88 ms /    12 runs   (  120.32 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1446.76 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  95%|▉| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1446.73 ms /    12 runs   (  120.56 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1449.59 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:   0%| |Llama.generate: 38 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =   10550.83 ms /    12 tokens (  879.24 ms per token,     1.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1567.98 ms /    10 runs   (  156.80 ms per token,     6.38 tokens per second)\n",
      "llama_perf_context_print:       total time =   12122.45 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:   5%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1572.04 ms /    11 runs   (  142.91 ms per token,     7.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1574.73 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  10%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1387.36 ms /    11 runs   (  126.12 ms per token,     7.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1389.93 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  15%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1336.87 ms /    11 runs   (  121.53 ms per token,     8.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1339.92 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  20%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1283.88 ms /    11 runs   (  116.72 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1287.25 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  25%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1361.31 ms /    11 runs   (  123.76 ms per token,     8.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1363.84 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  30%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1369.80 ms /    11 runs   (  124.53 ms per token,     8.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1372.27 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  35%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1407.95 ms /    11 runs   (  128.00 ms per token,     7.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1410.52 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  40%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1503.30 ms /    11 runs   (  136.66 ms per token,     7.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1505.89 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  45%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1539.47 ms /    11 runs   (  139.95 ms per token,     7.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1542.06 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  50%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1506.80 ms /    11 runs   (  136.98 ms per token,     7.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1509.58 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  55%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1510.77 ms /    11 runs   (  137.34 ms per token,     7.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1513.33 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  60%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1589.29 ms /    11 runs   (  144.48 ms per token,     6.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1591.93 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  65%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1523.19 ms /    11 runs   (  138.47 ms per token,     7.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1525.65 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  70%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1494.38 ms /    11 runs   (  135.85 ms per token,     7.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1496.81 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  75%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1439.08 ms /    11 runs   (  130.83 ms per token,     7.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1441.75 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  80%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1353.03 ms /    11 runs   (  123.00 ms per token,     8.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1355.42 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  85%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1396.09 ms /    11 runs   (  126.92 ms per token,     7.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1398.49 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  90%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1297.63 ms /    11 runs   (  117.97 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1300.15 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  95%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1316.39 ms /    11 runs   (  119.67 ms per token,     8.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1318.82 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • i cream\n",
      " • i cream\n",
      " • i cream\n",
      " • i tea\n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • i tea\n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • i cream\n",
      " •  \n",
      " • i cream\n",
      " •  \n",
      " • i cream\n",
      " • i tea\n",
      " •  \n",
      " • i cream\n",
      " • enjoy cream\n",
      " • enjoy cream\n",
      " • adore \n",
      " • enjoy \n",
      " • love \n",
      " • love cream\n",
      " • love \n",
      " • love \n",
      " • enjoy cream\n",
      " • love \n",
      " • eat cream\n",
      " • adore \n",
      " • love cream\n",
      " • enjoy cream\n",
      " • love cream\n",
      " • enjoy cream\n",
      " • love \n",
      " • enjoy [mask]\n",
      " • enjoy cream\n",
      " • enjoy cream\n",
      " • snow \n",
      " • popsicle \n",
      " • popsicle \n",
      " • lemon cream\n",
      " • lemon pops\n",
      " • popsicle \n",
      " •  cream\n",
      " • popsicle \n",
      " • popsicle \n",
      " • popsicle \n",
      " • popsicle \n",
      " • popsicle \n",
      " • popsicle \n",
      " • popsicle \n",
      " • popsicle \n",
      " • popsicle \n",
      " • popsicle \n",
      " • popsicle \n",
      " • popsicle \n",
      " • popsicle \n",
      " • corn ice cream\n",
      " • popsicles \n",
      " • popsicles \n",
      " • popsicles \n",
      " • popsicles \n",
      " • ice cream cones\n",
      " • popsicles \n",
      " • popsicles \n",
      " • popsicles \n",
      " • ice cream cones\n",
      " • popsicles \n",
      " • ice cream cones\n",
      " • popsicles \n",
      " • popsicles \n",
      " • popsicles \n",
      " • popsicles \n",
      " • popsicles \n",
      " • popsicles \n",
      " • candy apples\n",
      " • popsicles \n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream cones on\n",
      " • cream cones on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream sweltering\n",
      " • cream hot\n",
      " • cream hot\n",
      " • cream hot\n",
      " • cream hot\n",
      " • cream hot\n",
      " • cream hot\n",
      " • cream hot\n",
      " • cream sunny\n",
      " • cream hot\n",
      " • cream hot\n",
      " • cream hot\n",
      " • cream sweltering\n",
      " • cream hot\n",
      " • cream sweltering\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream hot\n",
      " • cream sunny\n",
      " • cream hot\n",
      " • cream sunny\n",
      " • cream summer\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream summer\n",
      " • cream summer\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream summer\n",
      " • cream summer\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream summer\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      "\n",
      "Total generated: 160\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "children        PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "love            PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "sweet           PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "ice             PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "on              PMI=  0.000   P(x)=1.000  P(y)=0.900  P(xy)=0.900\n",
      "warm            PMI=   -inf   P(x)=1.000  P(y)=0.000  P(xy)=0.000\n",
      "summer          PMI=  0.000   P(x)=1.000  P(y)=0.300  P(xy)=0.300\n",
      "days            PMI=  0.000   P(x)=1.000  P(y)=1.000  P(xy)=1.000\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 5   |   Anchor word: 'on'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:   0%| | Llama.generate: 29 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =   12348.57 ms /    21 tokens (  588.03 ms per token,     1.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1417.16 ms /    10 runs   (  141.72 ms per token,     7.06 tokens per second)\n",
      "llama_perf_context_print:       total time =   13769.39 ms /    31 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:   5%| | Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', 'days'] ['he', 'loves', 'sweet', 'ice', 'cream', 'during', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1477.01 ms /    11 runs   (  134.27 ms per token,     7.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1479.58 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  10%| | Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1424.87 ms /    11 runs   (  129.53 ms per token,     7.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1427.32 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  15%|▏| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1509.44 ms /    10 runs   (  150.94 ms per token,     6.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1511.80 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  20%|▏| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1353.10 ms /    11 runs   (  123.01 ms per token,     8.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1355.72 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  25%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1430.06 ms /    11 runs   (  130.01 ms per token,     7.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1432.84 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  30%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1413.60 ms /    11 runs   (  128.51 ms per token,     7.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1416.13 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  35%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1330.36 ms /    11 runs   (  120.94 ms per token,     8.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1332.87 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  40%|▍| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1444.87 ms /    11 runs   (  131.35 ms per token,     7.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1447.37 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  45%|▍| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1369.84 ms /    11 runs   (  124.53 ms per token,     8.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1372.35 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  50%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1442.40 ms /    11 runs   (  131.13 ms per token,     7.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1445.29 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  55%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1410.54 ms /    11 runs   (  128.23 ms per token,     7.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1413.05 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  60%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1327.69 ms /    10 runs   (  132.77 ms per token,     7.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1330.18 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  65%|▋| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1407.42 ms /    11 runs   (  127.95 ms per token,     7.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1409.88 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  70%|▋| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1263.09 ms /    10 runs   (  126.31 ms per token,     7.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1265.43 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  75%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1378.29 ms /    11 runs   (  125.30 ms per token,     7.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1380.79 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  80%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1832.73 ms /    14 runs   (  130.91 ms per token,     7.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1835.92 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  85%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1314.30 ms /    11 runs   (  119.48 ms per token,     8.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1316.80 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  90%|▉| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1333.70 ms /    11 runs   (  121.25 ms per token,     8.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1336.22 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream [MASK] warm summer days:  95%|▉| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1343.43 ms /    11 runs   (  122.13 ms per token,     8.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1345.96 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:   0%Llama.generate: 29 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =    6070.91 ms /    21 tokens (  289.09 ms per token,     3.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1175.83 ms /     9 runs   (  130.65 ms per token,     7.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    7250.40 ms /    30 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:   5%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1339.69 ms /    10 runs   (  133.97 ms per token,     7.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1342.16 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  10%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1306.97 ms /    10 runs   (  130.70 ms per token,     7.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1309.43 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  15%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1393.49 ms /    11 runs   (  126.68 ms per token,     7.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1396.37 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  20%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1356.06 ms /    11 runs   (  123.28 ms per token,     8.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1359.05 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  25%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1397.02 ms /    11 runs   (  127.00 ms per token,     7.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1399.58 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  30%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1189.99 ms /    10 runs   (  119.00 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1192.34 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  35%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1421.20 ms /    12 runs   (  118.43 ms per token,     8.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1423.83 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  40%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1280.56 ms /    11 runs   (  116.41 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1283.06 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  45%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1155.82 ms /    10 runs   (  115.58 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1158.42 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  50%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1122.29 ms /    10 runs   (  112.23 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1124.74 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  55%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1228.44 ms /    11 runs   (  111.68 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1231.62 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  60%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1151.41 ms /    10 runs   (  115.14 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1153.59 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  65%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1272.06 ms /    11 runs   (  115.64 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1274.69 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  70%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1109.43 ms /    10 runs   (  110.94 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1111.66 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  75%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1346.20 ms /    12 runs   (  112.18 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1348.77 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  80%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1232.01 ms /    11 runs   (  112.00 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1234.65 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  85%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1232.08 ms /    11 runs   (  112.01 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1234.57 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  90%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1122.11 ms /    10 runs   (  112.21 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1124.26 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream [MASK] warm summer days:  95%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1122.64 ms /    10 runs   (  112.26 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1124.80 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:   0%|Llama.generate: 30 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     532.73 ms /    20 tokens (   26.64 ms per token,    37.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1236.16 ms /    11 runs   (  112.38 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1771.54 ms /    31 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:   5%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1196.71 ms /    11 runs   (  108.79 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1199.03 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  10%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1335.01 ms /    12 runs   (  111.25 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1337.60 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  15%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1581.02 ms /    12 runs   (  131.75 ms per token,     7.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1583.57 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  20%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1316.30 ms /    12 runs   (  109.69 ms per token,     9.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1319.03 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  25%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1342.72 ms /    12 runs   (  111.89 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1345.24 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  30%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1254.00 ms /    11 runs   (  114.00 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1256.42 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  35%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1314.50 ms /    12 runs   (  109.54 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1317.35 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  40%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1232.80 ms /    11 runs   (  112.07 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1235.23 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  45%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1342.01 ms /    12 runs   (  111.83 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1344.60 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  50%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1214.78 ms /    11 runs   (  110.43 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1217.10 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  55%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1325.26 ms /    12 runs   (  110.44 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1327.85 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  60%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1237.39 ms /    11 runs   (  112.49 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1239.86 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  65%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1197.32 ms /    11 runs   (  108.85 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1200.01 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  70%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1308.21 ms /    12 runs   (  109.02 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1310.74 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  75%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1308.69 ms /    12 runs   (  109.06 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1311.20 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  80%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1300.69 ms /    12 runs   (  108.39 ms per token,     9.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1303.21 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  85%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1304.41 ms /    12 runs   (  108.70 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1307.07 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  90%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1215.80 ms /    11 runs   (  110.53 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1218.13 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream [MASK] warm summer days:  95%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1311.93 ms /    12 runs   (  109.33 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1314.45 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:   0Llama.generate: 31 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     557.31 ms /    19 tokens (   29.33 ms per token,    34.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1331.87 ms /    10 runs   (  133.19 ms per token,     7.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1892.02 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:   5Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1499.79 ms /    12 runs   (  124.98 ms per token,     8.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1502.88 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  10Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1483.24 ms /    11 runs   (  134.84 ms per token,     7.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1485.95 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  15Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1610.65 ms /    12 runs   (  134.22 ms per token,     7.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1613.57 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  20Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1514.80 ms /    12 runs   (  126.23 ms per token,     7.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1517.69 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  25Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1438.88 ms /    11 runs   (  130.81 ms per token,     7.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1441.86 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  30Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1490.89 ms /    11 runs   (  135.54 ms per token,     7.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1494.14 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  35Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1464.00 ms /    12 runs   (  122.00 ms per token,     8.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1466.98 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  40Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1376.61 ms /    11 runs   (  125.15 ms per token,     7.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1379.19 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  45Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1343.23 ms /    11 runs   (  122.11 ms per token,     8.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1345.87 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  50Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1341.34 ms /    11 runs   (  121.94 ms per token,     8.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1343.99 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  55Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1314.18 ms /    11 runs   (  119.47 ms per token,     8.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1316.77 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  60Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1395.55 ms /    12 runs   (  116.30 ms per token,     8.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1398.40 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  65Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1281.62 ms /    11 runs   (  116.51 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1284.31 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  70Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1248.14 ms /    11 runs   (  113.47 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1250.77 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  75Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1354.49 ms /    12 runs   (  112.87 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1357.51 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  80Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1262.08 ms /    11 runs   (  114.73 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1264.83 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  85Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1379.81 ms /    12 runs   (  114.98 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1382.86 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  90Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1373.90 ms /    12 runs   (  114.49 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1376.87 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream [MASK] warm summer days:  95Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1261.15 ms /    11 runs   (  114.65 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1263.79 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:   0%|Llama.generate: 32 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     603.11 ms /    18 tokens (   33.51 ms per token,    29.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1358.54 ms /    12 runs   (  113.21 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1964.79 ms /    30 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:   5%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1260.88 ms /    11 runs   (  114.63 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1263.52 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  10%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1458.51 ms /    13 runs   (  112.19 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1461.54 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  15%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1265.18 ms /    11 runs   (  115.02 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1267.77 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  20%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1235.16 ms /    11 runs   (  112.29 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1237.74 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  25%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1240.25 ms /    11 runs   (  112.75 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1242.82 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  30%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1233.47 ms /    11 runs   (  112.13 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1236.19 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  35%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1233.60 ms /    11 runs   (  112.15 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1236.20 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  40%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1242.71 ms /    11 runs   (  112.97 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1245.29 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  45%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1240.96 ms /    11 runs   (  112.81 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1243.51 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  50%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1258.55 ms /    11 runs   (  114.41 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1261.15 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  55%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1244.02 ms /    11 runs   (  113.09 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1246.61 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  60%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1243.56 ms /    11 runs   (  113.05 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1246.13 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  65%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1261.46 ms /    11 runs   (  114.68 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1264.06 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  70%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1216.51 ms /    11 runs   (  110.59 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1219.08 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  75%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1242.01 ms /    11 runs   (  112.91 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1244.60 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  80%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1261.50 ms /    11 runs   (  114.68 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1264.06 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  85%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1223.55 ms /    11 runs   (  111.23 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1226.46 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  90%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1207.53 ms /    11 runs   (  109.78 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1210.16 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] [MASK] warm summer days:  95%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1257.30 ms /    11 runs   (  114.30 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1259.78 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:   0%Llama.generate: 33 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     629.50 ms /    17 tokens (   37.03 ms per token,    27.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1722.24 ms /    13 runs   (  132.48 ms per token,     7.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    2355.53 ms /    30 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:   5%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1224.34 ms /    11 runs   (  111.30 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1226.70 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  10%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1545.65 ms /    14 runs   (  110.40 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1548.64 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  15%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1564.14 ms /    14 runs   (  111.72 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1567.11 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  20%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1757.00 ms /    16 runs   (  109.81 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1760.49 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  25%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1307.88 ms /    12 runs   (  108.99 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1310.43 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  30%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1321.48 ms /    12 runs   (  110.12 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1323.99 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  35%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2103.92 ms /    19 runs   (  110.73 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    2108.12 ms /    20 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  40%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1456.22 ms /    13 runs   (  112.02 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1458.99 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  45%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1326.30 ms /    12 runs   (  110.52 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1328.81 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  50%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1222.02 ms /    11 runs   (  111.09 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1224.65 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  55%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1446.11 ms /    13 runs   (  111.24 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1448.97 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  60%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1556.46 ms /    14 runs   (  111.18 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1559.45 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  65%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1539.01 ms /    14 runs   (  109.93 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1541.95 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  70%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1551.04 ms /    14 runs   (  110.79 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1554.59 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  75%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1436.86 ms /    13 runs   (  110.53 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1439.79 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  80%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1438.65 ms /    13 runs   (  110.67 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1441.52 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  85%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1320.49 ms /    12 runs   (  110.04 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1323.08 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  90%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1208.24 ms /    11 runs   (  109.84 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1210.74 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  95%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1233.39 ms /    11 runs   (  112.13 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1235.73 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:   0%| Llama.generate: 37 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     706.18 ms /    13 tokens (   54.32 ms per token,    18.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1362.66 ms /    11 runs   (  123.88 ms per token,     8.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    2071.86 ms /    24 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:   5%| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', '[mask]', 'days'] ['children', 'love', 'sweet', 'ice', 'cream', 'on', 'hot', 'sunny', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1616.08 ms /    14 runs   (  115.43 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1619.49 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  10%| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1478.31 ms /    12 runs   (  123.19 ms per token,     8.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1481.15 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  15%|▏Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1434.90 ms /    12 runs   (  119.57 ms per token,     8.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1438.45 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  20%|▏Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1477.21 ms /    12 runs   (  123.10 ms per token,     8.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1480.10 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  25%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', '[mask]', 'days'] ['children', 'love', 'sweet', 'ice', 'cream', 'on', 'hot', 'sunny', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1477.47 ms /    12 runs   (  123.12 ms per token,     8.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1480.29 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  30%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1445.11 ms /    12 runs   (  120.43 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1447.97 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  35%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', '[mask]', 'days'] ['children', 'love', 'sweet', 'ice', 'cream', 'on', 'hot', 'sunny', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1437.57 ms /    12 runs   (  119.80 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1441.04 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  40%|▍Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1323.90 ms /    11 runs   (  120.35 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1326.50 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  45%|▍Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', '[mask]', 'days'] ['children', 'love', 'sweet', 'ice', 'cream', 'on', 'hot', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1589.63 ms /    14 runs   (  113.55 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1592.70 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  50%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1557.23 ms /    13 runs   (  119.79 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1560.05 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  55%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1551.59 ms /    13 runs   (  119.35 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1554.39 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  60%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1412.59 ms /    12 runs   (  117.72 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1415.21 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  65%|▋Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1436.10 ms /    12 runs   (  119.68 ms per token,     8.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1438.97 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  70%|▋Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1409.66 ms /    12 runs   (  117.47 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1412.26 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  75%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', '[mask]', 'days'] ['children', 'love', 'sweet', 'ice', 'cream', 'on', 'hot', 'sunny', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1373.26 ms /    12 runs   (  114.44 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1375.89 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  80%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1389.21 ms /    12 runs   (  115.77 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1391.80 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  85%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1332.32 ms /    12 runs   (  111.03 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1334.91 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  90%|▉Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1375.66 ms /    12 runs   (  114.64 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1378.40 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  95%|▉Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1503.94 ms /    13 runs   (  115.69 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1506.89 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:   0%Llama.generate: 38 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     511.42 ms /    12 tokens (   42.62 ms per token,    23.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1071.30 ms /    10 runs   (  107.13 ms per token,     9.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1585.33 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:   5%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1357.36 ms /    13 runs   (  104.41 ms per token,     9.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1360.86 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  10%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1157.35 ms /    11 runs   (  105.21 ms per token,     9.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1159.88 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  15%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1397.14 ms /    13 runs   (  107.47 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1400.46 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  20%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1548.85 ms /    14 runs   (  110.63 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1552.09 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  25%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1201.26 ms /    11 runs   (  109.21 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1203.78 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  30%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1195.34 ms /    11 runs   (  108.67 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1197.88 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  35%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', '[mask]'] ['children', 'love', 'sweet', 'ice', 'cream', 'on', 'hot', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1218.54 ms /    11 runs   (  110.78 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1221.34 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  40%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1314.96 ms /    12 runs   (  109.58 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1317.76 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  45%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1392.67 ms /    13 runs   (  107.13 ms per token,     9.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1395.65 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  50%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1441.09 ms /    13 runs   (  110.85 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1444.31 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  55%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1423.74 ms /    13 runs   (  109.52 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1426.68 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  60%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1198.90 ms /    11 runs   (  108.99 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1201.44 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  65%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', '[mask]'] ['children', 'love', 'sweet', 'ice', 'cream', 'in', 'hot', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1160.25 ms /    11 runs   (  105.48 ms per token,     9.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1162.97 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  70%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', '[mask]'] ['children', 'love', 'sweet', 'ice', 'cream', 'on', 'hot', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1433.73 ms /    13 runs   (  110.29 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1436.72 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  75%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1291.79 ms /    12 runs   (  107.65 ms per token,     9.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1294.70 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  80%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1410.30 ms /    13 runs   (  108.48 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1413.26 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  85%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1656.44 ms /    15 runs   (  110.43 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1659.87 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  90%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1532.39 ms /    14 runs   (  109.46 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1535.75 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  95%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1517.08 ms /    14 runs   (  108.36 ms per token,     9.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1520.36 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " •  \n",
      " •  \n",
      " • i on\n",
      " • i on\n",
      " • i on\n",
      " •  \n",
      " •  \n",
      " • i during\n",
      " • i on\n",
      " • i on\n",
      " • i on\n",
      " •  \n",
      " • i on\n",
      " • i during\n",
      " • i on\n",
      " •  \n",
      " • my cream intensifies during\n",
      " •  \n",
      " • i on\n",
      " • i on\n",
      " • love during\n",
      " • enjoy on\n",
      " • love during\n",
      " • enjoy during\n",
      " • love during\n",
      " • adore on\n",
      " • love during\n",
      " • relish on\n",
      " • love on\n",
      " • love during\n",
      " • love on\n",
      " • love during\n",
      " • love during\n",
      " • love on\n",
      " • love on\n",
      " • adore during\n",
      " • love on\n",
      " • love on\n",
      " • love during\n",
      " • love during\n",
      " • vanilla on\n",
      " • chocolate on\n",
      " • vanilla on\n",
      " • vanilla on\n",
      " • vanilla on\n",
      " • vanilla on\n",
      " • chocolate on\n",
      " • vanilla on\n",
      " • chocolate on\n",
      " • vanilla on\n",
      " • chocolate on\n",
      " • vanilla on\n",
      " • chocolate on\n",
      " • chocolate on\n",
      " • vanilla on\n",
      " • vanilla on\n",
      " • vanilla on\n",
      " • vanilla on\n",
      " • chocolate on\n",
      " • vanilla on\n",
      " • ice on\n",
      " • strawberry on\n",
      " • ice on\n",
      " • dessert during\n",
      " • strawberry during\n",
      " • chocolate on\n",
      " • ice on\n",
      " • vanilla on\n",
      " • ice on\n",
      " • ice on\n",
      " • ice on\n",
      " • chocolate on\n",
      " • vanilla on\n",
      " • ice on\n",
      " • ice on\n",
      " • vanilla on\n",
      " • chocolate on\n",
      " • strawberry on\n",
      " • strawberry on\n",
      " • ice on\n",
      " • cream cones on\n",
      " • cream on\n",
      " • cream cones on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cream on\n",
      " • cones during sunny\n",
      " • during warm\n",
      " • cones on sunny\n",
      " • on hot sunny\n",
      " • cones and popsicles during\n",
      " • on sunny\n",
      " • in the hot\n",
      " • in the hot [mask] during [mask]\n",
      " • cones on hot\n",
      " • cones during\n",
      " • on hot\n",
      " • cones during hot\n",
      " • during hot sunny\n",
      " • cones on sunny\n",
      " • on sunny warm\n",
      " • on warm sunny\n",
      " • during warm sunny\n",
      " • on sunny\n",
      " • on hot\n",
      " • on hot\n",
      " •  \n",
      " • in the sunshine on \n",
      " • in sunny\n",
      " • in sunny\n",
      " •  \n",
      " • on sunny\n",
      " •  \n",
      " • on sunny\n",
      " •  \n",
      " • in the sun on summer\n",
      " • on sunny\n",
      " • in sunny\n",
      " • creamy sunny\n",
      " • on sunny\n",
      " •  \n",
      " • on sunny\n",
      " • on sunny\n",
      " • on sunny \n",
      " • on sunny\n",
      " • on sunny\n",
      " • during days\n",
      " • on sunny days\n",
      " • during days\n",
      " • cones during days\n",
      " • cones during evenings\n",
      " • on days\n",
      " •  \n",
      " • on days\n",
      " • in the weather\n",
      " • cones during days\n",
      " • cones during days\n",
      " • on sunny days\n",
      " •  \n",
      " •  \n",
      " • cones during days\n",
      " • in the days\n",
      " • cones during days\n",
      " • cones in afternoons\n",
      " • in the sunny days\n",
      " • in the sunny days\n",
      "\n",
      "Total generated: 160\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "children        PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "love            PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "sweet           PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "ice             PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "cream           PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "warm            PMI=   -inf   P(x)=0.400  P(y)=0.050  P(xy)=0.000\n",
      "summer          PMI=   -inf   P(x)=0.400  P(y)=0.050  P(xy)=0.000\n",
      "days            PMI=  0.515   P(x)=0.100  P(y)=0.700  P(xy)=0.100\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 6   |   Anchor word: 'warm'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:   0%| | 0/Llama.generate: 29 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     579.63 ms /    21 tokens (   27.60 ms per token,    36.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1154.08 ms /    11 runs   (  104.92 ms per token,     9.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1736.47 ms /    32 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:   5%| | 1/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', '[mask]', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'sunny', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1229.58 ms /    11 runs   (  111.78 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1232.09 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  10%| | 2/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', '[mask]', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'hot', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1130.33 ms /    10 runs   (  113.03 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1132.56 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  15%|▏| 3/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1241.76 ms /    11 runs   (  112.89 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1244.18 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  20%|▏| 4/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1205.74 ms /    11 runs   (  109.61 ms per token,     9.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1208.21 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  25%|▎| 5/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1215.08 ms /    11 runs   (  110.46 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1217.53 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  30%|▎| 6/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', '[mask]', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1385.63 ms /    12 runs   (  115.47 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1388.62 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  35%|▎| 7/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1260.32 ms /    11 runs   (  114.57 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1262.88 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  40%|▍| 8/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1271.58 ms /    11 runs   (  115.60 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1274.28 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  45%|▍| 9/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', '[mask]', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'hot', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1260.31 ms /    11 runs   (  114.57 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1262.83 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  50%|▌| 10Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1167.14 ms /    11 runs   (  106.10 ms per token,     9.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1169.61 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  55%|▌| 11Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1232.13 ms /    11 runs   (  112.01 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1234.56 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  60%|▌| 12Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1240.83 ms /    11 runs   (  112.80 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1243.24 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  65%|▋| 13Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', '[mask]', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'hot', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1401.26 ms /    11 runs   (  127.39 ms per token,     7.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1403.85 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  70%|▋| 14Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1401.43 ms /    11 runs   (  127.40 ms per token,     7.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1403.89 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  75%|▊| 15Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', '[mask]', 'summer', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'hot', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1385.39 ms /    11 runs   (  125.94 ms per token,     7.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1387.94 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  80%|▊| 16Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1287.17 ms /    11 runs   (  117.02 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1289.81 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  85%|▊| 17Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1383.05 ms /    11 runs   (  125.73 ms per token,     7.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1385.50 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  90%|▉| 18Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1383.50 ms /    11 runs   (  125.77 ms per token,     7.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1385.82 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on [MASK] summer days:  95%|▉| 19Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1278.76 ms /    11 runs   (  116.25 ms per token,     8.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1281.53 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:   0%| Llama.generate: 29 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     474.33 ms /    21 tokens (   22.59 ms per token,    44.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1156.12 ms /    10 runs   (  115.61 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1632.97 ms /    31 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:   5%| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1320.83 ms /    11 runs   (  120.08 ms per token,     8.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1323.25 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  10%| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1701.04 ms /    14 runs   (  121.50 ms per token,     8.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1704.32 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  15%|▏Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1680.59 ms /    14 runs   (  120.04 ms per token,     8.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1683.64 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  20%|▏Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1655.79 ms /    14 runs   (  118.27 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1658.83 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  25%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1356.75 ms /    11 runs   (  123.34 ms per token,     8.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1359.21 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  30%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1429.01 ms /    12 runs   (  119.08 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1431.63 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  35%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1159.08 ms /    10 runs   (  115.91 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1161.53 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  40%|▍Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1203.41 ms /    10 runs   (  120.34 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1205.64 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  45%|▍Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1189.49 ms /    10 runs   (  118.95 ms per token,     8.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1191.90 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  50%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1242.38 ms /    11 runs   (  112.94 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1244.89 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  55%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1218.55 ms /    11 runs   (  110.78 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1221.29 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  60%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1166.85 ms /    10 runs   (  116.69 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1169.04 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  65%|▋Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1374.96 ms /    12 runs   (  114.58 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1377.61 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  70%|▋Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1255.39 ms /    11 runs   (  114.13 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1258.01 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  75%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1256.72 ms /    11 runs   (  114.25 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1259.39 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  80%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1274.77 ms /    11 runs   (  115.89 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1277.30 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  85%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1471.23 ms /    13 runs   (  113.17 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1474.04 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  90%|▉Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1204.79 ms /    11 runs   (  109.53 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1207.22 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on [MASK] summer days:  95%|▉Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1237.65 ms /    11 runs   (  112.51 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1240.06 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:   0%| |Llama.generate: 30 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     505.32 ms /    20 tokens (   25.27 ms per token,    39.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1439.16 ms /    13 runs   (  110.70 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1947.73 ms /    33 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:   5%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1485.94 ms /    13 runs   (  114.30 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1488.90 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  10%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1288.07 ms /    12 runs   (  107.34 ms per token,     9.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1290.72 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  15%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1496.84 ms /    14 runs   (  106.92 ms per token,     9.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1499.91 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  20%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1642.32 ms /    15 runs   (  109.49 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1645.63 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  25%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1406.28 ms /    13 runs   (  108.18 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1409.25 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  30%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1388.95 ms /    13 runs   (  106.84 ms per token,     9.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1392.04 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  35%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1314.36 ms /    12 runs   (  109.53 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1317.10 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  40%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1221.94 ms /    11 runs   (  111.09 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1224.52 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  45%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1329.06 ms /    12 runs   (  110.75 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1331.78 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  50%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1321.27 ms /    12 runs   (  110.11 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1324.25 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  55%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1400.21 ms /    13 runs   (  107.71 ms per token,     9.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1403.09 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  60%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1279.96 ms /    12 runs   (  106.66 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1282.96 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  65%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1268.77 ms /    12 runs   (  105.73 ms per token,     9.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1271.42 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  70%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1299.63 ms /    12 runs   (  108.30 ms per token,     9.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1302.27 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  75%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1325.39 ms /    12 runs   (  110.45 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1328.04 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  80%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1633.84 ms /    13 runs   (  125.68 ms per token,     7.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1636.78 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  85%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1573.10 ms /    15 runs   (  104.87 ms per token,     9.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1576.40 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  90%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1534.56 ms /    14 runs   (  109.61 ms per token,     9.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1538.05 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love [MASK] ice cream on [MASK] summer days:  95%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1298.63 ms /    12 runs   (  108.22 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1301.51 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:   0%|Llama.generate: 31 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     556.83 ms /    19 tokens (   29.31 ms per token,    34.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1394.41 ms /    12 runs   (  116.20 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1954.17 ms /    31 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:   5%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1272.91 ms /    11 runs   (  115.72 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1275.32 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  10%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1468.77 ms /    13 runs   (  112.98 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1471.67 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  15%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1397.39 ms /    12 runs   (  116.45 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1400.02 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  20%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1259.92 ms /    11 runs   (  114.54 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1262.33 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  25%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1460.41 ms /    13 runs   (  112.34 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1463.27 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  30%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1402.61 ms /    12 runs   (  116.88 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1405.27 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  35%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1465.30 ms /    13 runs   (  112.72 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1468.58 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  40%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1397.88 ms /    12 runs   (  116.49 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1400.57 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  45%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1465.33 ms /    13 runs   (  112.72 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1468.19 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  50%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1250.03 ms /    11 runs   (  113.64 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1252.46 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  55%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1410.05 ms /    12 runs   (  117.50 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1412.68 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  60%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1467.91 ms /    13 runs   (  112.92 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1470.75 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  65%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1542.30 ms /    14 runs   (  110.16 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1545.36 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  70%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1330.72 ms /    12 runs   (  110.89 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1333.37 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  75%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1329.08 ms /    12 runs   (  110.76 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1331.70 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  80%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1405.50 ms /    13 runs   (  108.12 ms per token,     9.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1408.36 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  85%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1473.07 ms /    13 runs   (  113.31 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1476.07 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  90%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1230.45 ms /    11 runs   (  111.86 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1233.12 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on [MASK] summer days:  95%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1158.31 ms /    11 runs   (  105.30 ms per token,     9.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1160.81 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', '[mask]', 'cream', 'on', '[mask]', 'summer', 'days'] ['children', 'love', 'sweet', 'corn', 'on', 'sunny', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:   0%| |Llama.generate: 32 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     606.66 ms /    18 tokens (   33.70 ms per token,    29.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1371.88 ms /    11 runs   (  124.72 ms per token,     8.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1981.26 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:   5%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1469.30 ms /    12 runs   (  122.44 ms per token,     8.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1471.91 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  10%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1548.33 ms /    12 runs   (  129.03 ms per token,     7.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1550.99 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  15%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1529.60 ms /    12 runs   (  127.47 ms per token,     7.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1533.04 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  20%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1302.61 ms /    11 runs   (  118.42 ms per token,     8.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1305.08 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  25%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1388.54 ms /    11 runs   (  126.23 ms per token,     7.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1390.94 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  30%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1386.56 ms /    11 runs   (  126.05 ms per token,     7.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1389.43 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  35%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1303.45 ms /    11 runs   (  118.50 ms per token,     8.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1305.90 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  40%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1475.09 ms /    12 runs   (  122.92 ms per token,     8.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1477.78 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  45%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1347.71 ms /    11 runs   (  122.52 ms per token,     8.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1350.25 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  50%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1222.07 ms /    11 runs   (  111.10 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1224.48 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  55%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1226.69 ms /    11 runs   (  111.52 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1229.17 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  60%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1205.67 ms /    11 runs   (  109.61 ms per token,     9.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1208.41 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  65%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1188.60 ms /    11 runs   (  108.05 ms per token,     9.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1191.00 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  70%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1185.72 ms /    11 runs   (  107.79 ms per token,     9.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1188.17 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  75%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1186.15 ms /    11 runs   (  107.83 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1188.56 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  80%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1283.06 ms /    12 runs   (  106.92 ms per token,     9.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1285.66 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  85%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1293.53 ms /    12 runs   (  107.79 ms per token,     9.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1296.14 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  90%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1307.96 ms /    12 runs   (  109.00 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1310.58 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on [MASK] summer days:  95%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1208.67 ms /    11 runs   (  109.88 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1211.16 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:   0%Llama.generate: 33 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     728.50 ms /    17 tokens (   42.85 ms per token,    23.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1300.77 ms /    12 runs   (  108.40 ms per token,     9.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    2032.31 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:   5%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1769.58 ms /    16 runs   (  110.60 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1773.19 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  10%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1241.54 ms /    11 runs   (  112.87 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1244.22 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  15%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1416.32 ms /    13 runs   (  108.95 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1419.15 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  20%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1437.00 ms /    13 runs   (  110.54 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1439.84 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  25%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1446.66 ms /    13 runs   (  111.28 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1449.50 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  30%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1588.91 ms /    14 runs   (  113.49 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1592.11 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  35%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1563.97 ms /    14 runs   (  111.71 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1567.58 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  40%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1764.66 ms /    16 runs   (  110.29 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1768.15 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  45%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1545.23 ms /    14 runs   (  110.37 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1548.31 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  50%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1545.66 ms /    14 runs   (  110.40 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1548.72 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  55%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1435.17 ms /    13 runs   (  110.40 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1438.00 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  60%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1523.29 ms /    14 runs   (  108.81 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1526.30 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  65%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1331.67 ms /    12 runs   (  110.97 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1334.27 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  70%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1553.75 ms /    14 runs   (  110.98 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1557.01 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  75%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1291.76 ms /    12 runs   (  107.65 ms per token,     9.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1294.38 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  80%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1910.98 ms /    17 runs   (  112.41 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1914.72 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  85%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', 'ice', 'cream', '[mask]', '[mask]', 'summer', 'days'] ['children', 'love', 'sweet', 'ice', 'cream', 'on', 'hot', '[mask]', 'days', 'in', 'the', '[mask]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1222.59 ms /    11 runs   (  111.14 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1225.18 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  90%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1341.88 ms /    12 runs   (  111.82 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1344.87 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] [MASK] summer days:  95%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1224.84 ms /    11 runs   (  111.35 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1227.29 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:   0%| | Llama.generate: 34 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     541.78 ms /    16 tokens (   33.86 ms per token,    29.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1261.76 ms /    11 runs   (  114.71 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1806.72 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:   5%| | Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1463.69 ms /    12 runs   (  121.97 ms per token,     8.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1466.82 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  10%| | Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1329.36 ms /    11 runs   (  120.85 ms per token,     8.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1332.69 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  15%|▏| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1400.41 ms /    12 runs   (  116.70 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1403.25 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  20%|▏| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1466.72 ms /    12 runs   (  122.23 ms per token,     8.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1469.52 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  25%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1464.49 ms /    12 runs   (  122.04 ms per token,     8.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1467.46 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  30%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1425.80 ms /    12 runs   (  118.82 ms per token,     8.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1428.62 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  35%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1399.82 ms /    12 runs   (  116.65 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1402.80 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  40%|▍| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1349.55 ms /    12 runs   (  112.46 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1353.04 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  45%|▍| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1311.51 ms /    12 runs   (  109.29 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1314.12 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  50%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', 'ice', 'cream', 'on', '[mask]', '[mask]', 'days'] ['children', 'love', 'sweet', 'ice', 'cream', 'on', 'sunny', 'sundays']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1238.72 ms /    11 runs   (  112.61 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1241.14 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  55%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', 'ice', 'cream', 'on', '[mask]', '[mask]', 'days'] ['children', 'love', 'sweet', 'ice', 'cream', 'on', 'summer', 'sundays']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1339.13 ms /    12 runs   (  111.59 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1341.75 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  60%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1347.89 ms /    12 runs   (  112.32 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1350.48 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  65%|▋| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1365.77 ms /    12 runs   (  113.81 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1368.42 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  70%|▋| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1284.29 ms /    11 runs   (  116.75 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1286.73 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  75%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1230.77 ms /    11 runs   (  111.89 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1233.22 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  80%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1283.51 ms /    11 runs   (  116.68 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1285.97 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  85%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1362.21 ms /    12 runs   (  113.52 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1364.84 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  90%|▉| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1351.46 ms /    12 runs   (  112.62 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1354.58 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  95%|▉| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1239.16 ms /    11 runs   (  112.65 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1241.67 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:   0%| Llama.generate: 38 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     380.96 ms /    12 tokens (   31.75 ms per token,    31.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1531.09 ms /    10 runs   (  153.11 ms per token,     6.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1914.63 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:   5%| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1321.45 ms /    11 runs   (  120.13 ms per token,     8.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1323.87 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  10%| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1337.85 ms /    11 runs   (  121.62 ms per token,     8.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1340.43 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  15%|▏Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1334.56 ms /    11 runs   (  121.32 ms per token,     8.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1336.97 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  20%|▏Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1327.91 ms /    11 runs   (  120.72 ms per token,     8.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1330.32 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  25%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1308.95 ms /    11 runs   (  119.00 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1311.30 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  30%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1315.37 ms /    11 runs   (  119.58 ms per token,     8.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1317.97 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  35%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1293.52 ms /    11 runs   (  117.59 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1296.05 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  40%|▍Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1297.71 ms /    11 runs   (  117.97 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1300.33 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  45%|▍Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1327.84 ms /    11 runs   (  120.71 ms per token,     8.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1330.31 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  50%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1294.34 ms /    11 runs   (  117.67 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1296.69 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  55%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1286.21 ms /    11 runs   (  116.93 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1288.60 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  60%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1281.74 ms /    11 runs   (  116.52 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1284.15 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  65%|▋Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1250.90 ms /    11 runs   (  113.72 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1253.25 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  70%|▋Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1272.01 ms /    11 runs   (  115.64 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1274.36 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  75%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1279.64 ms /    11 runs   (  116.33 ms per token,     8.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1282.05 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  80%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1230.96 ms /    11 runs   (  111.91 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1233.50 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  85%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1251.21 ms /    11 runs   (  113.75 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1253.82 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  90%|▉Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1247.91 ms /    11 runs   (  113.45 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1250.28 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  95%|▉Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1326.08 ms /    12 runs   (  110.51 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1328.67 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " •  \n",
      " •  \n",
      " • i warm\n",
      " • i warm\n",
      " • i hot\n",
      " •  \n",
      " • i sunny\n",
      " • i hot\n",
      " •  \n",
      " • i hot\n",
      " • i warm\n",
      " • they hot\n",
      " •  \n",
      " • i hot\n",
      " •  \n",
      " • i hot\n",
      " • i hot\n",
      " • i warm\n",
      " • i hot\n",
      " • i warm\n",
      " • love warm\n",
      " • love hot\n",
      " • love hot\n",
      " • love hot\n",
      " • love to eat sunny\n",
      " • love sunny\n",
      " • enjoy sunny\n",
      " • love warm\n",
      " • love hot\n",
      " • love hot\n",
      " • love sunny\n",
      " • delight in warm\n",
      " • love hot\n",
      " • adore sunny\n",
      " • love hot\n",
      " • love sunny\n",
      " • love sunny\n",
      " • delight in sunny\n",
      " • relish warm\n",
      " • enjoy sunny\n",
      " • chocolate sweltering\n",
      " • vanilla sunny\n",
      " • chocolate sunny\n",
      " • chocolate sweltering\n",
      " • vanilla sweltering\n",
      " • vanilla sunny\n",
      " • vanilla sunny\n",
      " • chocolate sunny\n",
      " • chocolate hot\n",
      " • vanilla hot\n",
      " • chocolate sunny\n",
      " • vanilla sunny\n",
      " • vanilla warm\n",
      " • vanilla hot\n",
      " • vanilla warm\n",
      " • vanilla hot\n",
      " • vanilla sunny\n",
      " • vanilla sweltering\n",
      " • chocolate sweltering\n",
      " • chocolate sunny\n",
      " • vanilla sunny\n",
      " • ice hot\n",
      " • strawberry sunny\n",
      " • vanilla hot\n",
      " • ice hot\n",
      " • strawberry sunny\n",
      " • vanilla hot\n",
      " • strawberry sunny\n",
      " • strawberry warm\n",
      " • strawberry sunny\n",
      " • ice warm\n",
      " • ice sunny\n",
      " • vanilla sunny\n",
      " • coconut sunny\n",
      " • ice sunny\n",
      " • ice sunny\n",
      " • vanilla sunny\n",
      " • strawberry sunny\n",
      " • ice warm\n",
      " •  \n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream hot\n",
      " • cream hot\n",
      " • cream hot\n",
      " • cream hot\n",
      " • cream sunny\n",
      " • cream hot\n",
      " • cream hot\n",
      " • cream hot\n",
      " • cream hot\n",
      " • cream hot\n",
      " • cream hot\n",
      " • cream hot\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream hot\n",
      " • cones on hot\n",
      " • cones and popsicles during\n",
      " • on hot\n",
      " • cones on hot\n",
      " • on warm sunny\n",
      " • on warm sunny\n",
      " • cones on sunny\n",
      " • cones on sunny\n",
      " • cones and popsicles during\n",
      " • cones on sunny\n",
      " • cones on sunny\n",
      " • cones during hot\n",
      " • cones during sunny\n",
      " • on sunny\n",
      " • cones on sunny\n",
      " • on sunny\n",
      " •  \n",
      " • on warm\n",
      " • in the hot\n",
      " • during hot\n",
      " • sunny summer\n",
      " • hot sunny\n",
      " • hot summer\n",
      " • sunny summer\n",
      " • hot sunny\n",
      " • sunny warm\n",
      " • hot sunny\n",
      " • sunny summer\n",
      " • warm sunny\n",
      " •  \n",
      " •  \n",
      " • hot sunny\n",
      " • sunny summer\n",
      " • warm sunny\n",
      " • hot summer\n",
      " • summer vacation\n",
      " • hot summer\n",
      " • hot sunny\n",
      " • hot sunny\n",
      " • hot summer\n",
      " • hot days\n",
      " • warm days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • warm days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • sunny days\n",
      "\n",
      "Total generated: 160\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "children        PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "love            PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "sweet           PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "ice             PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "cream           PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "on              PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "summer          PMI=   -inf   P(x)=0.100  P(y)=0.400  P(xy)=0.000\n",
      "days            PMI=  0.000   P(x)=0.100  P(y)=1.000  P(xy)=0.100\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 7   |   Anchor word: 'summer'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:   0%| | 0/20Llama.generate: 29 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     568.25 ms /    21 tokens (   27.06 ms per token,    36.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =     159.90 ms /     1 runs   (  159.90 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =     728.87 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          0\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:   5%| | 1/20Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['summer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1451.79 ms /    11 runs   (  131.98 ms per token,     7.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1454.46 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  10%| | 2/20Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1367.15 ms /    11 runs   (  124.29 ms per token,     8.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1369.89 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  15%|▏| 3/20Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     259.38 ms /     2 runs   (  129.69 ms per token,     7.71 tokens per second)\n",
      "llama_perf_context_print:       total time =     260.11 ms /     3 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  20%|▏| 4/20Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['summer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1373.18 ms /    11 runs   (  124.83 ms per token,     8.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1376.12 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  25%|▎| 5/20Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     245.21 ms /     2 runs   (  122.60 ms per token,     8.16 tokens per second)\n",
      "llama_perf_context_print:       total time =     245.90 ms /     3 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  30%|▎| 6/20Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['summer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1343.02 ms /    11 runs   (  122.09 ms per token,     8.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1345.81 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  35%|▎| 7/20Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     249.29 ms /     2 runs   (  124.64 ms per token,     8.02 tokens per second)\n",
      "llama_perf_context_print:       total time =     250.27 ms /     3 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  40%|▍| 8/20Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['summer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1288.18 ms /    11 runs   (  117.11 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1291.89 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  45%|▍| 9/20Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     238.11 ms /     2 runs   (  119.05 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:       total time =     238.78 ms /     3 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  50%|▌| 10/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['summer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1269.96 ms /    11 runs   (  115.45 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1272.50 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  55%|▌| 11/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1331.77 ms /    11 runs   (  121.07 ms per token,     8.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1334.48 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  60%|▌| 12/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1160.75 ms /    10 runs   (  116.08 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1163.20 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  65%|▋| 13/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1169.15 ms /    10 runs   (  116.92 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1171.56 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  70%|▋| 14/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1269.94 ms /    11 runs   (  115.45 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1272.39 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  75%|▊| 15/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1251.56 ms /    11 runs   (  113.78 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1253.99 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  80%|▊| 16/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1270.48 ms /    11 runs   (  115.50 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1272.98 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  85%|▊| 17/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['summer', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     235.21 ms /     2 runs   (  117.60 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =     235.92 ms /     3 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  90%|▉| 18/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['summer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     353.01 ms /     3 runs   (  117.67 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =     353.79 ms /     4 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Processing prompt: [MASK] love sweet ice cream on warm [MASK] days:  95%|▉| 19/2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     229.86 ms /     2 runs   (  114.93 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =     230.48 ms /     3 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', '[mask]', 'days'] ['summer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:   0%| | Llama.generate: 29 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     438.99 ms /    21 tokens (   20.90 ms per token,    47.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1244.00 ms /    11 runs   (  113.09 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1685.65 ms /    32 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:   5%| | Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1254.69 ms /    11 runs   (  114.06 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1257.26 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  10%| | Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1232.08 ms /    11 runs   (  112.01 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1234.43 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  15%|▏| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1243.39 ms /    11 runs   (  113.04 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1245.78 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  20%|▏| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1146.38 ms /    10 runs   (  114.64 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1148.54 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  25%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1266.75 ms /    11 runs   (  115.16 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1269.14 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  30%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1252.23 ms /    11 runs   (  113.84 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1254.59 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  35%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1331.91 ms /    12 runs   (  110.99 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1334.46 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  40%|▍| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1260.61 ms /    11 runs   (  114.60 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1263.26 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  45%|▍| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1254.72 ms /    11 runs   (  114.07 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1257.10 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  50%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1221.65 ms /    11 runs   (  111.06 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1224.07 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  55%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1262.16 ms /    11 runs   (  114.74 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1264.54 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  60%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1237.56 ms /    11 runs   (  112.51 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1239.93 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  65%|▋| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1539.67 ms /    12 runs   (  128.31 ms per token,     7.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1542.33 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  70%|▋| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1397.55 ms /    12 runs   (  116.46 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1400.45 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  75%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1273.86 ms /    11 runs   (  115.81 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1276.35 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  80%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1405.27 ms /    12 runs   (  117.11 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1408.04 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  85%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1338.75 ms /    12 runs   (  111.56 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1341.60 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  90%|▉| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1147.13 ms /    10 runs   (  114.71 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1149.34 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on warm [MASK] days:  95%|▉| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1207.90 ms /    11 runs   (  109.81 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1210.33 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:   0%| | 0Llama.generate: 30 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     462.64 ms /    20 tokens (   23.13 ms per token,    43.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1339.45 ms /    11 runs   (  121.77 ms per token,     8.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1804.80 ms /    31 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:   5%| | 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1472.85 ms /    12 runs   (  122.74 ms per token,     8.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1475.78 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  10%| | 2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1349.57 ms /    11 runs   (  122.69 ms per token,     8.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1351.93 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  15%|▏| 3Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1603.88 ms /    13 runs   (  123.38 ms per token,     8.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1606.80 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  20%|▏| 4Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1529.12 ms /    12 runs   (  127.43 ms per token,     7.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1531.76 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  25%|▎| 5Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1515.74 ms /    12 runs   (  126.31 ms per token,     7.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1518.74 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  30%|▎| 6Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1655.24 ms /    13 runs   (  127.33 ms per token,     7.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1658.24 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  35%|▎| 7Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1462.50 ms /    12 runs   (  121.88 ms per token,     8.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1465.16 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  40%|▍| 8Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1405.22 ms /    12 runs   (  117.10 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1407.97 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  45%|▍| 9Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1292.47 ms /    11 runs   (  117.50 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1295.20 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  50%|▌| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1453.89 ms /    13 runs   (  111.84 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1456.80 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  55%|▌| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1480.33 ms /    13 runs   (  113.87 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1483.14 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  60%|▌| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1443.78 ms /    13 runs   (  111.06 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1446.65 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  65%|▋| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1358.40 ms /    12 runs   (  113.20 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1361.11 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  70%|▋| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1341.56 ms /    12 runs   (  111.80 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1344.19 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  75%|▊| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1327.43 ms /    12 runs   (  110.62 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1330.07 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  80%|▊| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1350.04 ms /    12 runs   (  112.50 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1352.68 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  85%|▊| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1231.20 ms /    11 runs   (  111.93 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1233.62 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  90%|▉| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1218.08 ms /    11 runs   (  110.73 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1220.54 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on warm [MASK] days:  95%|▉| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1217.54 ms /    11 runs   (  110.69 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1220.00 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:   0%| |Llama.generate: 31 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     567.23 ms /    19 tokens (   29.85 ms per token,    33.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1101.28 ms /    10 runs   (  110.13 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1671.23 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:   5%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1335.63 ms /    12 runs   (  111.30 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1338.58 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  10%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1490.76 ms /    13 runs   (  114.67 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1493.86 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  15%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1466.07 ms /    13 runs   (  112.77 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1469.00 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  20%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1208.26 ms /    11 runs   (  109.84 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1211.03 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  25%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1264.23 ms /    11 runs   (  114.93 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1266.70 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  30%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1261.22 ms /    11 runs   (  114.66 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1263.73 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  35%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1361.60 ms /    12 runs   (  113.47 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1364.38 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  40%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1262.51 ms /    11 runs   (  114.77 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1265.03 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  45%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1371.82 ms /    12 runs   (  114.32 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1374.55 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  50%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1341.70 ms /    12 runs   (  111.81 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1344.53 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  55%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1389.69 ms /    12 runs   (  115.81 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1392.71 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  60%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1516.92 ms /    13 runs   (  116.69 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1519.99 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  65%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1206.13 ms /    11 runs   (  109.65 ms per token,     9.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1208.62 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  70%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1382.85 ms /    12 runs   (  115.24 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1385.63 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  75%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1754.20 ms /    13 runs   (  134.94 ms per token,     7.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1757.15 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  80%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1537.67 ms /    12 runs   (  128.14 ms per token,     7.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1540.39 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  85%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1468.36 ms /    12 runs   (  122.36 ms per token,     8.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1471.27 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  90%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1334.11 ms /    11 runs   (  121.28 ms per token,     8.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1336.54 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm [MASK] days:  95%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1326.36 ms /    11 runs   (  120.58 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1328.75 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:   0%| | 0Llama.generate: 32 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     564.22 ms /    18 tokens (   31.35 ms per token,    31.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1522.58 ms /    11 runs   (  138.42 ms per token,     7.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    2090.08 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:   5%| | 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1547.67 ms /    11 runs   (  140.70 ms per token,     7.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1550.16 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  10%| | 2Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1579.20 ms /    11 runs   (  143.56 ms per token,     6.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1581.61 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  15%|▏| 3Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1644.52 ms /    12 runs   (  137.04 ms per token,     7.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1647.28 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  20%|▏| 4Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1641.80 ms /    12 runs   (  136.82 ms per token,     7.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1644.68 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  25%|▎| 5Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1576.32 ms /    12 runs   (  131.36 ms per token,     7.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1578.98 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  30%|▎| 6Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1338.28 ms /    11 runs   (  121.66 ms per token,     8.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1340.69 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  35%|▎| 7Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1249.43 ms /    11 runs   (  113.58 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1251.90 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  40%|▍| 8Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1295.45 ms /    12 runs   (  107.95 ms per token,     9.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1298.22 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  45%|▍| 9Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1167.93 ms /    11 runs   (  106.18 ms per token,     9.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1170.63 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  50%|▌| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1310.82 ms /    12 runs   (  109.24 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1313.59 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  55%|▌| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1321.56 ms /    12 runs   (  110.13 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1324.18 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  60%|▌| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1319.50 ms /    12 runs   (  109.96 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1322.08 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  65%|▋| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1358.75 ms /    12 runs   (  113.23 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1361.32 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  70%|▋| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1306.50 ms /    12 runs   (  108.88 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1309.11 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  75%|▊| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1307.14 ms /    12 runs   (  108.93 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1309.72 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  80%|▊| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1279.81 ms /    12 runs   (  106.65 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1282.42 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  85%|▊| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1331.55 ms /    12 runs   (  110.96 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1334.13 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  90%|▉| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1323.33 ms /    12 runs   (  110.28 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1326.06 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice [MASK] on warm [MASK] days:  95%|▉| 1Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1293.70 ms /    12 runs   (  107.81 ms per token,     9.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1296.38 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:   0%| Llama.generate: 33 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     630.92 ms /    17 tokens (   37.11 ms per token,    26.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1280.13 ms /    10 runs   (  128.01 ms per token,     7.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1913.62 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:   5%| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1582.51 ms /    12 runs   (  131.88 ms per token,     7.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1585.59 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  10%| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1597.90 ms /    12 runs   (  133.16 ms per token,     7.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1600.84 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  15%|▏Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', '[mask]', 'days'] ['children', 'love', 'sweet', 'ice', 'cream', 'on', 'hot', 'sunny', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1550.65 ms /    12 runs   (  129.22 ms per token,     7.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1553.51 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  20%|▏Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', '[mask]', 'days'] ['children', 'love', 'sweet', 'ice', 'cream', 'on', 'hot', 'sunny', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1646.83 ms /    12 runs   (  137.24 ms per token,     7.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1649.77 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  25%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1485.26 ms /    11 runs   (  135.02 ms per token,     7.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1489.09 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  30%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1552.07 ms /    12 runs   (  129.34 ms per token,     7.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1554.98 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  35%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1312.52 ms /    10 runs   (  131.25 ms per token,     7.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1314.80 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  40%|▍Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1499.93 ms /    12 runs   (  124.99 ms per token,     8.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1502.75 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  45%|▍Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1404.77 ms /    12 runs   (  117.06 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1407.58 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  50%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1359.13 ms /    12 runs   (  113.26 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1362.23 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  55%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1317.35 ms /    12 runs   (  109.78 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1320.55 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  60%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', '[mask]', 'days'] ['children', 'love', 'sweet', 'ice', 'cream', 'on', 'hot', 'sunny', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1346.26 ms /    12 runs   (  112.19 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1348.96 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  65%|▋Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', '[mask]', 'days'] ['children', 'love', 'sweet', 'ice', 'cream', 'on', 'hot', 'sunny', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1314.44 ms /    12 runs   (  109.54 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1317.07 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  70%|▋Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', '[mask]', 'days'] ['children', 'love', 'sweet', 'ice', 'cream', 'on', 'hot', 'sunny', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1445.02 ms /    13 runs   (  111.16 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1448.04 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  75%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1436.36 ms /    13 runs   (  110.49 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1439.17 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  80%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', '[mask]', 'days'] ['children', 'love', 'sweet', 'ice', 'cream', 'on', 'hot', 'sunny', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1305.32 ms /    12 runs   (  108.78 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1307.98 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  85%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1605.95 ms /    12 runs   (  133.83 ms per token,     7.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1608.91 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  90%|▉Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1558.60 ms /    12 runs   (  129.88 ms per token,     7.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1561.68 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm [MASK] days:  95%|▉Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1549.03 ms /    12 runs   (  129.09 ms per token,     7.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1551.86 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:   0%| | Llama.generate: 34 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     462.19 ms /    16 tokens (   28.89 ms per token,    34.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1313.32 ms /    11 runs   (  119.39 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1778.81 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:   5%| | Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1441.46 ms /    12 runs   (  120.12 ms per token,     8.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1444.39 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  10%| | Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1463.03 ms /    12 runs   (  121.92 ms per token,     8.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1466.75 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  15%|▏| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1456.46 ms /    12 runs   (  121.37 ms per token,     8.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1459.16 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  20%|▏| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1350.26 ms /    11 runs   (  122.75 ms per token,     8.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1352.72 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  25%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1430.52 ms /    12 runs   (  119.21 ms per token,     8.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1433.24 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  30%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1428.92 ms /    12 runs   (  119.08 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1431.55 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  35%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1468.98 ms /    12 runs   (  122.42 ms per token,     8.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1471.61 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  40%|▍| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1439.79 ms /    12 runs   (  119.98 ms per token,     8.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1442.42 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  45%|▍| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1462.19 ms /    12 runs   (  121.85 ms per token,     8.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1464.87 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  50%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1436.18 ms /    12 runs   (  119.68 ms per token,     8.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1438.83 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  55%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1461.16 ms /    12 runs   (  121.76 ms per token,     8.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1463.83 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  60%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1443.89 ms /    12 runs   (  120.32 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1446.54 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  65%|▋| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1404.66 ms /    12 runs   (  117.06 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1407.28 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  70%|▋| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1399.07 ms /    12 runs   (  116.59 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1401.71 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  75%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1466.63 ms /    12 runs   (  122.22 ms per token,     8.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1469.30 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  80%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1265.50 ms /    11 runs   (  115.05 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1267.94 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  85%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', 'ice', 'cream', 'on', '[mask]', '[mask]', 'days'] ['children', 'love', 'sweet', 'ice', 'cream', 'on', 'summer', 'weekends']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1450.76 ms /    12 runs   (  120.90 ms per token,     8.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1453.51 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  90%|▉| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1431.67 ms /    12 runs   (  119.31 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1434.60 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] [MASK] days:  95%|▉| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1344.40 ms /    12 runs   (  112.03 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1347.09 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:   0%| | Llama.generate: 35 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     607.76 ms /    15 tokens (   40.52 ms per token,    24.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1018.68 ms /     9 runs   (  113.19 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1628.73 ms /    24 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:   5%| | Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1383.98 ms /    12 runs   (  115.33 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1386.65 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  10%| | Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1276.26 ms /    11 runs   (  116.02 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1278.69 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  15%|▏| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1284.06 ms /    11 runs   (  116.73 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1286.53 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  20%|▏| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1395.31 ms /    12 runs   (  116.28 ms per token,     8.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1397.98 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  25%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1499.24 ms /    13 runs   (  115.33 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1502.34 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  30%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1267.01 ms /    11 runs   (  115.18 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1269.40 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  35%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1484.57 ms /    13 runs   (  114.20 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1487.40 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  40%|▍| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1261.86 ms /    11 runs   (  114.71 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1264.25 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  45%|▍| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1513.11 ms /    13 runs   (  116.39 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1515.92 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  50%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1379.14 ms /    12 runs   (  114.93 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1381.76 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  55%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1257.88 ms /    11 runs   (  114.35 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1260.29 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  60%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1233.33 ms /    11 runs   (  112.12 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1235.74 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  65%|▋| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1324.96 ms /    12 runs   (  110.41 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1327.57 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  70%|▋| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1317.64 ms /    12 runs   (  109.80 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1320.27 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  75%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1337.80 ms /    12 runs   (  111.48 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1340.40 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  80%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1227.17 ms /    11 runs   (  111.56 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1229.64 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  85%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1196.00 ms /    11 runs   (  108.73 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1198.40 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  90%|▉| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1211.22 ms /    11 runs   (  110.11 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1213.62 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  95%|▉| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1518.52 ms /    11 runs   (  138.05 ms per token,     7.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1520.97 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " •  \n",
      " •  \n",
      " • i summer\n",
      " •  \n",
      " • i summer\n",
      " •  \n",
      " • i summer\n",
      " •  \n",
      " • i summer\n",
      " •  \n",
      " •  \n",
      " • i summer\n",
      " • summer summer\n",
      " • summer summer\n",
      " •  \n",
      " • i summer\n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • love sunny\n",
      " • love sunny\n",
      " • love sunny\n",
      " • enjoy sunny\n",
      " • love summer\n",
      " • enjoy sunny\n",
      " • love sunny\n",
      " • adore sunny\n",
      " • love sunny\n",
      " • love sunny\n",
      " • love sunny\n",
      " • adore summer\n",
      " • love sunny\n",
      " • enjoy sunny\n",
      " • crave sunny\n",
      " • love sunny\n",
      " • adore sunny\n",
      " • adore sunny\n",
      " • love summer\n",
      " • love sunny\n",
      " • chocolate sunny\n",
      " • chocolate sunny\n",
      " • chocolate summer\n",
      " • vanilla sunny\n",
      " • chocolate sunny\n",
      " • chocolate sunny\n",
      " • vanilla sunny\n",
      " • chocolate sunny\n",
      " • chocolate sunny\n",
      " • chocolate sunny\n",
      " • vanilla sunny\n",
      " • vanilla sunny\n",
      " • vanilla sunny\n",
      " • chocolate sunny\n",
      " • vanilla sunny\n",
      " • chocolate sunny\n",
      " • chocolate sunny\n",
      " • chocolate summer\n",
      " • chocolate sunny\n",
      " • chocolate sunny\n",
      " • chocolate summer\n",
      " • chocolate sunny\n",
      " • dessert sunny\n",
      " • vanilla sunny\n",
      " • ice summer\n",
      " • chocolate summer\n",
      " • ice summer\n",
      " • vanilla summer\n",
      " • chocolate summer\n",
      " • vanilla summer\n",
      " • strawberry summer\n",
      " • chocolate sunny\n",
      " • caramel summer\n",
      " • chocolate summer\n",
      " • vanilla summer\n",
      " • strawberry sunny\n",
      " • strawberry summer\n",
      " • ice sunny\n",
      " • chocolate summer\n",
      " • chocolate summer\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream summer\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream summer\n",
      " • cream summer\n",
      " • cream sunny\n",
      " • cream summer\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • cream sunny\n",
      " • on summer\n",
      " • on sunny\n",
      " •  \n",
      " •  \n",
      " • in the afternoon\n",
      " • on summer\n",
      " • on sunny\n",
      " • sunny \n",
      " • on sunny\n",
      " • on sunny\n",
      " • on sunny \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • in the sun on \n",
      " •  \n",
      " • on sunny\n",
      " • on sunny\n",
      " • on sunny\n",
      " • on sunny\n",
      " • sunny warm\n",
      " • hot sunny\n",
      " • sunny hot\n",
      " • sunny summer\n",
      " • hot summer\n",
      " • hot sunny\n",
      " • warm sunny\n",
      " • warm sunny\n",
      " • sunny warm\n",
      " • hot sunny\n",
      " • hot sunny\n",
      " • sunny warm\n",
      " • warm sunny\n",
      " • hot sunny\n",
      " • sunny summer\n",
      " • hot sunny\n",
      " •  \n",
      " • summer sunny\n",
      " • sunny summer\n",
      " • sunny summer\n",
      " • summer days\n",
      " • sunny days\n",
      " • summer days\n",
      " • summer days\n",
      " • sunny days\n",
      " • summer afternoons\n",
      " • summer evenings\n",
      " • summer afternoons\n",
      " • summer days\n",
      " • summer afternoons\n",
      " • sunny days\n",
      " • summer days\n",
      " • summer days\n",
      " • sunny days\n",
      " • summer evenings\n",
      " • sunny days\n",
      " • summer days\n",
      " • summer days\n",
      " • sunny days\n",
      " • summer days\n",
      "\n",
      "Total generated: 160\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "children        PMI=   -inf   P(x)=0.100  P(y)=0.000  P(xy)=0.000\n",
      "love            PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "sweet           PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "ice             PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "cream           PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "on              PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "warm            PMI=   -inf   P(x)=0.050  P(y)=0.150  P(xy)=0.000\n",
      "days            PMI= -0.222   P(x)=0.700  P(y)=0.750  P(xy)=0.450\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 8   |   Anchor word: 'days'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:   0%| | 0/Llama.generate: 29 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     604.67 ms /    21 tokens (   28.79 ms per token,    34.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1667.93 ms /    15 runs   (  111.20 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    2276.49 ms /    36 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:   5%| | 1/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1510.71 ms /    13 runs   (  116.21 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1513.62 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  10%| | 2/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['he', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'afternoons']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1232.61 ms /    11 runs   (  112.06 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1235.08 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  15%|▏| 3/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['summer', 'nights', 'are', 'perfect', 'for', 'enjoying', 'sweet', 'ice', 'cream']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1600.71 ms /    14 runs   (  114.34 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1603.75 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  20%|▏| 4/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['my', 'love', 'for', 'sweet', 'ice', 'cream', 'increases', 'in', 'warm', 'summer', 'evenings']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1243.76 ms /    11 runs   (  113.07 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1246.17 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  25%|▎| 5/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1373.95 ms /    12 runs   (  114.50 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1376.59 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  30%|▎| 6/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['he', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'evenings']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1266.94 ms /    11 runs   (  115.18 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1269.35 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  35%|▎| 7/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1220.58 ms /    11 runs   (  110.96 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1223.01 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  40%|▍| 8/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['he', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1249.79 ms /    11 runs   (  113.62 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1252.18 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  45%|▍| 9/Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1363.22 ms /    12 runs   (  113.60 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1365.87 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  50%|▌| 10Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['summer', 'evenings', 'are', 'perfect', 'for', 'enjoying', 'sweet', 'ice', 'cream']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1360.05 ms /    12 runs   (  113.34 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1362.64 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  55%|▌| 11Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1245.46 ms /    11 runs   (  113.22 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1248.00 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  60%|▌| 12Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1372.78 ms /    12 runs   (  114.40 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1375.64 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  65%|▋| 13Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['he', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'evenings']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1451.89 ms /    13 runs   (  111.68 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1454.74 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  70%|▋| 14Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'love', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', '[mask]'] ['she', 'loves', 'sweet', 'ice', 'cream', 'on', 'warm', 'summer', 'afternoons']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1365.44 ms /    12 runs   (  113.79 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1368.21 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  75%|▊| 15Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1253.37 ms /    11 runs   (  113.94 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1255.76 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  80%|▊| 16Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1249.02 ms /    11 runs   (  113.55 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1251.45 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  85%|▊| 17Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1474.85 ms /    13 runs   (  113.45 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1477.67 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  90%|▉| 18Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1250.26 ms /    11 runs   (  113.66 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1252.67 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] love sweet ice cream on warm summer [MASK]:  95%|▉| 19Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1250.58 ms /    11 runs   (  113.69 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1252.99 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:   0%| Llama.generate: 29 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     522.52 ms /    21 tokens (   24.88 ms per token,    40.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1123.63 ms /     9 runs   (  124.85 ms per token,     8.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1648.85 ms /    30 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:   5%| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1283.61 ms /    10 runs   (  128.36 ms per token,     7.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1286.44 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  10%| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1360.92 ms /    11 runs   (  123.72 ms per token,     8.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1363.62 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  15%|▏Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1285.60 ms /    10 runs   (  128.56 ms per token,     7.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1287.98 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  20%|▏Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1276.55 ms /    10 runs   (  127.65 ms per token,     7.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1279.09 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  25%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1534.14 ms /    12 runs   (  127.84 ms per token,     7.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1538.12 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  30%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1641.10 ms /    13 runs   (  126.24 ms per token,     7.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1644.05 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  35%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1207.23 ms /    10 runs   (  120.72 ms per token,     8.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1209.46 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  40%|▍Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1280.83 ms /    11 runs   (  116.44 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1283.28 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  45%|▍Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1496.97 ms /    13 runs   (  115.15 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1500.04 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  50%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1126.91 ms /    10 runs   (  112.69 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1129.17 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  55%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1117.53 ms /    10 runs   (  111.75 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1119.70 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  60%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1593.41 ms /    14 runs   (  113.81 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1596.47 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  65%|▋Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1142.06 ms /    10 runs   (  114.21 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1144.50 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  70%|▋Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1352.45 ms /    12 runs   (  112.70 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1355.08 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  75%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1134.42 ms /    10 runs   (  113.44 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1136.58 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  80%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1155.95 ms /    10 runs   (  115.60 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1158.26 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  85%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1578.28 ms /    14 runs   (  112.73 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1581.38 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  90%|▉Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1254.36 ms /    11 runs   (  114.03 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1256.75 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children [MASK] sweet ice cream on warm summer [MASK]:  95%|▉Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1134.22 ms /    10 runs   (  113.42 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1136.39 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:   0%| |Llama.generate: 30 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     592.05 ms /    20 tokens (   29.60 ms per token,    33.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1199.64 ms /    11 runs   (  109.06 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1794.33 ms /    31 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:   5%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1236.83 ms /    11 runs   (  112.44 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1239.22 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  10%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1354.07 ms /    12 runs   (  112.84 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1356.62 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  15%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1228.58 ms /    11 runs   (  111.69 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1230.98 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  20%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1353.48 ms /    12 runs   (  112.79 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1356.03 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  25%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1467.03 ms /    13 runs   (  112.85 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1469.85 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  30%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1438.70 ms /    13 runs   (  110.67 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1441.46 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  35%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1238.10 ms /    11 runs   (  112.55 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1240.43 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  40%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1364.45 ms /    12 runs   (  113.70 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1367.04 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  45%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1241.63 ms /    11 runs   (  112.88 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1244.23 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  50%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1236.29 ms /    11 runs   (  112.39 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1238.98 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  55%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1235.61 ms /    11 runs   (  112.33 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1237.98 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  60%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1557.41 ms /    14 runs   (  111.24 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1560.40 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  65%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1256.42 ms /    11 runs   (  114.22 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1258.79 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  70%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1363.57 ms /    12 runs   (  113.63 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1366.15 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  75%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1244.39 ms /    11 runs   (  113.13 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1246.72 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  80%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1381.63 ms /    12 runs   (  115.14 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1384.24 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  85%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1245.52 ms /    11 runs   (  113.23 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1248.01 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  90%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1259.21 ms /    11 runs   (  114.47 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1261.82 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love [MASK] ice cream on warm summer [MASK]:  95%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1527.84 ms /    14 runs   (  109.13 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1530.82 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:   0%|Llama.generate: 31 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     634.86 ms /    19 tokens (   33.41 ms per token,    29.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1203.61 ms /    10 runs   (  120.36 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1841.03 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:   5%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1460.29 ms /    13 runs   (  112.33 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1463.08 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  10%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1321.81 ms /    12 runs   (  110.15 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1324.54 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  15%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1263.67 ms /    11 runs   (  114.88 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1266.12 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  20%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1203.90 ms /    11 runs   (  109.45 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1206.26 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  25%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1325.94 ms /    12 runs   (  110.50 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1328.66 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  30%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1443.05 ms /    13 runs   (  111.00 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1445.90 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  35%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1222.75 ms /    11 runs   (  111.16 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1225.27 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  40%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1214.55 ms /    11 runs   (  110.41 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1216.98 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  45%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', '[mask]', 'cream', 'on', 'warm', 'summer', '[mask]'] ['children', 'love', 'sweet', 'cotton', 'candy', 'on', 'warm', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1437.84 ms /    13 runs   (  110.60 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1440.64 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  50%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1209.32 ms /    11 runs   (  109.94 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1211.82 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  55%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1256.65 ms /    11 runs   (  114.24 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1259.11 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  60%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1482.45 ms /    13 runs   (  114.03 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1485.54 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  65%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1434.91 ms /    13 runs   (  110.38 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1437.74 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  70%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1372.72 ms /    12 runs   (  114.39 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1375.34 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  75%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1259.17 ms /    11 runs   (  114.47 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1261.55 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  80%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1230.18 ms /    11 runs   (  111.83 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1232.67 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  85%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1380.82 ms /    12 runs   (  115.07 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1383.44 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  90%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1258.22 ms /    11 runs   (  114.38 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1260.79 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet [MASK] cream on warm summer [MASK]:  95%|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1218.36 ms /    11 runs   (  110.76 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1220.82 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:   0%| |Llama.generate: 32 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     454.01 ms /    18 tokens (   25.22 ms per token,    39.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1215.59 ms /    10 runs   (  121.56 ms per token,     8.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1672.30 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:   5%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1388.55 ms /    11 runs   (  126.23 ms per token,     7.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1391.37 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  10%| |Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1272.16 ms /    11 runs   (  115.65 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1274.83 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  15%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1361.15 ms /    11 runs   (  123.74 ms per token,     8.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1363.93 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  20%|▏|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1338.12 ms /    11 runs   (  121.65 ms per token,     8.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1341.72 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  25%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1242.74 ms /    11 runs   (  112.98 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1245.30 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  30%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1346.14 ms /    11 runs   (  122.38 ms per token,     8.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1348.68 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  35%|▎|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1268.85 ms /    11 runs   (  115.35 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1271.37 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  40%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1292.72 ms /    11 runs   (  117.52 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1295.26 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  45%|▍|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1331.08 ms /    11 runs   (  121.01 ms per token,     8.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1333.55 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  50%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1308.29 ms /    11 runs   (  118.94 ms per token,     8.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1310.82 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  55%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1218.15 ms /    11 runs   (  110.74 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1220.94 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  60%|▌|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1241.60 ms /    11 runs   (  112.87 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1244.20 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  65%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1244.48 ms /    11 runs   (  113.13 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1246.91 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  70%|▋|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1195.56 ms /    11 runs   (  108.69 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1197.97 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  75%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1253.08 ms /    11 runs   (  113.92 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1255.59 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  80%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1265.33 ms /    11 runs   (  115.03 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1267.77 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  85%|▊|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1224.42 ms /    11 runs   (  111.31 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1226.82 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  90%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1254.14 ms /    11 runs   (  114.01 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1256.56 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice [MASK] on warm summer [MASK]:  95%|▉|Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1253.29 ms /    11 runs   (  113.94 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1255.71 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:   0%Llama.generate: 33 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     458.80 ms /    17 tokens (   26.99 ms per token,    37.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1389.00 ms /    13 runs   (  106.85 ms per token,     9.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1851.01 ms /    30 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:   5%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1441.02 ms /    13 runs   (  110.85 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1443.86 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  10%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1439.96 ms /    13 runs   (  110.77 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1442.80 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  15%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1391.45 ms /    13 runs   (  107.03 ms per token,     9.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1394.44 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  20%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1106.51 ms /    10 runs   (  110.65 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1108.77 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  25%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', '[mask]'] ['children', 'love', 'sweet', 'ice', 'cream', 'in', 'cold', 'winter']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1200.77 ms /    11 runs   (  109.16 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1203.16 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  30%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', '[mask]'] ['children', 'love', 'sweet', 'ice', 'cream', 'on', 'hot', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1406.97 ms /    13 runs   (  108.23 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1410.07 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  35%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1204.57 ms /    11 runs   (  109.51 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1207.03 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  40%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1395.12 ms /    13 runs   (  107.32 ms per token,     9.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1397.91 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  45%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1615.42 ms /    15 runs   (  107.69 ms per token,     9.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1618.66 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  50%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', '[mask]'] ['children', 'love', 'sweet', 'ice', 'cream', 'cones', 'in', 'warm', 'sunny', 'weather']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1183.60 ms /    11 runs   (  107.60 ms per token,     9.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1185.97 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  55%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1203.55 ms /    11 runs   (  109.41 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1206.06 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  60%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1171.89 ms /    11 runs   (  106.54 ms per token,     9.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1174.38 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  65%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', '[mask]'] ['children', 'love', 'sweet', 'ice', 'cream', 'in', 'cold', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1184.19 ms /    11 runs   (  107.65 ms per token,     9.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1186.87 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  70%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1309.90 ms /    12 runs   (  109.16 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1312.73 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  75%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1415.19 ms /    13 runs   (  108.86 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1418.01 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  80%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1186.59 ms /    11 runs   (  107.87 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1188.98 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  85%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1608.88 ms /    15 runs   (  107.26 ms per token,     9.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1612.16 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  90%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1199.45 ms /    11 runs   (  109.04 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1201.81 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream [MASK] warm summer [MASK]:  95%Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['children', 'love', 'sweet', 'ice', 'cream', '[mask]', 'warm', 'summer', '[mask]'] ['children', 'love', 'sweet', 'ice', 'cream', 'on', 'hot', 'summer', 'days']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1422.44 ms /    13 runs   (  109.42 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1425.23 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:   0%| Llama.generate: 34 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     440.11 ms /    16 tokens (   27.51 ms per token,    36.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1123.74 ms /    10 runs   (  112.37 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1566.53 ms /    26 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:   5%| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1303.70 ms /    11 runs   (  118.52 ms per token,     8.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1306.23 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  10%| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1337.92 ms /    11 runs   (  121.63 ms per token,     8.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1340.61 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  15%|▏Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1345.91 ms /    11 runs   (  122.36 ms per token,     8.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1348.72 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  20%|▏Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1356.22 ms /    12 runs   (  113.02 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1358.95 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  25%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1346.96 ms /    11 runs   (  122.45 ms per token,     8.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1349.54 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  30%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1356.03 ms /    11 runs   (  123.28 ms per token,     8.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1358.68 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  35%|▎Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1280.76 ms /    11 runs   (  116.43 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1283.31 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  40%|▍Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1324.05 ms /    11 runs   (  120.37 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1326.70 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  45%|▍Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1312.82 ms /    11 runs   (  119.35 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1315.44 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  50%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1223.41 ms /    11 runs   (  111.22 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1226.08 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  55%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1271.74 ms /    11 runs   (  115.61 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1274.49 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  60%|▌Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1475.16 ms /    11 runs   (  134.11 ms per token,     7.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1477.75 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  65%|▋Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1274.94 ms /    11 runs   (  115.90 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1277.41 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  70%|▋Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1282.11 ms /    11 runs   (  116.56 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1284.69 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  75%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1269.17 ms /    11 runs   (  115.38 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1272.02 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  80%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1258.03 ms /    11 runs   (  114.37 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1260.49 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  85%|▊Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1257.21 ms /    11 runs   (  114.29 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1259.64 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  90%|▉Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1267.31 ms /    11 runs   (  115.21 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1269.76 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on [MASK] summer [MASK]:  95%|▉Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1258.63 ms /    11 runs   (  114.42 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1261.02 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:   0%| | Llama.generate: 35 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     485.69 ms /    15 tokens (   32.38 ms per token,    30.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1411.96 ms /    12 runs   (  117.66 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1900.54 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:   5%| | Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1293.39 ms /    11 runs   (  117.58 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1296.02 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  10%| | Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1309.82 ms /    11 runs   (  119.07 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1312.17 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  15%|▏| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1327.72 ms /    11 runs   (  120.70 ms per token,     8.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1330.12 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  20%|▏| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1304.38 ms /    11 runs   (  118.58 ms per token,     8.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1306.73 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  25%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1444.61 ms /    12 runs   (  120.38 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1447.21 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  30%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1310.81 ms /    11 runs   (  119.16 ms per token,     8.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1313.37 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  35%|▎| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1306.13 ms /    11 runs   (  118.74 ms per token,     8.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1308.83 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  40%|▍| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1338.40 ms /    12 runs   (  111.53 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1341.00 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  45%|▍| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1283.36 ms /    11 runs   (  116.67 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1285.81 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  50%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1285.60 ms /    11 runs   (  116.87 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1288.02 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  55%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1212.90 ms /    11 runs   (  110.26 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1215.31 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  60%|▌| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1375.02 ms /    12 runs   (  114.59 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1377.67 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  65%|▋| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1364.12 ms /    12 runs   (  113.68 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1366.68 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  70%|▋| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1190.51 ms /    11 runs   (  108.23 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1193.00 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  75%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1242.45 ms /    11 runs   (  112.95 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1244.93 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  80%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1245.51 ms /    11 runs   (  113.23 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1248.10 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  85%|▊| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1220.89 ms /    11 runs   (  110.99 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1223.52 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  90%|▉| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1226.82 ms /    11 runs   (  111.53 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1229.29 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: children love sweet ice cream on warm [MASK] [MASK]:  95%|▉| Llama.generate: 49 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1213.55 ms /    11 runs   (  110.32 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1216.00 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • my warm summer evenings\n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • i days\n",
      " •  \n",
      " • i evenings\n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • i evenings\n",
      " • i days\n",
      " • i days\n",
      " • i afternoons\n",
      " • i days\n",
      " • i days\n",
      " • love days\n",
      " • love days\n",
      " • love days\n",
      " • love days\n",
      " • love days\n",
      " • happily enjoy days\n",
      " • eagerly enjoy days\n",
      " • enjoy days\n",
      " • enjoy evenings\n",
      " • devour afternoons\n",
      " • love days\n",
      " • love days\n",
      " • eagerly eat afternoons\n",
      " • love days\n",
      " • happily eat days\n",
      " • love days\n",
      " • love days\n",
      " • eagerly enjoy afternoons\n",
      " • adore days\n",
      " • love days\n",
      " • vanilla days\n",
      " • chocolate days\n",
      " • vanilla days\n",
      " • chocolate days\n",
      " • vanilla days\n",
      " • vanilla evenings\n",
      " • vanilla evenings\n",
      " • chocolate days\n",
      " • vanilla days\n",
      " • chocolate days\n",
      " • chocolate days\n",
      " • chocolate days\n",
      " • vanilla afternoons\n",
      " • chocolate days\n",
      " • vanilla days\n",
      " • chocolate days\n",
      " • vanilla days\n",
      " • chocolate days\n",
      " • chocolate days\n",
      " • vanilla afternoons\n",
      " • chocolate days\n",
      " • chocolate afternoons\n",
      " • dessert days\n",
      " • chocolate days\n",
      " • candy days\n",
      " • strawberry days\n",
      " • coconut days\n",
      " • chocolate nights\n",
      " •  \n",
      " • coconut days\n",
      " • chocolate nights\n",
      " • chocolate days\n",
      " • coconut days\n",
      " • coconut days\n",
      " • chocolate evenings\n",
      " • cotton days\n",
      " • ice days\n",
      " • strawberry days\n",
      " • chocolate days\n",
      " • ice days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cream days\n",
      " • cones in evenings\n",
      " • cones in days\n",
      " • cones in days\n",
      " • cones on days\n",
      " •  \n",
      " •  \n",
      " • cones during days\n",
      " • during days\n",
      " • cones on days\n",
      " •  \n",
      " • during days\n",
      " • on days\n",
      " •  \n",
      " • in the \n",
      " • in evenings\n",
      " • cones during days\n",
      " • in days\n",
      " • flavored [mask] days\n",
      " •  \n",
      " • cones on days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • sunny days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • hot days\n",
      " • summer afternoons\n",
      " • summer days\n",
      " • summer days\n",
      " • summer days\n",
      " • summer days\n",
      " • sunny days\n",
      " • summer days\n",
      " • summer days\n",
      " • sunny days\n",
      " • summer days\n",
      " • summer days\n",
      " • summer days\n",
      " • sunny days\n",
      " • sunny days\n",
      " • summer days\n",
      " • summer days\n",
      " • summer days\n",
      " • summer evenings\n",
      " • summer days\n",
      " • summer days\n",
      "\n",
      "Total generated: 160\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "children        PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "love            PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "sweet           PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "ice             PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "cream           PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "on              PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "warm            PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "summer          PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "\n",
      "##############################################################################################################\n",
      "ANALYZING SENTENCE:\n",
      "   'plants require sunlight and water to grow'\n",
      "##############################################################################################################\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 0   |   Anchor word: 'plants'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:   0%| | 0/20 [00:00Llama.generate: 29 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     522.02 ms /    19 tokens (   27.47 ms per token,    36.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =     799.99 ms /     7 runs   (  114.28 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1324.28 ms /    26 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:   5%| | 1/20 [00:01Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1173.21 ms /    10 runs   (  117.32 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1175.73 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  10%| | 2/20 [00:02Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1178.92 ms /    10 runs   (  117.89 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1181.21 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  15%|▏| 3/20 [00:03Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     916.74 ms /     8 runs   (  114.59 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =     918.59 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  20%|▏| 4/20 [00:04Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1469.11 ms /    13 runs   (  113.01 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1471.96 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  25%|▎| 5/20 [00:06Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     949.94 ms /     8 runs   (  118.74 ms per token,     8.42 tokens per second)\n",
      "llama_perf_context_print:       total time =     951.75 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  30%|▎| 6/20 [00:07Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1032.60 ms /     9 runs   (  114.73 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1034.57 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  35%|▎| 7/20 [00:08Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'sunlight', 'and', 'water', 'to', 'grow'] ['plants', 'need', 'sunshine', 'and', 'water', 'to', 'grow']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1123.74 ms /    10 runs   (  112.37 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1126.07 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  40%|▍| 8/20 [00:09Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2294.34 ms /    21 runs   (  109.25 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    2299.00 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =         20\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  45%|▍| 9/20 [00:11Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'sunlight', 'and', 'water', 'to', 'grow'] ['the', 'plants', 'need', '[sunlight]', 'and', '[water]', 'in', 'order', 'to', 'grow']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2192.34 ms /    19 runs   (  115.39 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    2196.65 ms /    20 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  50%|▌| 10/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'sunlight', 'and', 'water', 'to', 'grow'] ['tomatoes', 'require', '[sunlight]', 'and', '[water]', 'to', 'grow']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1162.32 ms /    10 runs   (  116.23 ms per token,     8.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1164.54 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  55%|▌| 11/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1107.15 ms /    10 runs   (  110.71 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1109.36 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  60%|▌| 12/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1015.27 ms /     9 runs   (  112.81 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1017.31 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  65%|▋| 13/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1103.34 ms /    10 runs   (  110.33 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1105.55 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  70%|▋| 14/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1094.35 ms /    10 runs   (  109.44 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1096.56 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  75%|▊| 15/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1088.72 ms /    10 runs   (  108.87 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1090.95 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  80%|▊| 16/20 [00:2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1075.85 ms /    10 runs   (  107.58 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1078.03 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  85%|▊| 17/20 [00:2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1096.72 ms /    10 runs   (  109.67 ms per token,     9.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1098.95 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  90%|▉| 18/20 [00:2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1682.93 ms /    13 runs   (  129.46 ms per token,     7.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1685.90 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  95%|▉| 19/20 [00:2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     940.10 ms /     8 runs   (  117.51 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =     941.90 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:   0%| | 0/20 [00:00<Llama.generate: 32 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     388.77 ms /    16 tokens (   24.30 ms per token,    41.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1128.49 ms /     9 runs   (  125.39 ms per token,     7.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1519.69 ms /    25 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:   5%| | 1/20 [00:01<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1255.57 ms /    10 runs   (  125.56 ms per token,     7.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1258.09 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  10%| | 2/20 [00:02<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1238.96 ms /    10 runs   (  123.90 ms per token,     8.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1241.28 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  15%|▏| 3/20 [00:04<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1225.55 ms /    10 runs   (  122.55 ms per token,     8.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1227.85 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  20%|▏| 4/20 [00:05<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1208.07 ms /    10 runs   (  120.81 ms per token,     8.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1210.34 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  25%|▎| 5/20 [00:06<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1242.77 ms /    10 runs   (  124.28 ms per token,     8.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1245.05 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  30%|▎| 6/20 [00:07<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1241.16 ms /    10 runs   (  124.12 ms per token,     8.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1243.71 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  35%|▎| 7/20 [00:08<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1206.02 ms /    10 runs   (  120.60 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1208.73 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  40%|▍| 8/20 [00:10<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1290.59 ms /    10 runs   (  129.06 ms per token,     7.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1292.99 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  45%|▍| 9/20 [00:11<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1182.93 ms /    10 runs   (  118.29 ms per token,     8.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1185.36 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  50%|▌| 10/20 [00:12Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1137.70 ms /    10 runs   (  113.77 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1140.21 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  55%|▌| 11/20 [00:13Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1101.95 ms /    10 runs   (  110.20 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1105.33 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  60%|▌| 12/20 [00:14Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1112.71 ms /    10 runs   (  111.27 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1115.04 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  65%|▋| 13/20 [00:16Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1126.66 ms /    10 runs   (  112.67 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1128.98 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  70%|▋| 14/20 [00:17Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1148.35 ms /    10 runs   (  114.84 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1150.64 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  75%|▊| 15/20 [00:18Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1123.82 ms /    10 runs   (  112.38 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1126.45 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  80%|▊| 16/20 [00:19Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1107.22 ms /    10 runs   (  110.72 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1109.54 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  85%|▊| 17/20 [00:20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1123.12 ms /    10 runs   (  112.31 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1125.41 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  90%|▉| 18/20 [00:21Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1126.94 ms /    10 runs   (  112.69 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1129.27 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  95%|▉| 19/20 [00:22Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1067.66 ms /    10 runs   (  106.77 ms per token,     9.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1069.93 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:   0%| | 0/20 [0Llama.generate: 33 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     600.57 ms /    15 tokens (   40.04 ms per token,    24.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1277.23 ms /     9 runs   (  141.91 ms per token,     7.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1880.20 ms /    24 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:   5%| | 1/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1285.92 ms /    10 runs   (  128.59 ms per token,     7.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1288.20 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  10%| | 2/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1304.02 ms /    10 runs   (  130.40 ms per token,     7.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1306.36 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  15%|▏| 3/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1323.88 ms /    10 runs   (  132.39 ms per token,     7.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1326.28 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  20%|▏| 4/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1311.08 ms /    10 runs   (  131.11 ms per token,     7.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1313.73 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  25%|▎| 5/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1261.71 ms /    10 runs   (  126.17 ms per token,     7.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1264.11 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  30%|▎| 6/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1295.11 ms /    10 runs   (  129.51 ms per token,     7.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1297.47 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  35%|▎| 7/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1271.35 ms /    10 runs   (  127.13 ms per token,     7.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1273.59 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  40%|▍| 8/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1294.67 ms /    10 runs   (  129.47 ms per token,     7.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1297.25 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  45%|▍| 9/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1273.26 ms /    10 runs   (  127.33 ms per token,     7.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1275.74 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  50%|▌| 10/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1222.89 ms /    10 runs   (  122.29 ms per token,     8.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1225.08 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  55%|▌| 11/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1253.70 ms /    10 runs   (  125.37 ms per token,     7.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1255.84 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  60%|▌| 12/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1263.36 ms /    10 runs   (  126.34 ms per token,     7.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1265.52 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  65%|▋| 13/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1236.45 ms /    10 runs   (  123.64 ms per token,     8.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1238.65 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  70%|▋| 14/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1197.55 ms /    10 runs   (  119.75 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1199.74 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  75%|▊| 15/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1195.13 ms /    10 runs   (  119.51 ms per token,     8.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1197.29 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  80%|▊| 16/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1216.53 ms /    10 runs   (  121.65 ms per token,     8.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1218.72 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  85%|▊| 17/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1180.56 ms /    10 runs   (  118.06 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1182.80 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  90%|▉| 18/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1164.71 ms /    10 runs   (  116.47 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1166.85 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  95%|▉| 19/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1203.71 ms /    10 runs   (  120.37 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1205.91 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:   0%| | 0/20 [00:Llama.generate: 34 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     601.90 ms /    14 tokens (   42.99 ms per token,    23.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1015.45 ms /     9 runs   (  112.83 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1619.86 ms /    23 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:   5%| | 1/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1177.32 ms /    10 runs   (  117.73 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1179.68 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  10%| | 2/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1143.97 ms /    10 runs   (  114.40 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1146.54 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  15%|▏| 3/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1108.08 ms /    10 runs   (  110.81 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1110.26 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  20%|▏| 4/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1088.14 ms /    10 runs   (  108.81 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1090.35 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  25%|▎| 5/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1103.39 ms /    10 runs   (  110.34 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1105.53 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  30%|▎| 6/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1119.29 ms /    10 runs   (  111.93 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1121.47 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  35%|▎| 7/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1082.93 ms /    10 runs   (  108.29 ms per token,     9.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1085.04 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  40%|▍| 8/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1372.54 ms /    10 runs   (  137.25 ms per token,     7.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1374.86 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  45%|▍| 9/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1163.34 ms /    10 runs   (  116.33 ms per token,     8.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1165.63 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  50%|▌| 10/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1110.49 ms /    10 runs   (  111.05 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1112.80 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  55%|▌| 11/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1127.21 ms /    10 runs   (  112.72 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1129.68 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  60%|▌| 12/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1108.28 ms /    10 runs   (  110.83 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1110.55 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  65%|▋| 13/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1106.53 ms /    10 runs   (  110.65 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1108.73 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  70%|▋| 14/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1085.50 ms /    10 runs   (  108.55 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1087.81 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  75%|▊| 15/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1060.62 ms /    10 runs   (  106.06 ms per token,     9.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1063.02 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  80%|▊| 16/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1116.83 ms /    10 runs   (  111.68 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1119.05 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  85%|▊| 17/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1077.91 ms /    10 runs   (  107.79 ms per token,     9.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1080.06 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  90%|▉| 18/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1084.72 ms /    10 runs   (  108.47 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1086.88 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  95%|▉| 19/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1074.93 ms /    10 runs   (  107.49 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1077.17 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:   0%| | 0/20 [Llama.generate: 35 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     913.68 ms /    13 tokens (   70.28 ms per token,    14.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =     966.29 ms /     9 runs   (  107.37 ms per token,     9.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1882.26 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:   5%| | 1/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1077.31 ms /    10 runs   (  107.73 ms per token,     9.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1079.62 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  10%| | 2/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1061.74 ms /    10 runs   (  106.17 ms per token,     9.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1064.01 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  15%|▏| 3/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1099.50 ms /    10 runs   (  109.95 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1102.09 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  20%|▏| 4/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1331.21 ms /    12 runs   (  110.93 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1334.63 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  25%|▎| 5/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1075.64 ms /    10 runs   (  107.56 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1077.90 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  30%|▎| 6/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1101.45 ms /    10 runs   (  110.15 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1103.61 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  35%|▎| 7/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1088.27 ms /    10 runs   (  108.83 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1090.59 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  40%|▍| 8/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1073.75 ms /    10 runs   (  107.37 ms per token,     9.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1075.98 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  45%|▍| 9/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1066.36 ms /    10 runs   (  106.64 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1068.53 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  50%|▌| 10/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1073.11 ms /    10 runs   (  107.31 ms per token,     9.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1075.76 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  55%|▌| 11/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1086.92 ms /    10 runs   (  108.69 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1089.10 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  60%|▌| 12/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1076.89 ms /    10 runs   (  107.69 ms per token,     9.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1079.54 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  65%|▋| 13/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1085.00 ms /    10 runs   (  108.50 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1087.21 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  70%|▋| 14/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1086.39 ms /    10 runs   (  108.64 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1088.72 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  75%|▊| 15/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1082.19 ms /    10 runs   (  108.22 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1084.34 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  80%|▊| 16/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1103.22 ms /    10 runs   (  110.32 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1105.64 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  85%|▊| 17/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1090.96 ms /    10 runs   (  109.10 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1093.14 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  90%|▉| 18/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1077.14 ms /    10 runs   (  107.71 ms per token,     9.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1079.30 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  95%|▉| 19/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1109.89 ms /    10 runs   (  110.99 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1112.06 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:   0%| | 0/20 [00Llama.generate: 36 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     723.52 ms /    12 tokens (   60.29 ms per token,    16.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1059.57 ms /     9 runs   (  117.73 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1785.58 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:   5%| | 1/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1224.17 ms /    10 runs   (  122.42 ms per token,     8.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1226.36 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  10%| | 2/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1649.26 ms /    14 runs   (  117.80 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1652.62 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  15%|▏| 3/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1141.89 ms /    10 runs   (  114.19 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1144.12 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  20%|▏| 4/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1554.32 ms /    14 runs   (  111.02 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1557.27 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  25%|▎| 5/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1135.29 ms /    10 runs   (  113.53 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1137.44 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  30%|▎| 6/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1630.43 ms /    14 runs   (  116.46 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1633.43 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  35%|▎| 7/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1626.26 ms /    14 runs   (  116.16 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1629.30 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  40%|▍| 8/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1102.46 ms /    10 runs   (  110.25 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1104.67 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  45%|▍| 9/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1516.63 ms /    14 runs   (  108.33 ms per token,     9.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1519.74 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  50%|▌| 10/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1052.12 ms /    10 runs   (  105.21 ms per token,     9.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1054.33 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  55%|▌| 11/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1031.42 ms /    10 runs   (  103.14 ms per token,     9.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1033.54 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  60%|▌| 12/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1072.92 ms /    10 runs   (  107.29 ms per token,     9.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1075.67 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  65%|▋| 13/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1481.55 ms /    14 runs   (  105.82 ms per token,     9.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1484.54 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  70%|▋| 14/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1097.52 ms /    10 runs   (  109.75 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1099.76 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  75%|▊| 15/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1097.46 ms /    10 runs   (  109.75 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1099.91 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  80%|▊| 16/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1101.35 ms /    10 runs   (  110.14 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1103.45 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  85%|▊| 17/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1080.43 ms /    10 runs   (  108.04 ms per token,     9.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1082.61 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  90%|▉| 18/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1779.21 ms /    14 runs   (  127.09 ms per token,     7.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1782.37 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  95%|▉| 19/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1735.72 ms /    14 runs   (  123.98 ms per token,     8.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1739.05 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • plants require\n",
      " • plants require\n",
      " • plants need\n",
      " • plants need\n",
      " • tomatoes and lettuce need\n",
      " • plants need\n",
      " •  \n",
      " • plants require\n",
      " •  \n",
      " •  \n",
      " • plants absorb\n",
      " • plants require\n",
      " • plants absorb\n",
      " • tomatoes need\n",
      " • plants absorb\n",
      " • plants require\n",
      " • the plants need\n",
      " • plants require\n",
      " • the plant requires soil\n",
      " • plants require\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants in order to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants grow\n",
      " • plants grow\n",
      " • plants photosynthesize\n",
      " • plants grow\n",
      " • plants photosynthesize\n",
      " • plants grow\n",
      " • plants photosynthesize\n",
      " • plants photosynthesize\n",
      " • plants grow\n",
      " • plants photosynthesize\n",
      " • plants grow\n",
      " • plants grow\n",
      " • plants grow\n",
      " • plants photosynthesize\n",
      " • plants grow\n",
      " • plants grow\n",
      " • plants grow\n",
      " • plants grow\n",
      " • plants photosynthesize\n",
      " • plants photosynthesize\n",
      "\n",
      "Total generated: 120\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "require         PMI=  0.621   P(x)=0.650  P(y)=0.350  P(xy)=0.350\n",
      "sunlight        PMI=  0.000   P(x)=1.000  P(y)=1.000  P(xy)=1.000\n",
      "and             PMI=  0.000   P(x)=1.000  P(y)=1.000  P(xy)=1.000\n",
      "water           PMI=  0.000   P(x)=1.000  P(y)=1.000  P(xy)=1.000\n",
      "to              PMI=  0.000   P(x)=1.000  P(y)=0.950  P(xy)=0.950\n",
      "grow            PMI=  0.000   P(x)=1.000  P(y)=0.600  P(xy)=0.600\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 1   |   Anchor word: 'require'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:   0%| | 0/20 [00:00Llama.generate: 32 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     729.00 ms /    16 tokens (   45.56 ms per token,    21.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =     910.99 ms /     7 runs   (  130.14 ms per token,     7.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1641.97 ms /    23 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:   5%| | 1/20 [00:01Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1244.57 ms /    10 runs   (  124.46 ms per token,     8.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1247.35 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  10%| | 2/20 [00:02Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1219.89 ms /    10 runs   (  121.99 ms per token,     8.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1222.16 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  15%|▏| 3/20 [00:04Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1326.65 ms /    11 runs   (  120.60 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1329.03 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  20%|▏| 4/20 [00:05Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1239.12 ms /    10 runs   (  123.91 ms per token,     8.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1241.38 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  25%|▎| 5/20 [00:06Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1109.89 ms /     9 runs   (  123.32 ms per token,     8.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1112.02 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  30%|▎| 6/20 [00:07Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1134.74 ms /     9 runs   (  126.08 ms per token,     7.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1137.05 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  35%|▎| 7/20 [00:08Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1233.09 ms /    10 runs   (  123.31 ms per token,     8.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1235.28 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  40%|▍| 8/20 [00:10Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     960.61 ms /     8 runs   (  120.08 ms per token,     8.33 tokens per second)\n",
      "llama_perf_context_print:       total time =     962.39 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  45%|▍| 9/20 [00:11Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1185.61 ms /    10 runs   (  118.56 ms per token,     8.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1187.89 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  50%|▌| 10/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1470.15 ms /    13 runs   (  113.09 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1473.01 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  55%|▌| 11/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1175.85 ms /    10 runs   (  117.59 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1178.22 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  60%|▌| 12/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1161.08 ms /    10 runs   (  116.11 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1163.27 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  65%|▋| 13/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1152.82 ms /    10 runs   (  115.28 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1155.01 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  70%|▋| 14/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     937.88 ms /     8 runs   (  117.23 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =     939.69 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  75%|▊| 15/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1174.68 ms /    10 runs   (  117.47 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1176.85 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  80%|▊| 16/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1172.28 ms /    10 runs   (  117.23 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1174.45 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  85%|▊| 17/20 [00:2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     931.41 ms /     8 runs   (  116.43 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =     933.21 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  90%|▉| 18/20 [00:2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1179.89 ms /    10 runs   (  117.99 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1182.37 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] sunlight and water to grow:  95%|▉| 19/20 [00:2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1162.31 ms /    10 runs   (  116.23 ms per token,     8.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1164.55 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:   0%| | 0/20 [00:00<?Llama.generate: 29 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     525.24 ms /    19 tokens (   27.64 ms per token,    36.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1573.02 ms /    14 runs   (  112.36 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    2101.48 ms /    33 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:   5%| | 1/20 [00:02<0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1152.29 ms /    10 runs   (  115.23 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1154.51 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  10%| | 2/20 [00:03<0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1622.67 ms /    14 runs   (  115.91 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1625.75 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  15%|▏| 3/20 [00:04<0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     895.58 ms /     8 runs   (  111.95 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =     897.63 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  20%|▏| 4/20 [00:05<0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1508.37 ms /    13 runs   (  116.03 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1511.33 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  25%|▎| 5/20 [00:07<0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     933.31 ms /     8 runs   (  116.66 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =     935.15 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  30%|▎| 6/20 [00:08<0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1497.09 ms /    13 runs   (  115.16 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1500.00 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  35%|▎| 7/20 [00:09<0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1745.27 ms /    15 runs   (  116.35 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1748.54 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  40%|▍| 8/20 [00:11<0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1734.64 ms /    15 runs   (  115.64 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1738.01 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  45%|▍| 9/20 [00:13<0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1506.48 ms /    13 runs   (  115.88 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1509.35 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  50%|▌| 10/20 [00:14<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1524.34 ms /    13 runs   (  117.26 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1527.29 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  55%|▌| 11/20 [00:16<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1482.05 ms /    13 runs   (  114.00 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1484.91 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  60%|▌| 12/20 [00:17<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1515.57 ms /    13 runs   (  116.58 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1518.47 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  65%|▋| 13/20 [00:19<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     932.59 ms /     8 runs   (  116.57 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =     934.46 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  70%|▋| 14/20 [00:20<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     933.63 ms /     8 runs   (  116.70 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =     935.50 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  75%|▊| 15/20 [00:21<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     936.23 ms /     8 runs   (  117.03 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =     938.07 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  80%|▊| 16/20 [00:22<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1621.05 ms /    14 runs   (  115.79 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1624.10 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  85%|▊| 17/20 [00:23<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2483.96 ms /    22 runs   (  112.91 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    2488.76 ms /    23 tokens\n",
      "llama_perf_context_print:    graphs reused =         21\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  90%|▉| 18/20 [00:26<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['plants', '[mask]', '[mask]', 'and', 'water', 'to', 'grow'] ['plants', 'absorb', '[carbon', 'dioxide]', 'and', '[water]', 'to', 'grow']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     930.88 ms /     8 runs   (  116.36 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =     932.84 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  95%|▉| 19/20 [00:27<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1739.53 ms /    15 runs   (  115.97 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1742.86 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:   0%| | 0/20 [00Llama.generate: 33 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     551.98 ms /    15 tokens (   36.80 ms per token,    27.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =     803.71 ms /     7 runs   (  114.82 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1357.70 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:   5%| | 1/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     834.10 ms /     7 runs   (  119.16 ms per token,     8.39 tokens per second)\n",
      "llama_perf_context_print:       total time =     835.71 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  10%| | 2/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1076.42 ms /     9 runs   (  119.60 ms per token,     8.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1078.52 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  15%|▏| 3/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1162.24 ms /    10 runs   (  116.22 ms per token,     8.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1164.54 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  20%|▏| 4/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1185.49 ms /     8 runs   (  148.19 ms per token,     6.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1187.34 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  25%|▎| 5/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     803.94 ms /     7 runs   (  114.85 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =     805.54 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  30%|▎| 6/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     807.80 ms /     7 runs   (  115.40 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =     809.45 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  35%|▎| 7/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1138.53 ms /    10 runs   (  113.85 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1140.87 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  40%|▍| 8/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     923.48 ms /     8 runs   (  115.43 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =     925.41 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  45%|▍| 9/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     816.98 ms /     7 runs   (  116.71 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =     818.59 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  50%|▌| 10/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     895.21 ms /     8 runs   (  111.90 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =     897.00 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  55%|▌| 11/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     793.69 ms /     7 runs   (  113.38 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =     795.23 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  60%|▌| 12/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     794.84 ms /     7 runs   (  113.55 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =     796.36 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  65%|▋| 13/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     922.73 ms /     8 runs   (  115.34 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =     924.44 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  70%|▋| 14/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1208.44 ms /    11 runs   (  109.86 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1210.77 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  75%|▊| 15/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1169.11 ms /    10 runs   (  116.91 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1171.29 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  80%|▊| 16/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     906.97 ms /     8 runs   (  113.37 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =     908.69 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  85%|▊| 17/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1289.60 ms /    11 runs   (  117.24 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1292.00 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  90%|▉| 18/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     911.12 ms /     8 runs   (  113.89 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =     913.05 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  95%|▉| 19/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1129.39 ms /    10 runs   (  112.94 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1131.55 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:   0%| | 0/20 [00:0Llama.generate: 34 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     498.70 ms /    14 tokens (   35.62 ms per token,    28.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1073.99 ms /     9 runs   (  119.33 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1574.95 ms /    23 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:   5%| | 1/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1137.55 ms /    10 runs   (  113.76 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1139.76 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  10%| | 2/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     944.38 ms /     8 runs   (  118.05 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =     946.15 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  15%|▏| 3/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     924.27 ms /     8 runs   (  115.53 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =     926.18 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  20%|▏| 4/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     931.32 ms /     8 runs   (  116.41 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =     933.31 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  25%|▎| 5/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     931.96 ms /     8 runs   (  116.50 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =     933.72 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  30%|▎| 6/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1174.01 ms /    10 runs   (  117.40 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1176.40 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  35%|▎| 7/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1111.63 ms /    10 runs   (  111.16 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1113.74 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  40%|▍| 8/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1169.65 ms /    10 runs   (  116.96 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1171.77 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  45%|▍| 9/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     923.02 ms /     8 runs   (  115.38 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =     924.81 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  50%|▌| 10/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1125.80 ms /    10 runs   (  112.58 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1127.90 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  55%|▌| 11/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1152.69 ms /    10 runs   (  115.27 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1154.79 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  60%|▌| 12/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     924.52 ms /     8 runs   (  115.56 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =     926.53 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  65%|▋| 13/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1154.63 ms /    10 runs   (  115.46 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1156.76 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  70%|▋| 14/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     908.53 ms /     8 runs   (  113.57 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =     910.60 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  75%|▊| 15/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     922.95 ms /     8 runs   (  115.37 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =     924.72 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  80%|▊| 16/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1140.46 ms /    10 runs   (  114.05 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1142.63 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  85%|▊| 17/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1152.77 ms /    10 runs   (  115.28 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1155.35 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  90%|▉| 18/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     906.36 ms /     8 runs   (  113.29 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =     908.25 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  95%|▉| 19/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1158.90 ms /    10 runs   (  115.89 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1161.15 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:   0%| | 0/20 [0Llama.generate: 35 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     534.40 ms /    13 tokens (   41.11 ms per token,    24.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1069.83 ms /     9 runs   (  118.87 ms per token,     8.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1606.64 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:   5%| | 1/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1139.97 ms /    10 runs   (  114.00 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1142.15 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  10%| | 2/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     915.77 ms /     8 runs   (  114.47 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =     917.77 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  15%|▏| 3/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     923.73 ms /     8 runs   (  115.47 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =     925.71 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  20%|▏| 4/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1153.10 ms /    10 runs   (  115.31 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1155.35 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  25%|▎| 5/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     900.83 ms /     8 runs   (  112.60 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =     902.69 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  30%|▎| 6/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1143.85 ms /    10 runs   (  114.38 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1146.04 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  35%|▎| 7/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     922.00 ms /     8 runs   (  115.25 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =     923.89 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  40%|▍| 8/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     923.77 ms /     8 runs   (  115.47 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =     925.88 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  45%|▍| 9/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1135.38 ms /    10 runs   (  113.54 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1137.56 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  50%|▌| 10/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     896.45 ms /     8 runs   (  112.06 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =     898.18 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  55%|▌| 11/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1110.81 ms /    10 runs   (  111.08 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1112.94 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  60%|▌| 12/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1107.21 ms /    10 runs   (  110.72 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1109.34 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  65%|▋| 13/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     892.48 ms /     8 runs   (  111.56 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =     894.22 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  70%|▋| 14/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     893.99 ms /     8 runs   (  111.75 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =     895.74 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  75%|▊| 15/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     899.74 ms /     8 runs   (  112.47 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =     901.52 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  80%|▊| 16/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1112.84 ms /    10 runs   (  111.28 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1115.00 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  85%|▊| 17/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1114.18 ms /    10 runs   (  111.42 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1116.30 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  90%|▉| 18/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     913.63 ms /     8 runs   (  114.20 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =     915.65 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  95%|▉| 19/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1357.39 ms /    12 runs   (  113.12 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1359.96 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:   0%| | 0/20 [00:Llama.generate: 36 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     607.15 ms /    12 tokens (   50.60 ms per token,    19.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1057.34 ms /     7 runs   (  151.05 ms per token,     6.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1666.70 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:   5%| | 1/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1267.85 ms /    10 runs   (  126.78 ms per token,     7.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1270.62 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  10%| | 2/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1234.29 ms /    10 runs   (  123.43 ms per token,     8.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1236.76 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  15%|▏| 3/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     993.81 ms /     8 runs   (  124.23 ms per token,     8.05 tokens per second)\n",
      "llama_perf_context_print:       total time =     995.83 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  20%|▏| 4/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1670.11 ms /    14 runs   (  119.29 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1673.49 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  25%|▎| 5/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1229.44 ms /    10 runs   (  122.94 ms per token,     8.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1232.08 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  30%|▎| 6/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1001.17 ms /     8 runs   (  125.15 ms per token,     7.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1003.33 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  35%|▎| 7/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     986.96 ms /     8 runs   (  123.37 ms per token,     8.11 tokens per second)\n",
      "llama_perf_context_print:       total time =     989.90 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  40%|▍| 8/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1598.39 ms /    13 runs   (  122.95 ms per token,     8.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1601.49 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  45%|▍| 9/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     921.80 ms /     8 runs   (  115.22 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =     923.71 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  50%|▌| 10/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1383.29 ms /    12 runs   (  115.27 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1386.07 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  55%|▌| 11/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     917.55 ms /     8 runs   (  114.69 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =     919.52 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  60%|▌| 12/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     907.28 ms /     8 runs   (  113.41 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =     909.52 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  65%|▋| 13/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1534.21 ms /    14 runs   (  109.59 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1537.66 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  70%|▋| 14/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1435.21 ms /    13 runs   (  110.40 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1438.49 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  75%|▊| 15/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     861.03 ms /     8 runs   (  107.63 ms per token,     9.29 tokens per second)\n",
      "llama_perf_context_print:       total time =     862.86 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  80%|▊| 16/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1366.35 ms /    13 runs   (  105.10 ms per token,     9.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1369.26 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  85%|▊| 17/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     839.62 ms /     8 runs   (  104.95 ms per token,     9.53 tokens per second)\n",
      "llama_perf_context_print:       total time =     841.46 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  90%|▉| 18/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     855.62 ms /     8 runs   (  106.95 ms per token,     9.35 tokens per second)\n",
      "llama_perf_context_print:       total time =     857.51 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  95%|▉| 19/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1189.81 ms /    11 runs   (  108.16 ms per token,     9.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1192.33 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • plants require\n",
      " • plants use\n",
      " • plants utilize\n",
      " • tomato plants need\n",
      " • plants absorb\n",
      " • plants require\n",
      " • plants require\n",
      " • plants require\n",
      " • plants require\n",
      " • plants utilize\n",
      " • tomatoes and lettuce need\n",
      " • plants need\n",
      " • plants need\n",
      " • plants absorb\n",
      " • plants need\n",
      " • plants use\n",
      " • plants use\n",
      " • plants require\n",
      " • tomatoes need\n",
      " • plants require\n",
      " • require sunlight water\n",
      " • need sunlight\n",
      " • require sunlight [mask] nutrients\n",
      " • need sunlight\n",
      " • need sunlight soil\n",
      " • require sunlight\n",
      " • need sunlight [mask]\n",
      " • need sunlight nutrients\n",
      " • require sunlight nutrients\n",
      " • require sunlight soil\n",
      " • need sunlight soil\n",
      " • need sunlight soil\n",
      " • need light soil\n",
      " • need sunlight\n",
      " • require sunlight\n",
      " • require sunlight\n",
      " • require sunlight [mask] [mask]\n",
      " •  \n",
      " • need sunlight\n",
      " • require sunlight water\n",
      " • absorb and\n",
      " • need \n",
      " • absorb and require\n",
      " • need and\n",
      " • absorb and\n",
      " • need \n",
      " • absorb \n",
      " • need and\n",
      " • absorb and\n",
      " • absorb \n",
      " • require and\n",
      " • need \n",
      " • need \n",
      " • absorb and\n",
      " • absorb and require\n",
      " • require and\n",
      " • require and\n",
      " • absorb and require\n",
      " • require and\n",
      " • need and\n",
      " • require water\n",
      " • need water\n",
      " • need water\n",
      " • require water\n",
      " • absorb water\n",
      " • require water\n",
      " • require water\n",
      " • need water\n",
      " • require water\n",
      " • require water\n",
      " • need water\n",
      " • absorb water\n",
      " • require water\n",
      " • require water\n",
      " • require water\n",
      " • require water\n",
      " • require water\n",
      " • require water\n",
      " • need water\n",
      " • require water\n",
      " • require to\n",
      " • require to\n",
      " • require to\n",
      " • need to\n",
      " • require to\n",
      " • require to\n",
      " • require to\n",
      " • need to\n",
      " • require to\n",
      " • require to\n",
      " • need to\n",
      " • require to\n",
      " • need to\n",
      " • need to\n",
      " • require to\n",
      " • require to\n",
      " • need to\n",
      " • need to\n",
      " • require to\n",
      " • require in order to\n",
      " • require grow\n",
      " • absorb grow\n",
      " • utilize grow\n",
      " • absorb grow\n",
      " • require photosynthesize\n",
      " • absorb grow\n",
      " • require grow\n",
      " • require grow\n",
      " • require photosynthesize\n",
      " • need grow\n",
      " • require photosynthesize\n",
      " • need grow\n",
      " • require grow\n",
      " • absorb photosynthesize\n",
      " • absorb photosynthesize\n",
      " • absorb grow\n",
      " • absorb grow and thrive\n",
      " • need grow\n",
      " • need grow\n",
      " • absorb grow and thrive\n",
      "\n",
      "Total generated: 120\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "plants          PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "sunlight        PMI= -0.070   P(x)=0.450  P(y)=0.350  P(xy)=0.150\n",
      "and             PMI=  0.862   P(x)=0.200  P(y)=0.550  P(xy)=0.200\n",
      "water           PMI=  0.000   P(x)=0.650  P(y)=1.000  P(xy)=0.650\n",
      "to              PMI= -0.041   P(x)=0.650  P(y)=0.950  P(xy)=0.600\n",
      "grow            PMI= -0.186   P(x)=0.350  P(y)=0.650  P(xy)=0.200\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 2   |   Anchor word: 'sunlight'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] require [MASK] and water to grow:   0%| | 0/20 [00:00<Llama.generate: 29 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     751.27 ms /    19 tokens (   39.54 ms per token,    25.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1016.87 ms /     9 runs   (  112.98 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1770.46 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:   5%| | 1/20 [00:01<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1185.04 ms /    10 runs   (  118.50 ms per token,     8.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1187.37 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  10%| | 2/20 [00:02<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1188.01 ms /    10 runs   (  118.80 ms per token,     8.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1190.32 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  15%|▏| 3/20 [00:04<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1175.15 ms /    10 runs   (  117.52 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1177.48 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  20%|▏| 4/20 [00:05<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1215.55 ms /    10 runs   (  121.55 ms per token,     8.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1217.80 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  25%|▎| 5/20 [00:06<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1248.42 ms /    10 runs   (  124.84 ms per token,     8.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1250.65 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  30%|▎| 6/20 [00:07<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1195.82 ms /    10 runs   (  119.58 ms per token,     8.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1198.05 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  35%|▎| 7/20 [00:09<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1171.71 ms /    10 runs   (  117.17 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1173.92 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  40%|▍| 8/20 [00:10<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1167.69 ms /    10 runs   (  116.77 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1169.92 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  45%|▍| 9/20 [00:11<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1167.27 ms /    10 runs   (  116.73 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1169.50 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  50%|▌| 10/20 [00:12Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1191.76 ms /    10 runs   (  119.18 ms per token,     8.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1193.96 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  55%|▌| 11/20 [00:13Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1181.24 ms /    10 runs   (  118.12 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1183.49 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  60%|▌| 12/20 [00:14Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1262.15 ms /    10 runs   (  126.21 ms per token,     7.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1264.39 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  65%|▋| 13/20 [00:16Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1185.79 ms /    10 runs   (  118.58 ms per token,     8.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1187.97 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  70%|▋| 14/20 [00:17Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1220.99 ms /    10 runs   (  122.10 ms per token,     8.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1223.35 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  75%|▊| 15/20 [00:18Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1255.70 ms /    10 runs   (  125.57 ms per token,     7.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1257.95 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  80%|▊| 16/20 [00:19Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1225.28 ms /    10 runs   (  122.53 ms per token,     8.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1227.48 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  85%|▊| 17/20 [00:21Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1179.06 ms /    10 runs   (  117.91 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1181.25 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  90%|▉| 18/20 [00:22Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1197.61 ms /    10 runs   (  119.76 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1199.84 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require [MASK] and water to grow:  95%|▉| 19/20 [00:23Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1189.19 ms /    10 runs   (  118.92 ms per token,     8.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1191.58 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:   0%| | 0/20 [00:00<?Llama.generate: 29 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     558.65 ms /    19 tokens (   29.40 ms per token,    34.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1169.81 ms /    10 runs   (  116.98 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1731.00 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:   5%| | 1/20 [00:01<0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1008.93 ms /     8 runs   (  126.12 ms per token,     7.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1011.09 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  10%| | 2/20 [00:02<0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1019.53 ms /     8 runs   (  127.44 ms per token,     7.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1021.42 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  15%|▏| 3/20 [00:03<0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1823.28 ms /    15 runs   (  121.55 ms per token,     8.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1826.56 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  20%|▏| 4/20 [00:05<0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2707.52 ms /    23 runs   (  117.72 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    2712.62 ms /    24 tokens\n",
      "llama_perf_context_print:    graphs reused =         22\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  25%|▎| 5/20 [00:08<0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['plants', '[mask]', '[mask]', 'and', 'water', 'to', 'grow'] ['plants', 'absorb', '[carbon', 'dioxide]', 'and', 'release', '[oxygen]', 'to', 'grow']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1777.42 ms /    15 runs   (  118.49 ms per token,     8.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1780.73 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  30%|▎| 6/20 [00:10<0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     958.41 ms /     8 runs   (  119.80 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:       total time =     960.23 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  35%|▎| 7/20 [00:11<0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1615.29 ms /    13 runs   (  124.25 ms per token,     8.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1618.11 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  40%|▍| 8/20 [00:12<0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1931.54 ms /    15 runs   (  128.77 ms per token,     7.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1934.91 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  45%|▍| 9/20 [00:14<0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1714.74 ms /    15 runs   (  114.32 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1718.22 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  50%|▌| 10/20 [00:16<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1401.28 ms /    13 runs   (  107.79 ms per token,     9.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1404.21 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  55%|▌| 11/20 [00:17<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1525.35 ms /    14 runs   (  108.95 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1528.45 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  60%|▌| 12/20 [00:19<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     916.62 ms /     8 runs   (  114.58 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =     918.71 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  65%|▋| 13/20 [00:20<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1236.42 ms /    11 runs   (  112.40 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1238.92 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  70%|▋| 14/20 [00:21<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     856.16 ms /     8 runs   (  107.02 ms per token,     9.34 tokens per second)\n",
      "llama_perf_context_print:       total time =     857.98 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  75%|▊| 15/20 [00:22<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1403.37 ms /    13 runs   (  107.95 ms per token,     9.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1406.18 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  80%|▊| 16/20 [00:23<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     873.19 ms /     8 runs   (  109.15 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =     875.03 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  85%|▊| 17/20 [00:24<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     898.58 ms /     8 runs   (  112.32 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =     900.39 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  90%|▉| 18/20 [00:25<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1210.73 ms /    11 runs   (  110.07 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1213.24 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: plants [MASK] [MASK] and water to grow:  95%|▉| 19/20 [00:26<Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1429.64 ms /    13 runs   (  109.97 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1432.68 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:   0%| | 0/20 [00:Llama.generate: 30 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     550.27 ms /    18 tokens (   30.57 ms per token,    32.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1096.97 ms /    10 runs   (  109.70 ms per token,     9.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1650.32 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:   5%| | 1/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1338.59 ms /    12 runs   (  111.55 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1341.34 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  10%| | 2/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['plants', 'require', '[mask]', '[mask]', 'water', 'to', 'grow'] ['plants', 'require', 'specific', 'nutrients', 'and', 'temperature', 'to', 'grow']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1341.64 ms /    12 runs   (  111.80 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1344.34 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  15%|▏| 3/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['plants', 'require', '[mask]', '[mask]', 'water', 'to', 'grow'] ['plants', 'require', 'nutrients', 'and', 'sunlight', 'to', 'grow']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1046.66 ms /    10 runs   (  104.67 ms per token,     9.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1048.97 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  20%|▏| 4/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1315.62 ms /    12 runs   (  109.63 ms per token,     9.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1318.40 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  25%|▎| 5/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1107.74 ms /    10 runs   (  110.77 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1110.14 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  30%|▎| 6/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1084.58 ms /    10 runs   (  108.46 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1087.01 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  35%|▎| 7/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1634.62 ms /    15 runs   (  108.97 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1638.12 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  40%|▍| 8/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1437.50 ms /    13 runs   (  110.58 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1440.45 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  45%|▍| 9/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['plants', 'require', '[mask]', '[mask]', 'water', 'to', 'grow'] ['plants', 'require', 'essential', 'nutrients', 'and', 'sunlight', 'to', 'grow']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1070.38 ms /    10 runs   (  107.04 ms per token,     9.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1072.68 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  50%|▌| 10/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1109.94 ms /    10 runs   (  110.99 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1112.87 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  55%|▌| 11/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1375.82 ms /    13 runs   (  105.83 ms per token,     9.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1378.88 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  60%|▌| 12/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1097.54 ms /    10 runs   (  109.75 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1099.97 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  65%|▋| 13/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['plants', 'require', '[mask]', '[mask]', 'water', 'to', 'grow'] ['plants', 'require', 'nutrients', 'to', 'grow']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1107.69 ms /    10 runs   (  110.77 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1110.06 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  70%|▋| 14/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1095.06 ms /    10 runs   (  109.51 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1097.40 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  75%|▊| 15/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1214.05 ms /    11 runs   (  110.37 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1217.24 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  80%|▊| 16/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1061.24 ms /    10 runs   (  106.12 ms per token,     9.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1063.68 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  85%|▊| 17/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1102.66 ms /    10 runs   (  110.27 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1105.00 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  90%|▉| 18/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1052.76 ms /    10 runs   (  105.28 ms per token,     9.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1055.17 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  95%|▉| 19/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1067.27 ms /    10 runs   (  106.73 ms per token,     9.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1069.84 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:   0%| | 0/20 [00:00Llama.generate: 34 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     412.19 ms /    14 tokens (   29.44 ms per token,    33.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =     969.88 ms /     9 runs   (  107.76 ms per token,     9.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1384.54 ms /    23 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:   5%| | 1/20 [00:01Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1104.71 ms /    10 runs   (  110.47 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1107.00 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  10%| | 2/20 [00:02Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1117.17 ms /    10 runs   (  111.72 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1119.45 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  15%|▏| 3/20 [00:03Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1070.64 ms /    10 runs   (  107.06 ms per token,     9.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1072.89 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  20%|▏| 4/20 [00:04Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1122.25 ms /    10 runs   (  112.23 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1124.50 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  25%|▎| 5/20 [00:05Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1142.77 ms /    10 runs   (  114.28 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1145.08 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  30%|▎| 6/20 [00:06Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1132.93 ms /    10 runs   (  113.29 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1135.65 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  35%|▎| 7/20 [00:08Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1132.25 ms /    10 runs   (  113.23 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1134.55 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  40%|▍| 8/20 [00:09Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1123.27 ms /    10 runs   (  112.33 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1125.47 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  45%|▍| 9/20 [00:10Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1120.10 ms /    10 runs   (  112.01 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1122.38 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  50%|▌| 10/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1092.77 ms /    10 runs   (  109.28 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1095.05 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  55%|▌| 11/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1133.24 ms /    10 runs   (  113.32 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1135.51 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  60%|▌| 12/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1114.22 ms /    10 runs   (  111.42 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1116.73 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  65%|▋| 13/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1111.98 ms /    10 runs   (  111.20 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1114.28 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  70%|▋| 14/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1104.30 ms /    10 runs   (  110.43 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1106.52 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  75%|▊| 15/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1120.71 ms /    10 runs   (  112.07 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1122.95 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  80%|▊| 16/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1360.49 ms /    10 runs   (  136.05 ms per token,     7.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1362.79 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  85%|▊| 17/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1069.54 ms /    10 runs   (  106.95 ms per token,     9.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1071.80 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  90%|▉| 18/20 [00:2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1061.16 ms /    10 runs   (  106.12 ms per token,     9.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1063.38 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  95%|▉| 19/20 [00:2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1105.53 ms /    10 runs   (  110.55 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1108.01 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:   0%| | 0/20 [00Llama.generate: 35 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     454.44 ms /    13 tokens (   34.96 ms per token,    28.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1047.61 ms /     9 runs   (  116.40 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1504.55 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:   5%| | 1/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1161.46 ms /    10 runs   (  116.15 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1163.86 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  10%| | 2/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1242.61 ms /    10 runs   (  124.26 ms per token,     8.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1245.00 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  15%|▏| 3/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1248.82 ms /    10 runs   (  124.88 ms per token,     8.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1251.42 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  20%|▏| 4/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1254.59 ms /    10 runs   (  125.46 ms per token,     7.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1257.06 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  25%|▎| 5/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1252.50 ms /    10 runs   (  125.25 ms per token,     7.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1254.94 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  30%|▎| 6/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1249.24 ms /    10 runs   (  124.92 ms per token,     8.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1251.65 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  35%|▎| 7/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1238.68 ms /    10 runs   (  123.87 ms per token,     8.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1242.01 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  40%|▍| 8/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1246.95 ms /    10 runs   (  124.70 ms per token,     8.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1249.19 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  45%|▍| 9/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1494.12 ms /    12 runs   (  124.51 ms per token,     8.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1496.74 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  50%|▌| 10/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1155.27 ms /    10 runs   (  115.53 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1157.76 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  55%|▌| 11/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2461.88 ms /    20 runs   (  123.09 ms per token,     8.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    2466.31 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =         19\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  60%|▌| 12/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1165.74 ms /    10 runs   (  116.57 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1167.95 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  65%|▋| 13/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1198.42 ms /    10 runs   (  119.84 ms per token,     8.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1200.66 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  70%|▋| 14/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1186.47 ms /    10 runs   (  118.65 ms per token,     8.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1188.75 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  75%|▊| 15/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1160.48 ms /    10 runs   (  116.05 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1162.68 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  80%|▊| 16/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1150.15 ms /    10 runs   (  115.02 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1152.35 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  85%|▊| 17/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1147.47 ms /    10 runs   (  114.75 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1149.69 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  90%|▉| 18/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1356.56 ms /    12 runs   (  113.05 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1359.37 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  95%|▉| 19/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1049.95 ms /    10 runs   (  105.00 ms per token,     9.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1052.52 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:   0%| | 0/20 [00:0Llama.generate: 36 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     546.34 ms /    12 tokens (   45.53 ms per token,    21.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =     937.37 ms /     9 runs   (  104.15 ms per token,     9.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1485.98 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:   5%| | 1/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1598.33 ms /    12 runs   (  133.19 ms per token,     7.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1601.14 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  10%| | 2/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1199.55 ms /    10 runs   (  119.96 ms per token,     8.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1202.09 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  15%|▏| 3/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1165.50 ms /    10 runs   (  116.55 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1167.93 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  20%|▏| 4/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1121.30 ms /    10 runs   (  112.13 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1123.90 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  25%|▎| 5/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1107.39 ms /    10 runs   (  110.74 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1110.12 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  30%|▎| 6/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1110.35 ms /    10 runs   (  111.03 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1112.69 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  35%|▎| 7/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1324.96 ms /    12 runs   (  110.41 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1327.81 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  40%|▍| 8/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1104.62 ms /    10 runs   (  110.46 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1107.04 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  45%|▍| 9/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1123.76 ms /    10 runs   (  112.38 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1126.19 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  50%|▌| 10/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1159.24 ms /    10 runs   (  115.92 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1161.69 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  55%|▌| 11/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1587.51 ms /    14 runs   (  113.39 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1590.90 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  60%|▌| 12/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1424.87 ms /    12 runs   (  118.74 ms per token,     8.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1427.81 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  65%|▋| 13/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1352.83 ms /    12 runs   (  112.74 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1356.58 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  70%|▋| 14/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1108.23 ms /    10 runs   (  110.82 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1110.71 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  75%|▊| 15/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1102.09 ms /    10 runs   (  110.21 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1104.41 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  80%|▊| 16/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1323.66 ms /    12 runs   (  110.30 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1326.43 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  85%|▊| 17/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1114.21 ms /    10 runs   (  111.42 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1117.51 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  90%|▉| 18/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1101.18 ms /    10 runs   (  110.12 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1103.45 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  95%|▉| 19/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1086.41 ms /    10 runs   (  108.64 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1088.64 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • plants sunlight\n",
      " • require sunlight [mask]\n",
      " • need sunlight\n",
      " • require sunlight\n",
      " • require sunlight nutrients\n",
      " •  \n",
      " • need sunlight water\n",
      " • need sunlight\n",
      " • require sunlight water\n",
      " • need sunshine [mask] [mask]\n",
      " • need sunlight water\n",
      " • need sunlight soil\n",
      " • need sunlight [mask] [mask]\n",
      " • require sunlight\n",
      " • require sunlight [mask]\n",
      " • need sunlight\n",
      " • require sunlight soil\n",
      " • require sunlight\n",
      " • require sunlight\n",
      " • require light [mask]\n",
      " • need sunlight soil\n",
      " • adequate sunlight and\n",
      " •  \n",
      " •  \n",
      " • sunlight and\n",
      " • nutrientrich \n",
      " • sunlight and\n",
      " • sunlight and\n",
      " • nutrientrich appropriate temperature\n",
      " •  \n",
      " • sunlight and\n",
      " • sunlight and\n",
      " • nutrientrich sunlight\n",
      " •  \n",
      " • specific temperature specific\n",
      " • sunlight and\n",
      " • sufficient sunlight and\n",
      " • sunlight and\n",
      " • sunlight and\n",
      " • sunlight and\n",
      " • sunlight and\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • nutrients to\n",
      " • sunlight to\n",
      " • sunlight without these essential elements they cannot\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • nutrients to\n",
      " • sunlight to\n",
      " • sunlight grow\n",
      " • nutrients grow\n",
      " • sunlight grow\n",
      " • sunlight grow\n",
      " • sunlight grow\n",
      " • sunlight grow\n",
      " • sunlight grow\n",
      " • nutrients grow\n",
      " • sunlight grow\n",
      " • sunlight grow\n",
      " • sunlight grow\n",
      " • sunlight photosynthesize\n",
      " • nutrients grow\n",
      " • nutrients grow\n",
      " • sunlight grow\n",
      " • sunlight grow\n",
      " • nutrients grow\n",
      " • sunlight grow\n",
      " • sunlight grow\n",
      " • sunlight grow\n",
      "\n",
      "Total generated: 120\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "plants          PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "require         PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "and             PMI=  1.000   P(x)=0.500  P(y)=0.500  P(xy)=0.500\n",
      "water           PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "to              PMI= -0.008   P(x)=0.900  P(y)=0.950  P(xy)=0.850\n",
      "grow            PMI= -0.026   P(x)=0.750  P(y)=0.950  P(xy)=0.700\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 3   |   Anchor word: 'and'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:   0%| | 0/20 [0Llama.generate: 29 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     706.79 ms /    19 tokens (   37.20 ms per token,    26.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1208.37 ms /     9 runs   (  134.26 ms per token,     7.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1917.60 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:   5%| | 1/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1335.70 ms /    10 runs   (  133.57 ms per token,     7.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1338.31 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  10%| | 2/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1195.60 ms /     9 runs   (  132.84 ms per token,     7.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1197.90 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  15%|▏| 3/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1288.96 ms /    10 runs   (  128.90 ms per token,     7.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1291.60 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  20%|▏| 4/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1285.65 ms /    10 runs   (  128.57 ms per token,     7.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1288.09 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  25%|▎| 5/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1304.34 ms /    10 runs   (  130.43 ms per token,     7.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1306.80 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  30%|▎| 6/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1283.77 ms /    10 runs   (  128.38 ms per token,     7.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1286.23 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  35%|▎| 7/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1249.51 ms /    10 runs   (  124.95 ms per token,     8.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1252.05 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  40%|▍| 8/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1239.02 ms /    10 runs   (  123.90 ms per token,     8.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1242.72 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  45%|▍| 9/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1213.01 ms /    10 runs   (  121.30 ms per token,     8.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1215.34 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  50%|▌| 10/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1209.78 ms /    10 runs   (  120.98 ms per token,     8.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1212.51 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  55%|▌| 11/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1213.59 ms /    10 runs   (  121.36 ms per token,     8.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1215.98 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  60%|▌| 12/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1226.90 ms /    10 runs   (  122.69 ms per token,     8.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1229.13 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  65%|▋| 13/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1221.87 ms /    10 runs   (  122.19 ms per token,     8.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1224.70 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  70%|▋| 14/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1189.98 ms /    10 runs   (  119.00 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1192.17 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  75%|▊| 15/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1179.34 ms /    10 runs   (  117.93 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1181.56 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  80%|▊| 16/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1162.41 ms /    10 runs   (  116.24 ms per token,     8.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1165.30 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  85%|▊| 17/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1135.74 ms /    10 runs   (  113.57 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1137.95 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  90%|▉| 18/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1146.79 ms /    10 runs   (  114.68 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1148.98 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight [MASK] water to grow:  95%|▉| 19/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1143.32 ms /    10 runs   (  114.33 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1145.70 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:   0%| | 0/20 [00Llama.generate: 29 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     495.16 ms /    19 tokens (   26.06 ms per token,    38.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =     976.73 ms /     9 runs   (  108.53 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1474.13 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:   5%| | 1/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     819.68 ms /     7 runs   (  117.10 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =     821.40 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  10%| | 2/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1155.03 ms /    10 runs   (  115.50 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1157.55 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  15%|▏| 3/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     936.97 ms /     8 runs   (  117.12 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =     939.35 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  20%|▏| 4/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     805.63 ms /     7 runs   (  115.09 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =     807.28 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  25%|▎| 5/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1151.87 ms /    10 runs   (  115.19 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1154.15 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  30%|▎| 6/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1293.73 ms /    11 runs   (  117.61 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1296.17 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  35%|▎| 7/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1154.67 ms /    10 runs   (  115.47 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1156.86 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  40%|▍| 8/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     806.05 ms /     7 runs   (  115.15 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =     807.75 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  45%|▍| 9/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1271.08 ms /    11 runs   (  115.55 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1273.51 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  50%|▌| 10/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1158.70 ms /    10 runs   (  115.87 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1161.19 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  55%|▌| 11/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     809.99 ms /     7 runs   (  115.71 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =     811.62 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  60%|▌| 12/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1083.10 ms /    10 runs   (  108.31 ms per token,     9.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1085.31 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  65%|▋| 13/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     810.46 ms /     7 runs   (  115.78 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =     812.08 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  70%|▋| 14/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     927.45 ms /     8 runs   (  115.93 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =     929.56 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  75%|▊| 15/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1159.18 ms /    10 runs   (  115.92 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1161.43 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  80%|▊| 16/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     896.75 ms /     8 runs   (  112.09 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =     899.15 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  85%|▊| 17/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     936.66 ms /     8 runs   (  117.08 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =     938.53 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  90%|▉| 18/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     936.89 ms /     8 runs   (  117.11 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =     938.76 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight [MASK] water to grow:  95%|▉| 19/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     810.95 ms /     7 runs   (  115.85 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =     813.65 ms /     8 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:   0%| | 0/20 [00:Llama.generate: 30 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     576.49 ms /    18 tokens (   32.03 ms per token,    31.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1014.87 ms /     9 runs   (  112.76 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1594.47 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:   5%| | 1/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['plants', 'require', '[mask]', '[mask]', 'water', 'to', 'grow'] ['plants', 'require', 'nutrients', 'to', 'grow']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2000.54 ms /    18 runs   (  111.14 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    2004.76 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  10%| | 2/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['plants', 'require', '[mask]', '[mask]', 'water', 'to', 'grow'] ['plants', 'require', 'sufficient', 'nutrients', 'and', 'the', 'right', 'balance', 'of', 'minerals', 'to', 'grow']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1132.63 ms /    10 runs   (  113.26 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1134.87 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  15%|▏| 3/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1118.36 ms /    10 runs   (  111.84 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1120.56 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  20%|▏| 4/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1781.00 ms /    16 runs   (  111.31 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1784.49 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  25%|▎| 5/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1132.15 ms /    10 runs   (  113.22 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1134.53 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  30%|▎| 6/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1229.02 ms /    11 runs   (  111.73 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1231.41 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  35%|▎| 7/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1342.85 ms /    12 runs   (  111.90 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1345.43 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  40%|▍| 8/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1228.03 ms /    11 runs   (  111.64 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1230.41 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  45%|▍| 9/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1838.38 ms /    16 runs   (  114.90 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1841.89 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  50%|▌| 10/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['plants', 'require', '[mask]', '[mask]', 'water', 'to', 'grow'] ['plants', 'require', 'nutrientrich', '[water]', 'to', 'grow']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1125.21 ms /    10 runs   (  112.52 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1127.41 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  55%|▌| 11/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1682.70 ms /    15 runs   (  112.18 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1686.09 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  60%|▌| 12/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['plants', 'require', '[mask]', '[mask]', 'water', 'to', 'grow'] ['plants', 'require', 'essential', 'nutrients', 'and', 'a', 'specific', 'temperature', 'to', 'grow']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1243.80 ms /    11 runs   (  113.07 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1246.31 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  65%|▋| 13/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1127.55 ms /    10 runs   (  112.75 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1129.72 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  70%|▋| 14/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1310.23 ms /    12 runs   (  109.19 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1312.99 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  75%|▊| 15/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1481.05 ms /    13 runs   (  113.93 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1483.87 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  80%|▊| 16/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1332.68 ms /    12 runs   (  111.06 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1335.26 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  85%|▊| 17/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['plants', 'require', '[mask]', '[mask]', 'water', 'to', 'grow'] ['plants', 'require', 'nutrients', 'and', 'sunlight', 'to', 'grow']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1358.55 ms /    12 runs   (  113.21 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1361.15 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  90%|▉| 18/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1709.50 ms /    15 runs   (  113.97 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1712.80 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require [MASK] [MASK] water to grow:  95%|▉| 19/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1612.91 ms /    14 runs   (  115.21 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1615.98 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:   0%| | 0/20 [Llama.generate: 31 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     498.47 ms /    17 tokens (   29.32 ms per token,    34.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1530.19 ms /    14 runs   (  109.30 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    2032.10 ms /    31 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:   5%| | 1/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1664.92 ms /    15 runs   (  110.99 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1668.19 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  10%| | 2/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1642.22 ms /    15 runs   (  109.48 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1645.39 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  15%|▏| 3/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1458.22 ms /    13 runs   (  112.17 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1461.20 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  20%|▏| 4/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1689.97 ms /    15 runs   (  112.66 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1693.23 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  25%|▎| 5/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1422.70 ms /    13 runs   (  109.44 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1425.48 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  30%|▎| 6/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1780.88 ms /    16 runs   (  111.31 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1784.28 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  35%|▎| 7/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1421.86 ms /    13 runs   (  109.37 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1424.77 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  40%|▍| 8/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1479.45 ms /    13 runs   (  113.80 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1482.29 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  45%|▍| 9/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1697.10 ms /    15 runs   (  113.14 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1700.30 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  50%|▌| 10/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1412.95 ms /    13 runs   (  108.69 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1415.96 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  55%|▌| 11/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1705.56 ms /    15 runs   (  113.70 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1708.80 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  60%|▌| 12/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1649.27 ms /    15 runs   (  109.95 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1652.49 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  65%|▋| 13/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1824.75 ms /    16 runs   (  114.05 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1828.22 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  70%|▋| 14/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1693.27 ms /    15 runs   (  112.88 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1696.51 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  75%|▊| 15/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1713.50 ms /    15 runs   (  114.23 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1716.70 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  80%|▊| 16/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1696.31 ms /    15 runs   (  113.09 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1699.53 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  85%|▊| 17/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1656.27 ms /    15 runs   (  110.42 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1659.49 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  90%|▉| 18/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1698.80 ms /    15 runs   (  113.25 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1702.03 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  95%|▉| 19/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1647.64 ms /    15 runs   (  109.84 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1650.86 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:   0%| | 0/2Llama.generate: 35 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     748.54 ms /    13 tokens (   57.58 ms per token,    17.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1768.68 ms /    14 runs   (  126.33 ms per token,     7.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    2520.58 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:   5%| | 1/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1777.21 ms /    15 runs   (  118.48 ms per token,     8.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1780.90 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  10%| | 2/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1769.44 ms /    15 runs   (  117.96 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1773.31 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  15%|▏| 3/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1789.97 ms /    15 runs   (  119.33 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1793.26 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  20%|▏| 4/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2008.32 ms /    17 runs   (  118.14 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    2012.16 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  25%|▎| 5/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1989.46 ms /    17 runs   (  117.03 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1993.23 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  30%|▎| 6/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1747.88 ms /    15 runs   (  116.53 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1751.42 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  35%|▎| 7/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1714.89 ms /    15 runs   (  114.33 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1718.08 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  40%|▍| 8/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2182.25 ms /    18 runs   (  121.24 ms per token,     8.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    2186.15 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  45%|▍| 9/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2008.26 ms /    15 runs   (  133.88 ms per token,     7.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    2011.53 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  50%|▌| 10/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1810.71 ms /    15 runs   (  120.71 ms per token,     8.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1813.97 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  55%|▌| 11/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2222.35 ms /    20 runs   (  111.12 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    2226.75 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =         19\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  60%|▌| 12/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1681.83 ms /    15 runs   (  112.12 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1685.34 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  65%|▋| 13/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2033.73 ms /    18 runs   (  112.98 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    2037.65 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  70%|▋| 14/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2473.15 ms /    22 runs   (  112.42 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    2478.07 ms /    23 tokens\n",
      "llama_perf_context_print:    graphs reused =         20\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  75%|▊| 15/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1642.45 ms /    15 runs   (  109.50 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1645.81 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  80%|▊| 16/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1975.73 ms /    18 runs   (  109.76 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1979.84 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  85%|▊| 17/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1875.43 ms /    17 runs   (  110.32 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1879.18 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  90%|▉| 18/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1442.83 ms /    13 runs   (  110.99 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1445.66 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  95%|▉| 19/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1708.38 ms /    15 runs   (  113.89 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1711.66 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:   0%| | 0/20 Llama.generate: 36 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     566.55 ms /    12 tokens (   47.21 ms per token,    21.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1249.04 ms /     9 runs   (  138.78 ms per token,     7.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1818.25 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:   5%| | 1/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2566.03 ms /    19 runs   (  135.05 ms per token,     7.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    2570.97 ms /    20 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  10%| | 2/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2542.47 ms /    19 runs   (  133.81 ms per token,     7.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    2547.15 ms /    20 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  15%|▏| 3/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2317.93 ms /    18 runs   (  128.77 ms per token,     7.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    2323.49 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  20%|▏| 4/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1883.08 ms /    15 runs   (  125.54 ms per token,     7.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1886.53 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  25%|▎| 5/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2205.62 ms /    18 runs   (  122.53 ms per token,     8.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    2209.61 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  30%|▎| 6/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2336.72 ms /    20 runs   (  116.84 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    2341.15 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  35%|▎| 7/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1611.38 ms /    14 runs   (  115.10 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1614.47 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  40%|▍| 8/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1889.63 ms /    17 runs   (  111.15 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1893.45 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  45%|▍| 9/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2001.80 ms /    18 runs   (  111.21 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    2006.07 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  50%|▌| 10/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1110.17 ms /    10 runs   (  111.02 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1112.43 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  55%|▌| 11/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2209.90 ms /    19 runs   (  116.31 ms per token,     8.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    2214.12 ms /    20 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  60%|▌| 12/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1892.65 ms /    17 runs   (  111.33 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1896.38 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  65%|▋| 13/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2217.34 ms /    20 runs   (  110.87 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    2221.84 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =         19\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  70%|▋| 14/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2017.41 ms /    18 runs   (  112.08 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    2021.64 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  75%|▊| 15/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2124.08 ms /    19 runs   (  111.79 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    2128.30 ms /    20 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  80%|▊| 16/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2270.58 ms /    20 runs   (  113.53 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    2275.25 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  85%|▊| 17/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1969.88 ms /    18 runs   (  109.44 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1973.96 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  90%|▉| 18/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2222.74 ms /    20 runs   (  111.14 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    2227.37 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  95%|▉| 19/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2340.86 ms /    18 runs   (  130.05 ms per token,     7.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    2345.15 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • plants and\n",
      " • absorb and\n",
      " • require \n",
      " • need and\n",
      " • require and\n",
      " • require \n",
      " • need and\n",
      " • absorb and utilize\n",
      " • need and\n",
      " • require \n",
      " • absorb and require\n",
      " • absorb and\n",
      " • need \n",
      " • require and\n",
      " • require \n",
      " • absorb and\n",
      " • absorb and\n",
      " • require and\n",
      " • absorb and\n",
      " • require and\n",
      " • need \n",
      " •  \n",
      " •  \n",
      " • sunlight and\n",
      " • sunlight and\n",
      " • specific amounts of nutrients in their\n",
      " • sunlight and\n",
      " • sunlight [mask]\n",
      " • nutrientrich \n",
      " • clean warm\n",
      " •  \n",
      " • sunlight and\n",
      " •  \n",
      " • oxygenrich \n",
      " • sunlight and\n",
      " • nutrientrich \n",
      " • sufficient nutrients and\n",
      " •  \n",
      " • sunlight [mask]\n",
      " • sunlight nutrients and\n",
      " • clean nutrientrich\n",
      " • water and nutrients\n",
      " • water and nutrients\n",
      " • water and nutrients\n",
      " • for photosynthesis\n",
      " • water and nutrients\n",
      " • for photosynthesis\n",
      " • water and soil nutrients\n",
      " • for photosynthesis\n",
      " • for photosynthesis\n",
      " • water and nutrients\n",
      " • for photosynthesis\n",
      " • water and nutrients\n",
      " • water and nutrients\n",
      " • water and soil nutrients\n",
      " • water and nutrients\n",
      " • water and nutrients\n",
      " • water and nutrients\n",
      " • water and nutrients\n",
      " • water and nutrients\n",
      " • water and nutrients\n",
      " •  and nutrients to\n",
      " •  and nutrients to\n",
      " •  and nutrients to\n",
      " • adequate and optimal temperatures to\n",
      " • sufficient and proper nutrients to\n",
      " • adequate and proper nutrients to\n",
      " • enough and optimal conditions to\n",
      " •  and nutrients to\n",
      " • ample and proper nutrients to\n",
      " •  and nutrients to\n",
      " •  and nutrients to\n",
      " • enough and nutrientrich soil in order to\n",
      " •  and nutrients to\n",
      " • adequate and nutrientrich soil to\n",
      " • adequate and proper nutrients in order to\n",
      " •  and nutrients to\n",
      " • enough and welldraining soil to\n",
      " • sufficient and proper nutrients to\n",
      " •  and soil to\n",
      " •  and nutrients to\n",
      " • and grow\n",
      " • nutrientrich soil and adequate thrive\n",
      " • adequate nutrients to thrive and grow\n",
      " •  nutrients to grow and thrive\n",
      " •  nutrients to grow\n",
      " •  nutrients to grow and thrive\n",
      " • adequate proper nutrients to grow and thrive\n",
      " • and photosynthesize\n",
      " • sufficient nutrients to thrive\n",
      " •  nutrients to grow and thrive\n",
      " • and grow\n",
      " • adequate nutrients to grow and thrive\n",
      " • adequate nutrients to thrive\n",
      " • adequate proper nutrients to thrive and grow\n",
      " • adequate proper nutrients to thrive\n",
      " • enough nutrients to grow and thrive\n",
      " • ample nutrients to grow and thrive\n",
      " •  nutrients to grow and thrive\n",
      " • adequate moisture and appropriate amounts of thrive and grow\n",
      " •  nutrients to grow and thrive\n",
      "\n",
      "Total generated: 120\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "plants          PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "require         PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "sunlight        PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "water           PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "to              PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "grow            PMI=  2.737   P(x)=0.150  P(y)=0.100  P(xy)=0.100\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 4   |   Anchor word: 'water'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:   0%| | 0/20 [00:Llama.generate: 29 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     700.03 ms /    19 tokens (   36.84 ms per token,    27.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1121.36 ms /     9 runs   (  124.60 ms per token,     8.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1823.85 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:   5%| | 1/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1234.44 ms /    10 runs   (  123.44 ms per token,     8.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1236.76 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  10%| | 2/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1183.04 ms /    10 runs   (  118.30 ms per token,     8.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1185.49 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  15%|▏| 3/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1242.45 ms /    10 runs   (  124.25 ms per token,     8.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1244.71 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  20%|▏| 4/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1213.05 ms /    10 runs   (  121.31 ms per token,     8.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1215.30 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  25%|▎| 5/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1188.66 ms /    10 runs   (  118.87 ms per token,     8.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1190.91 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  30%|▎| 6/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1139.19 ms /    10 runs   (  113.92 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1141.61 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  35%|▎| 7/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1140.04 ms /    10 runs   (  114.00 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1142.32 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  40%|▍| 8/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1133.45 ms /    10 runs   (  113.35 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1135.70 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  45%|▍| 9/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1131.25 ms /    10 runs   (  113.12 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1133.46 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  50%|▌| 10/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1105.79 ms /    10 runs   (  110.58 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1108.01 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  55%|▌| 11/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1088.61 ms /    10 runs   (  108.86 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1090.78 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  60%|▌| 12/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1086.90 ms /    10 runs   (  108.69 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1089.09 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  65%|▋| 13/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1127.16 ms /    10 runs   (  112.72 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1129.35 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  70%|▋| 14/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1107.88 ms /    10 runs   (  110.79 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1110.08 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  75%|▊| 15/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1058.80 ms /    10 runs   (  105.88 ms per token,     9.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1061.03 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  80%|▊| 16/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1126.92 ms /    10 runs   (  112.69 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1129.21 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  85%|▊| 17/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1120.88 ms /    10 runs   (  112.09 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1123.06 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  90%|▉| 18/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1096.20 ms /    10 runs   (  109.62 ms per token,     9.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1098.71 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and [MASK] to grow:  95%|▉| 19/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1049.58 ms /    10 runs   (  104.96 ms per token,     9.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1051.77 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:   0%| | 0/20 [00:0Llama.generate: 29 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     516.38 ms /    19 tokens (   27.18 ms per token,    36.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1102.75 ms /     9 runs   (  122.53 ms per token,     8.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1621.67 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:   5%| | 1/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1203.13 ms /    10 runs   (  120.31 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1205.79 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  10%| | 2/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     888.80 ms /     8 runs   (  111.10 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =     890.76 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  15%|▏| 3/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     961.74 ms /     8 runs   (  120.22 ms per token,     8.32 tokens per second)\n",
      "llama_perf_context_print:       total time =     963.64 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  20%|▏| 4/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1203.21 ms /    10 runs   (  120.32 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1205.55 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  25%|▎| 5/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1197.64 ms /    10 runs   (  119.76 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1200.02 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  30%|▎| 6/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     962.96 ms /     8 runs   (  120.37 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =     964.89 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  35%|▎| 7/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1210.87 ms /    10 runs   (  121.09 ms per token,     8.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1213.42 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  40%|▍| 8/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1214.08 ms /    10 runs   (  121.41 ms per token,     8.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1217.66 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  45%|▍| 9/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1142.48 ms /    10 runs   (  114.25 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1144.71 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  50%|▌| 10/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1203.31 ms /    10 runs   (  120.33 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1205.55 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  55%|▌| 11/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     976.94 ms /     8 runs   (  122.12 ms per token,     8.19 tokens per second)\n",
      "llama_perf_context_print:       total time =     978.80 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  60%|▌| 12/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     970.49 ms /     8 runs   (  121.31 ms per token,     8.24 tokens per second)\n",
      "llama_perf_context_print:       total time =     972.29 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  65%|▋| 13/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1104.11 ms /    10 runs   (  110.41 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1106.36 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  70%|▋| 14/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1147.26 ms /    10 runs   (  114.73 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1149.53 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  75%|▊| 15/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     881.32 ms /     8 runs   (  110.17 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =     883.12 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  80%|▊| 16/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     887.66 ms /     8 runs   (  110.96 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =     889.60 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  85%|▊| 17/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     841.88 ms /     8 runs   (  105.24 ms per token,     9.50 tokens per second)\n",
      "llama_perf_context_print:       total time =     843.70 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  90%|▉| 18/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1111.96 ms /    10 runs   (  111.20 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1114.10 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and [MASK] to grow:  95%|▉| 19/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     904.50 ms /     8 runs   (  113.06 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =     906.26 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:   0%| | 0/20 [00:00Llama.generate: 30 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     651.07 ms /    18 tokens (   36.17 ms per token,    27.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =     965.73 ms /     9 runs   (  107.30 ms per token,     9.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1619.02 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:   5%| | 1/20 [00:01Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1121.48 ms /    10 runs   (  112.15 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1123.82 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  10%| | 2/20 [00:02Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1108.07 ms /    10 runs   (  110.81 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1110.36 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  15%|▏| 3/20 [00:03Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1120.06 ms /    10 runs   (  112.01 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1122.52 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  20%|▏| 4/20 [00:04Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1059.68 ms /    10 runs   (  105.97 ms per token,     9.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1061.85 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  25%|▎| 5/20 [00:06Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1074.19 ms /    10 runs   (  107.42 ms per token,     9.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1076.37 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  30%|▎| 6/20 [00:07Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1092.61 ms /    10 runs   (  109.26 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1094.72 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  35%|▎| 7/20 [00:08Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1075.09 ms /    10 runs   (  107.51 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1077.34 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  40%|▍| 8/20 [00:09Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1073.37 ms /    10 runs   (  107.34 ms per token,     9.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1075.54 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  45%|▍| 9/20 [00:10Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1098.76 ms /    10 runs   (  109.88 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1101.00 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  50%|▌| 10/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1044.22 ms /    10 runs   (  104.42 ms per token,     9.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1046.43 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  55%|▌| 11/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1271.19 ms /    10 runs   (  127.12 ms per token,     7.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1273.38 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  60%|▌| 12/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1132.50 ms /    10 runs   (  113.25 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1135.04 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  65%|▋| 13/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1131.43 ms /    10 runs   (  113.14 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1133.64 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  70%|▋| 14/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1124.33 ms /    10 runs   (  112.43 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1126.56 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  75%|▊| 15/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1113.07 ms /    10 runs   (  111.31 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1115.27 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  80%|▊| 16/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1086.15 ms /    10 runs   (  108.62 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1088.45 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  85%|▊| 17/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1115.51 ms /    10 runs   (  111.55 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1117.71 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  90%|▉| 18/20 [00:2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1096.91 ms /    10 runs   (  109.69 ms per token,     9.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1099.07 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and [MASK] to grow:  95%|▉| 19/20 [00:2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1117.76 ms /    10 runs   (  111.78 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1120.61 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:   0%| | 0/20 [Llama.generate: 31 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     589.64 ms /    17 tokens (   34.68 ms per token,    28.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1708.97 ms /    15 runs   (  113.93 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    2302.17 ms /    32 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:   5%| | 1/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1726.78 ms /    15 runs   (  115.12 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1730.08 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  10%| | 2/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1774.88 ms /    16 runs   (  110.93 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1778.31 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  15%|▏| 3/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1763.60 ms /    16 runs   (  110.23 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1767.57 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  20%|▏| 4/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1662.49 ms /    15 runs   (  110.83 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1665.75 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  25%|▎| 5/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1792.45 ms /    16 runs   (  112.03 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1796.19 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  30%|▎| 6/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1314.64 ms /    13 runs   (  101.13 ms per token,     9.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1317.45 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  35%|▎| 7/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1647.47 ms /    16 runs   (  102.97 ms per token,     9.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1651.04 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  40%|▍| 8/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1547.92 ms /    15 runs   (  103.19 ms per token,     9.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1551.10 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  45%|▍| 9/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1535.56 ms /    15 runs   (  102.37 ms per token,     9.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1538.75 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  50%|▌| 10/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1523.74 ms /    15 runs   (  101.58 ms per token,     9.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1526.99 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  55%|▌| 11/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1598.88 ms /    15 runs   (  106.59 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1602.10 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  60%|▌| 12/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1561.13 ms /    15 runs   (  104.08 ms per token,     9.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1564.55 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  65%|▋| 13/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1370.08 ms /    13 runs   (  105.39 ms per token,     9.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1372.99 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  70%|▋| 14/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1320.93 ms /    13 runs   (  101.61 ms per token,     9.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1323.72 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  75%|▊| 15/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1312.45 ms /    13 runs   (  100.96 ms per token,     9.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1315.21 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  80%|▊| 16/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1318.97 ms /    13 runs   (  101.46 ms per token,     9.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1321.94 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  85%|▊| 17/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1511.00 ms /    15 runs   (  100.73 ms per token,     9.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1514.19 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  90%|▉| 18/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1572.88 ms /    15 runs   (  104.86 ms per token,     9.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1576.03 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] [MASK] to grow:  95%|▉| 19/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1328.29 ms /    13 runs   (  102.18 ms per token,     9.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1331.06 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:   0%| | 0/20 Llama.generate: 32 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     761.00 ms /    16 tokens (   47.56 ms per token,    21.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =     996.45 ms /     9 runs   (  110.72 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1760.04 ms /    25 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:   5%| | 1/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['plants', 'require', 'sunlight', 'and', '[mask]', '[mask]', 'grow'] ['plants', 'require', 'sunlight', 'and', 'water', 'for', 'growth']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1138.09 ms /    10 runs   (  113.81 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1140.32 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  10%| | 2/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1132.96 ms /    10 runs   (  113.30 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1135.15 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  15%|▏| 3/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1138.13 ms /    10 runs   (  113.81 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1140.29 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  20%|▏| 4/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1136.48 ms /    10 runs   (  113.65 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1138.61 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  25%|▎| 5/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1136.31 ms /    10 runs   (  113.63 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1138.48 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  30%|▎| 6/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1123.23 ms /    10 runs   (  112.32 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1125.66 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  35%|▎| 7/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1142.44 ms /    10 runs   (  114.24 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1144.67 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  40%|▍| 8/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2373.34 ms /    21 runs   (  113.02 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    2377.80 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =         20\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  45%|▍| 9/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1145.38 ms /    10 runs   (  114.54 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1147.55 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  50%|▌| 10/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1895.59 ms /    17 runs   (  111.51 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1899.24 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  55%|▌| 11/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1119.46 ms /    10 runs   (  111.95 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1121.66 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  60%|▌| 12/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1209.15 ms /    11 runs   (  109.92 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1212.03 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  65%|▋| 13/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['plants', 'require', 'sunlight', 'and', '[mask]', '[mask]', 'grow'] ['plants', 'require', 'sunlight', 'and', 'water', 'for', 'optimal', 'growth']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1368.16 ms /    10 runs   (  136.82 ms per token,     7.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1370.78 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  70%|▋| 14/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1109.09 ms /    10 runs   (  110.91 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1111.27 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  75%|▊| 15/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1203.52 ms /    10 runs   (  120.35 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1205.68 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  80%|▊| 16/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1194.12 ms /    10 runs   (  119.41 ms per token,     8.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1196.36 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  85%|▊| 17/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1696.55 ms /    15 runs   (  113.10 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1700.26 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  90%|▉| 18/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['plants', 'require', 'sunlight', 'and', '[mask]', '[mask]', 'grow'] ['plants', 'require', 'sunlight', 'and', 'water', '[mask]', '[mask]', 'flourish']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     929.49 ms /     8 runs   (  116.19 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =     931.30 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  95%|▉| 19/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['plants', 'require', 'sunlight', 'and', '[mask]', '[mask]', 'grow'] ['plants', 'require', 'sunlight', 'and', 'water']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1286.59 ms /    11 runs   (  116.96 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1288.91 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['plants', 'require', 'sunlight', 'and', '[mask]', '[mask]', 'grow'] ['plants', 'require', 'sunlight', 'and', 'water', 'for', 'optimal', 'growth']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:   0%| | 0/20 [0Llama.generate: 36 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     389.42 ms /    12 tokens (   32.45 ms per token,    30.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =     986.52 ms /     9 runs   (  109.61 ms per token,     9.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1378.34 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:   5%| | 1/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1104.77 ms /    10 runs   (  110.48 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1106.96 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  10%| | 2/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1086.85 ms /    10 runs   (  108.68 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1089.04 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  15%|▏| 3/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1067.87 ms /    10 runs   (  106.79 ms per token,     9.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1070.05 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  20%|▏| 4/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1090.87 ms /    10 runs   (  109.09 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1093.07 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  25%|▎| 5/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1110.97 ms /    10 runs   (  111.10 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1113.15 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  30%|▎| 6/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1085.01 ms /    10 runs   (  108.50 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1087.19 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  35%|▎| 7/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1046.57 ms /    10 runs   (  104.66 ms per token,     9.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1048.72 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  40%|▍| 8/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1052.43 ms /    10 runs   (  105.24 ms per token,     9.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1054.70 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  45%|▍| 9/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1091.87 ms /    10 runs   (  109.19 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1094.30 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  50%|▌| 10/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1713.75 ms /    16 runs   (  107.11 ms per token,     9.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1717.25 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  55%|▌| 11/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1040.17 ms /    10 runs   (  104.02 ms per token,     9.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1042.33 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  60%|▌| 12/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1469.99 ms /    14 runs   (  105.00 ms per token,     9.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1472.93 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  65%|▋| 13/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     849.62 ms /     8 runs   (  106.20 ms per token,     9.42 tokens per second)\n",
      "llama_perf_context_print:       total time =     851.36 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  70%|▋| 14/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1327.71 ms /    13 runs   (  102.13 ms per token,     9.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1330.44 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  75%|▊| 15/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1042.52 ms /    10 runs   (  104.25 ms per token,     9.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1044.82 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  80%|▊| 16/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1054.51 ms /    10 runs   (  105.45 ms per token,     9.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1056.71 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  85%|▊| 17/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1050.56 ms /    10 runs   (  105.06 ms per token,     9.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1052.72 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  90%|▉| 18/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1054.20 ms /    10 runs   (  105.42 ms per token,     9.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1056.34 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  95%|▉| 19/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1058.55 ms /    10 runs   (  105.85 ms per token,     9.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1060.91 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • tomatoes water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • plants water\n",
      " • require water\n",
      " • absorb water\n",
      " • require water\n",
      " • require water\n",
      " • require water\n",
      " • absorb water\n",
      " • require water\n",
      " • absorb water\n",
      " • absorb water\n",
      " • require water\n",
      " • need water\n",
      " • need water\n",
      " • require water\n",
      " • absorb water\n",
      " • need water\n",
      " • need water\n",
      " • require water\n",
      " • require water\n",
      " • require water\n",
      " • need water\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • sunlight water\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water sunlight\n",
      " • water and soil nutrients\n",
      " • water and nutrients\n",
      " • in order\n",
      " • water and soil nutrients\n",
      " • water and nutrients\n",
      " • water and soil nutrients\n",
      " • for photosynthesis\n",
      " • water and soil nutrients\n",
      " • water and nutrients\n",
      " • water and nutrients\n",
      " • water and nutrients\n",
      " • water and nutrients\n",
      " • water and nutrients\n",
      " • for photosynthesis\n",
      " • for photosynthesis\n",
      " • for photosynthesis\n",
      " • for photosynthesis\n",
      " • water and nutrients\n",
      " • water and nutrients\n",
      " • for photosynthesis\n",
      " •  \n",
      " • water to\n",
      " • water to\n",
      " • water to\n",
      " • water to\n",
      " • water to\n",
      " • water to\n",
      " • water to\n",
      " • water as well as nutrients from the soil to\n",
      " • water to\n",
      " • water water and nutrients help them\n",
      " • water to\n",
      " •  \n",
      " • water to\n",
      " • water to\n",
      " • water to\n",
      " • water to\n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • water grow\n",
      " • water grow\n",
      " • water grow\n",
      " • water grow\n",
      " • water grow\n",
      " • water grow\n",
      " • water grow\n",
      " • water grow\n",
      " • water grow\n",
      " • water grow\n",
      " • water grow and photosynthesize\n",
      " • water grow\n",
      " • water photosynthesize\n",
      " • water grow\n",
      " • water grow and thrive\n",
      " • water grow\n",
      " • water grow\n",
      " • water grow\n",
      " • water grow\n",
      " • water grow\n",
      "\n",
      "Total generated: 120\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "plants          PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "require         PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "sunlight        PMI=  0.074   P(x)=0.950  P(y)=0.950  P(xy)=0.950\n",
      "and             PMI=   -inf   P(x)=0.650  P(y)=0.000  P(xy)=0.000\n",
      "to              PMI=  0.415   P(x)=0.750  P(y)=0.650  P(xy)=0.650\n",
      "grow            PMI=  0.000   P(x)=1.000  P(y)=0.850  P(xy)=0.850\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 5   |   Anchor word: 'to'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:   0%| | 0/20 [Llama.generate: 29 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     556.77 ms /    19 tokens (   29.30 ms per token,    34.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =     910.15 ms /     9 runs   (  101.13 ms per token,     9.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1469.25 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:   5%| | 1/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1015.84 ms /    10 runs   (  101.58 ms per token,     9.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1017.99 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  10%| | 2/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1077.16 ms /    10 runs   (  107.72 ms per token,     9.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1079.38 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  15%|▏| 3/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1072.01 ms /    10 runs   (  107.20 ms per token,     9.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1074.25 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  20%|▏| 4/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1065.95 ms /    10 runs   (  106.60 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1068.17 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  25%|▎| 5/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     966.93 ms /     9 runs   (  107.44 ms per token,     9.31 tokens per second)\n",
      "llama_perf_context_print:       total time =     968.92 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  30%|▎| 6/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1066.29 ms /    10 runs   (  106.63 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1068.43 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  35%|▎| 7/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     856.18 ms /     8 runs   (  107.02 ms per token,     9.34 tokens per second)\n",
      "llama_perf_context_print:       total time =     857.99 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  40%|▍| 8/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     967.37 ms /     9 runs   (  107.49 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =     969.36 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  45%|▍| 9/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1068.45 ms /    10 runs   (  106.85 ms per token,     9.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1070.64 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  50%|▌| 10/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1078.44 ms /    10 runs   (  107.84 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1080.62 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  55%|▌| 11/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1105.15 ms /    10 runs   (  110.51 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1107.34 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  60%|▌| 12/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1053.19 ms /    10 runs   (  105.32 ms per token,     9.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1055.39 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  65%|▋| 13/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1088.72 ms /    10 runs   (  108.87 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1091.01 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  70%|▋| 14/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1100.26 ms /    10 runs   (  110.03 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1102.49 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  75%|▊| 15/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1290.50 ms /    12 runs   (  107.54 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1293.11 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  80%|▊| 16/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1022.94 ms /    10 runs   (  102.29 ms per token,     9.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1025.12 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  85%|▊| 17/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1080.30 ms /    10 runs   (  108.03 ms per token,     9.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1082.47 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  90%|▉| 18/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1077.36 ms /    10 runs   (  107.74 ms per token,     9.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1079.73 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water [MASK] grow:  95%|▉| 19/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1064.59 ms /    10 runs   (  106.46 ms per token,     9.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1066.83 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:   0%| | 0/20 [0Llama.generate: 29 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     534.69 ms /    19 tokens (   28.14 ms per token,    35.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =     722.56 ms /     7 runs   (  103.22 ms per token,     9.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1259.07 ms /    26 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:   5%| | 1/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     854.84 ms /     8 runs   (  106.85 ms per token,     9.36 tokens per second)\n",
      "llama_perf_context_print:       total time =     856.54 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  10%| | 2/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     882.77 ms /     8 runs   (  110.35 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =     884.51 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  15%|▏| 3/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1076.94 ms /    10 runs   (  107.69 ms per token,     9.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1079.04 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  20%|▏| 4/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     885.07 ms /     8 runs   (  110.63 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =     886.78 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  25%|▎| 5/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     883.25 ms /     8 runs   (  110.41 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =     885.14 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  30%|▎| 6/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1104.86 ms /    10 runs   (  110.49 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1107.24 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  35%|▎| 7/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     871.78 ms /     8 runs   (  108.97 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =     873.66 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  40%|▍| 8/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1053.56 ms /     8 runs   (  131.70 ms per token,     7.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1055.27 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  45%|▍| 9/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1322.51 ms /    12 runs   (  110.21 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1325.04 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  50%|▌| 10/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     871.64 ms /     8 runs   (  108.96 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =     873.34 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  55%|▌| 11/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1066.84 ms /    10 runs   (  106.68 ms per token,     9.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1068.93 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  60%|▌| 12/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     875.24 ms /     8 runs   (  109.41 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =     876.97 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  65%|▋| 13/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     828.41 ms /     8 runs   (  103.55 ms per token,     9.66 tokens per second)\n",
      "llama_perf_context_print:       total time =     830.12 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  70%|▋| 14/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1083.50 ms /    10 runs   (  108.35 ms per token,     9.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1085.64 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  75%|▊| 15/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1068.63 ms /    10 runs   (  106.86 ms per token,     9.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1070.76 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  80%|▊| 16/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1306.43 ms /    12 runs   (  108.87 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1308.99 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  85%|▊| 17/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     876.02 ms /     8 runs   (  109.50 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =     877.78 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  90%|▉| 18/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     871.51 ms /     8 runs   (  108.94 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =     873.26 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water [MASK] grow:  95%|▉| 19/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1059.76 ms /    10 runs   (  105.98 ms per token,     9.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1061.90 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:   0%| | 0/20 [00Llama.generate: 30 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     629.92 ms /    18 tokens (   35.00 ms per token,    28.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1008.04 ms /     9 runs   (  112.00 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1640.33 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:   5%| | 1/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1027.60 ms /    10 runs   (  102.76 ms per token,     9.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1029.87 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  10%| | 2/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1036.10 ms /    10 runs   (  103.61 ms per token,     9.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1038.25 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  15%|▏| 3/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1037.76 ms /    10 runs   (  103.78 ms per token,     9.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1039.88 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  20%|▏| 4/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1050.59 ms /    10 runs   (  105.06 ms per token,     9.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1053.03 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  25%|▎| 5/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1048.61 ms /    10 runs   (  104.86 ms per token,     9.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1050.73 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  30%|▎| 6/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1024.52 ms /    10 runs   (  102.45 ms per token,     9.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1026.85 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  35%|▎| 7/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1027.11 ms /    10 runs   (  102.71 ms per token,     9.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1029.77 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  40%|▍| 8/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1033.37 ms /    10 runs   (  103.34 ms per token,     9.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1035.50 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  45%|▍| 9/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1055.55 ms /    10 runs   (  105.56 ms per token,     9.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1057.92 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  50%|▌| 10/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1025.21 ms /    10 runs   (  102.52 ms per token,     9.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1027.34 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  55%|▌| 11/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1013.91 ms /    10 runs   (  101.39 ms per token,     9.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1016.02 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  60%|▌| 12/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1012.54 ms /    10 runs   (  101.25 ms per token,     9.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1014.88 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  65%|▋| 13/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1006.60 ms /    10 runs   (  100.66 ms per token,     9.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1008.71 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  70%|▋| 14/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1020.62 ms /    10 runs   (  102.06 ms per token,     9.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1022.90 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  75%|▊| 15/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1040.67 ms /    10 runs   (  104.07 ms per token,     9.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1043.23 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  80%|▊| 16/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1005.33 ms /    10 runs   (  100.53 ms per token,     9.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1007.51 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  85%|▊| 17/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1007.19 ms /    10 runs   (  100.72 ms per token,     9.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1009.32 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  90%|▉| 18/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1006.96 ms /    10 runs   (  100.70 ms per token,     9.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1009.13 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water [MASK] grow:  95%|▉| 19/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1246.33 ms /    12 runs   (  103.86 ms per token,     9.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1249.12 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:   0%| | 0/2Llama.generate: 31 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     590.07 ms /    17 tokens (   34.71 ms per token,    28.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1541.04 ms /    15 runs   (  102.74 ms per token,     9.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    2134.57 ms /    32 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:   5%| | 1/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1722.64 ms /    17 runs   (  101.33 ms per token,     9.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1726.20 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  10%| | 2/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1676.23 ms /    16 runs   (  104.76 ms per token,     9.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1679.59 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  15%|▏| 3/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1533.31 ms /    15 runs   (  102.22 ms per token,     9.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1536.53 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  20%|▏| 4/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1576.67 ms /    15 runs   (  105.11 ms per token,     9.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1579.85 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  25%|▎| 5/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1672.30 ms /    16 runs   (  104.52 ms per token,     9.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1675.67 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  30%|▎| 6/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1579.54 ms /    15 runs   (  105.30 ms per token,     9.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1582.69 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  35%|▎| 7/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1786.70 ms /    17 runs   (  105.10 ms per token,     9.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1790.30 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  40%|▍| 8/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1518.82 ms /    15 runs   (  101.25 ms per token,     9.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1521.99 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  45%|▍| 9/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1576.70 ms /    15 runs   (  105.11 ms per token,     9.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1579.87 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  50%|▌| 10/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1661.53 ms /    16 runs   (  103.85 ms per token,     9.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1665.06 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  55%|▌| 11/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1781.11 ms /    17 runs   (  104.77 ms per token,     9.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1784.70 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  60%|▌| 12/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1791.03 ms /    17 runs   (  105.35 ms per token,     9.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1794.66 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  65%|▋| 13/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1510.20 ms /    15 runs   (  100.68 ms per token,     9.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1513.39 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  70%|▋| 14/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1365.93 ms /    13 runs   (  105.07 ms per token,     9.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1369.27 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  75%|▊| 15/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1799.50 ms /    15 runs   (  119.97 ms per token,     8.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1802.68 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  80%|▊| 16/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1766.82 ms /    17 runs   (  103.93 ms per token,     9.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1770.46 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  85%|▊| 17/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1633.49 ms /    15 runs   (  108.90 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1636.76 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  90%|▉| 18/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1737.00 ms /    17 runs   (  102.18 ms per token,     9.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1740.97 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight [MASK] water [MASK] grow:  95%|▉| 19/Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1741.40 ms /    16 runs   (  108.84 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1744.84 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:   0%| | 0/20 Llama.generate: 32 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     699.12 ms /    16 tokens (   43.70 ms per token,    22.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =     989.94 ms /     9 runs   (  109.99 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1691.28 ms /    25 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:   5%| | 1/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1083.37 ms /    10 runs   (  108.34 ms per token,     9.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1085.54 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  10%| | 2/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1050.82 ms /    10 runs   (  105.08 ms per token,     9.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1052.98 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  15%|▏| 3/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2150.97 ms /    20 runs   (  107.55 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    2155.40 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =         19\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  20%|▏| 4/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1087.74 ms /    10 runs   (  108.77 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1090.06 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  25%|▎| 5/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1097.87 ms /    10 runs   (  109.79 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1100.35 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  30%|▎| 6/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1024.00 ms /    10 runs   (  102.40 ms per token,     9.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1026.17 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  35%|▎| 7/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1084.69 ms /    10 runs   (  108.47 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1086.83 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  40%|▍| 8/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1972.91 ms /    19 runs   (  103.84 ms per token,     9.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1977.19 ms /    20 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  45%|▍| 9/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1732.66 ms /    17 runs   (  101.92 ms per token,     9.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1736.25 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  50%|▌| 10/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2065.68 ms /    19 runs   (  108.72 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    2069.75 ms /    20 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  55%|▌| 11/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1277.98 ms /    12 runs   (  106.50 ms per token,     9.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1280.54 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  60%|▌| 12/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1497.05 ms /    14 runs   (  106.93 ms per token,     9.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1500.12 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  65%|▋| 13/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['plants', 'require', 'sunlight', 'and', '[mask]', '[mask]', 'grow'] ['plants', 'require', 'sunlight', 'and', 'water', 'they', 'thrive', 'in', 'it']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1159.48 ms /    11 runs   (  105.41 ms per token,     9.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1161.89 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  70%|▋| 14/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['plants', 'require', 'sunlight', 'and', '[mask]', '[mask]', 'grow'] ['plants', 'require', 'sunlight', 'and', 'water', 'for', 'proper', 'growth']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1034.67 ms /    10 runs   (  103.47 ms per token,     9.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1036.80 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  75%|▊| 15/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['plants', 'require', 'sunlight', 'and', '[mask]', '[mask]', 'grow'] ['plants', 'require', 'sunlight', 'and', 'water', 'for', 'growth']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1089.59 ms /    10 runs   (  108.96 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1091.73 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  80%|▊| 16/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1088.73 ms /    10 runs   (  108.87 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1091.14 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  85%|▊| 17/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1094.22 ms /    10 runs   (  109.42 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1096.78 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  90%|▉| 18/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1607.90 ms /    16 runs   (  100.49 ms per token,     9.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1611.30 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight and [MASK] [MASK] grow:  95%|▉| 19/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1092.24 ms /    10 runs   (  109.22 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1094.40 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:   0%| | 0/20Llama.generate: 33 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     433.71 ms /    15 tokens (   28.91 ms per token,    34.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1459.60 ms /    14 runs   (  104.26 ms per token,     9.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1896.57 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:   5%| | 1/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1724.90 ms /    16 runs   (  107.81 ms per token,     9.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1728.30 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  10%| | 2/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1431.24 ms /    13 runs   (  110.10 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1434.03 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  15%|▏| 3/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1351.30 ms /    13 runs   (  103.95 ms per token,     9.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1354.10 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  20%|▏| 4/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1646.53 ms /    15 runs   (  109.77 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1649.74 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  25%|▎| 5/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1824.86 ms /    17 runs   (  107.34 ms per token,     9.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1828.49 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  30%|▎| 6/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1425.61 ms /    13 runs   (  109.66 ms per token,     9.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1428.38 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  35%|▎| 7/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1845.43 ms /    17 runs   (  108.55 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1849.20 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  40%|▍| 8/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1792.33 ms /    17 runs   (  105.43 ms per token,     9.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1796.19 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  45%|▍| 9/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1425.84 ms /    13 runs   (  109.68 ms per token,     9.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1428.71 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  50%|▌| 10/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1379.46 ms /    13 runs   (  106.11 ms per token,     9.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1382.25 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  55%|▌| 11/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1436.78 ms /    13 runs   (  110.52 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1439.64 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  60%|▌| 12/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1419.32 ms /    13 runs   (  109.18 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1422.11 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  65%|▋| 13/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1788.53 ms /    17 runs   (  105.21 ms per token,     9.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1792.30 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  70%|▋| 14/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1605.36 ms /    15 runs   (  107.02 ms per token,     9.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1608.56 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  75%|▊| 15/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1645.84 ms /    13 runs   (  126.60 ms per token,     7.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1648.94 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  80%|▊| 16/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1231.58 ms /    12 runs   (  102.63 ms per token,     9.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1234.14 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  85%|▊| 17/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1317.93 ms /    13 runs   (  101.38 ms per token,     9.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1320.68 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  90%|▉| 18/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1750.31 ms /    17 runs   (  102.96 ms per token,     9.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1753.95 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  95%|▉| 19/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1829.98 ms /    18 runs   (  101.67 ms per token,     9.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1833.94 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants in order to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • plants to\n",
      " • require to\n",
      " • need to\n",
      " • need to\n",
      " • require to\n",
      " • need to\n",
      " • need to\n",
      " • need to\n",
      " • need to\n",
      " • require to\n",
      " • require in order to\n",
      " • need to\n",
      " • absorb to\n",
      " • absorb to\n",
      " • absorb to\n",
      " • need to\n",
      " • require to\n",
      " • require in order to\n",
      " • need to\n",
      " • require to\n",
      " • require to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • sunlight to\n",
      " • nutrients to\n",
      " • sufficient and the right temperature to\n",
      " • sufficient and proper nutrients to\n",
      " • adequate and nutrients to\n",
      " •  and nutrients to\n",
      " •  and nutrients to\n",
      " • enough and the right temperature to\n",
      " •  and nutrients to\n",
      " • sufficient and appropriate nutrients to\n",
      " • adequate and ideal temperature to\n",
      " • adequate and optimal temperature to\n",
      " • adequate and favorable conditions to\n",
      " • adequate and proper nutrients to\n",
      " • adequate and proper nutrients to\n",
      " •  and nutrients to\n",
      " •  and soil to\n",
      " •  and nutrients to\n",
      " • sufficient and proper nutrients to\n",
      " •  and nutrients to\n",
      " •  and fertile soil in order to\n",
      " • adequate and fertile soil to\n",
      " • water to\n",
      " • water to\n",
      " • water to\n",
      " • water without these essential elements they cannot\n",
      " • water to\n",
      " • water to\n",
      " • water to\n",
      " • water to\n",
      " • water and adequate nutrients for them to\n",
      " • water the more water the more they\n",
      " • water nutrients and the appropriate temperature to\n",
      " • water and they\n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • water to\n",
      " • water to\n",
      " • water to\n",
      " • water they\n",
      " • water to\n",
      " • for photosynthesis and absorption\n",
      " • for photosynthesis and proper growth\n",
      " • to grow and thrive\n",
      " • to grow and thrive\n",
      " • for photosynthesis and growth\n",
      " • for photosynthesis and nutrient absorption\n",
      " • to grow and thrive\n",
      " • for photosynthesis and nutrient absorption\n",
      " • for photosynthesis and nutrient absorption\n",
      " • to grow and thrive\n",
      " • to grow and thrive\n",
      " • to grow and thrive\n",
      " • to grow and thrive\n",
      " • for photosynthesis and nutrient absorption\n",
      " • for growth and photosynthesis\n",
      " • to grow and thrive\n",
      " • for growth and survival\n",
      " • to grow and thrive\n",
      " • for photosynthesis and nutrient absorption\n",
      " • in order to photosynthesize and grow\n",
      "\n",
      "Total generated: 120\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "plants          PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "require         PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "sunlight        PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "and             PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "water           PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "grow            PMI=   -inf   P(x)=0.450  P(y)=0.000  P(xy)=0.000\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 6   |   Anchor word: 'grow'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] require sunlight and water to [MASK]:   0%| | 0/20 [00Llama.generate: 29 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     632.49 ms /    19 tokens (   33.29 ms per token,    30.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =     926.75 ms /     9 runs   (  102.97 ms per token,     9.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1561.96 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:   5%| | 1/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1022.77 ms /    10 runs   (  102.28 ms per token,     9.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1024.94 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  10%| | 2/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1065.76 ms /    10 runs   (  106.58 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1068.12 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  15%|▏| 3/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1043.95 ms /    10 runs   (  104.40 ms per token,     9.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1046.09 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  20%|▏| 4/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1069.01 ms /    10 runs   (  106.90 ms per token,     9.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1071.38 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  25%|▎| 5/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1054.49 ms /    10 runs   (  105.45 ms per token,     9.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1056.73 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  30%|▎| 6/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1053.92 ms /    10 runs   (  105.39 ms per token,     9.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1056.06 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  35%|▎| 7/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1084.75 ms /    10 runs   (  108.48 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1086.86 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  40%|▍| 8/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1031.98 ms /    10 runs   (  103.20 ms per token,     9.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1034.10 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  45%|▍| 9/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1053.79 ms /    10 runs   (  105.38 ms per token,     9.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1055.89 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  50%|▌| 10/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1462.19 ms /    14 runs   (  104.44 ms per token,     9.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1465.19 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  55%|▌| 11/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1059.59 ms /    10 runs   (  105.96 ms per token,     9.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1061.73 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  60%|▌| 12/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1056.70 ms /    10 runs   (  105.67 ms per token,     9.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1058.86 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  65%|▋| 13/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1059.93 ms /    10 runs   (  105.99 ms per token,     9.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1062.04 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  70%|▋| 14/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1048.34 ms /    10 runs   (  104.83 ms per token,     9.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1050.45 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  75%|▊| 15/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1047.06 ms /    10 runs   (  104.71 ms per token,     9.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1049.20 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  80%|▊| 16/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1474.08 ms /    14 runs   (  105.29 ms per token,     9.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1477.04 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  85%|▊| 17/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1047.64 ms /    10 runs   (  104.76 ms per token,     9.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1049.76 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  90%|▉| 18/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1089.31 ms /    10 runs   (  108.93 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1091.45 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] require sunlight and water to [MASK]:  95%|▉| 19/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1485.87 ms /    14 runs   (  106.13 ms per token,     9.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1488.89 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:   0%| | 0/20 [00:Llama.generate: 29 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     557.37 ms /    19 tokens (   29.34 ms per token,    34.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1367.67 ms /    13 runs   (  105.21 ms per token,     9.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1928.47 ms /    32 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:   5%| | 1/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     848.84 ms /     8 runs   (  106.10 ms per token,     9.42 tokens per second)\n",
      "llama_perf_context_print:       total time =     850.91 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  10%| | 2/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     871.30 ms /     8 runs   (  108.91 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =     873.07 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  15%|▏| 3/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1730.49 ms /    16 runs   (  108.16 ms per token,     9.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1733.94 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  20%|▏| 4/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     835.57 ms /     8 runs   (  104.45 ms per token,     9.57 tokens per second)\n",
      "llama_perf_context_print:       total time =     837.28 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  25%|▎| 5/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1264.98 ms /    12 runs   (  105.42 ms per token,     9.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1267.52 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  30%|▎| 6/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1189.42 ms /    11 runs   (  108.13 ms per token,     9.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1191.72 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  35%|▎| 7/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     829.06 ms /     8 runs   (  103.63 ms per token,     9.65 tokens per second)\n",
      "llama_perf_context_print:       total time =     830.77 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  40%|▍| 8/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1372.42 ms /    13 runs   (  105.57 ms per token,     9.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1375.15 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  45%|▍| 9/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     865.92 ms /     8 runs   (  108.24 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =     867.69 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  50%|▌| 10/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1514.73 ms /    14 runs   (  108.20 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1517.68 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  55%|▌| 11/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     873.14 ms /     8 runs   (  109.14 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =     874.99 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  60%|▌| 12/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1225.66 ms /    12 runs   (  102.14 ms per token,     9.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1228.32 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  65%|▋| 13/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1139.49 ms /    11 runs   (  103.59 ms per token,     9.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1141.90 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  70%|▋| 14/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1347.02 ms /    13 runs   (  103.62 ms per token,     9.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1349.78 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  75%|▊| 15/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     842.91 ms /     8 runs   (  105.36 ms per token,     9.49 tokens per second)\n",
      "llama_perf_context_print:       total time =     844.67 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  80%|▊| 16/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1085.20 ms /    10 runs   (  108.52 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1087.43 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  85%|▊| 17/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     868.07 ms /     8 runs   (  108.51 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =     869.85 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  90%|▉| 18/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1412.91 ms /    13 runs   (  108.69 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1415.88 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants [MASK] sunlight and water to [MASK]:  95%|▉| 19/20 [00Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1579.56 ms /    15 runs   (  105.30 ms per token,     9.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1582.73 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:   0%| | 0/20 [00:0Llama.generate: 30 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     659.47 ms /    18 tokens (   36.64 ms per token,    27.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1157.72 ms /    11 runs   (  105.25 ms per token,     9.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1819.85 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:   5%| | 1/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1085.48 ms /    10 runs   (  108.55 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1087.65 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  10%| | 2/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1092.80 ms /    10 runs   (  109.28 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1095.06 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  15%|▏| 3/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1092.46 ms /    10 runs   (  109.25 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1094.70 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  20%|▏| 4/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1318.69 ms /    10 runs   (  131.87 ms per token,     7.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1320.88 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  25%|▎| 5/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1029.05 ms /    10 runs   (  102.90 ms per token,     9.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1031.30 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  30%|▎| 6/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1311.34 ms /    12 runs   (  109.28 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1314.03 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  35%|▎| 7/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1091.81 ms /    10 runs   (  109.18 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1094.29 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  40%|▍| 8/20 [00:0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1291.83 ms /    12 runs   (  107.65 ms per token,     9.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1294.46 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  45%|▍| 9/20 [00:1Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1108.72 ms /    10 runs   (  110.87 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1110.99 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  50%|▌| 10/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1526.83 ms /    14 runs   (  109.06 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1530.03 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  55%|▌| 11/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1274.34 ms /    12 runs   (  106.19 ms per token,     9.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1277.20 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  60%|▌| 12/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1082.20 ms /    10 runs   (  108.22 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1084.93 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  65%|▋| 13/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1082.18 ms /    10 runs   (  108.22 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1084.32 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  70%|▋| 14/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1083.31 ms /    10 runs   (  108.33 ms per token,     9.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1085.44 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  75%|▊| 15/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1222.76 ms /    12 runs   (  101.90 ms per token,     9.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1225.31 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  80%|▊| 16/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1516.35 ms /    14 runs   (  108.31 ms per token,     9.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1519.32 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  85%|▊| 17/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1083.73 ms /    10 runs   (  108.37 ms per token,     9.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1085.85 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  90%|▉| 18/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1235.24 ms /    12 runs   (  102.94 ms per token,     9.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1237.80 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require [MASK] and water to [MASK]:  95%|▉| 19/20 [00:Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1302.04 ms /    12 runs   (  108.50 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1304.62 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:   0%| | 0/20 Llama.generate: 31 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     635.84 ms /    17 tokens (   37.40 ms per token,    26.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1704.05 ms /    17 runs   (  100.24 ms per token,     9.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    2343.95 ms /    34 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:   5%| | 1/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2098.54 ms /    20 runs   (  104.93 ms per token,     9.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    2102.91 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  10%| | 2/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1890.60 ms /    18 runs   (  105.03 ms per token,     9.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1894.41 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  15%|▏| 3/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1986.87 ms /    19 runs   (  104.57 ms per token,     9.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1991.11 ms /    20 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  20%|▏| 4/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1859.99 ms /    18 runs   (  103.33 ms per token,     9.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1863.96 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  25%|▎| 5/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1988.83 ms /    19 runs   (  104.68 ms per token,     9.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1992.91 ms /    20 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  30%|▎| 6/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2161.36 ms /    20 runs   (  108.07 ms per token,     9.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    2165.80 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  35%|▎| 7/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2092.47 ms /    19 runs   (  110.13 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    2096.70 ms /    20 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  40%|▍| 8/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2235.53 ms /    21 runs   (  106.45 ms per token,     9.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    2240.14 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =         19\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  45%|▍| 9/20 Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1828.74 ms /    17 runs   (  107.57 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1833.21 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  50%|▌| 10/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2186.36 ms /    21 runs   (  104.11 ms per token,     9.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    2191.04 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =         20\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  55%|▌| 11/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1807.40 ms /    17 runs   (  106.32 ms per token,     9.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1811.21 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  60%|▌| 12/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1893.83 ms /    18 runs   (  105.21 ms per token,     9.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1897.97 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  65%|▋| 13/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1929.25 ms /    18 runs   (  107.18 ms per token,     9.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1933.22 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  70%|▋| 14/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2105.79 ms /    20 runs   (  105.29 ms per token,     9.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    2110.13 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  75%|▊| 15/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1909.09 ms /    18 runs   (  106.06 ms per token,     9.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1912.99 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  80%|▊| 16/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1486.77 ms /    14 runs   (  106.20 ms per token,     9.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1489.79 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  85%|▊| 17/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1383.59 ms /    13 runs   (  106.43 ms per token,     9.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1386.42 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  90%|▉| 18/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2240.53 ms /    21 runs   (  106.69 ms per token,     9.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    2245.13 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =         20\n",
      "Processing prompt: plants require sunlight [MASK] water to [MASK]:  95%|▉| 19/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2038.11 ms /    20 runs   (  101.91 ms per token,     9.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    2042.47 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:   0%| | 0/20 [0Llama.generate: 32 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     709.32 ms /    16 tokens (   44.33 ms per token,    22.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1067.15 ms /     9 runs   (  118.57 ms per token,     8.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1778.71 ms /    25 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:   5%| | 1/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1199.43 ms /    10 runs   (  119.94 ms per token,     8.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1202.02 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  10%| | 2/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1271.59 ms /    10 runs   (  127.16 ms per token,     7.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1274.05 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  15%|▏| 3/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1287.17 ms /    10 runs   (  128.72 ms per token,     7.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1289.56 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  20%|▏| 4/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1227.46 ms /    10 runs   (  122.75 ms per token,     8.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1229.85 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  25%|▎| 5/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1274.47 ms /    10 runs   (  127.45 ms per token,     7.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1276.86 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  30%|▎| 6/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1741.59 ms /    14 runs   (  124.40 ms per token,     8.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1744.95 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  35%|▎| 7/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     922.03 ms /     8 runs   (  115.25 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =     923.85 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  40%|▍| 8/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1698.85 ms /    14 runs   (  121.35 ms per token,     8.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1701.92 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  45%|▍| 9/20 [0Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1697.29 ms /    14 runs   (  121.23 ms per token,     8.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1700.29 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  50%|▌| 10/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1110.34 ms /    10 runs   (  111.03 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1112.48 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  55%|▌| 11/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1221.04 ms /    10 runs   (  122.10 ms per token,     8.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1223.24 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  60%|▌| 12/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1216.61 ms /    10 runs   (  121.66 ms per token,     8.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1218.78 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  65%|▋| 13/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1590.51 ms /    14 runs   (  113.61 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1594.07 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  70%|▋| 14/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1177.31 ms /    10 runs   (  117.73 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1179.72 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  75%|▊| 15/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1120.52 ms /    10 runs   (  112.05 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1122.69 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  80%|▊| 16/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1104.38 ms /    10 runs   (  110.44 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1106.71 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  85%|▊| 17/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1549.78 ms /    14 runs   (  110.70 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1552.86 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  90%|▉| 18/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1144.84 ms /    10 runs   (  114.48 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1146.98 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and [MASK] to [MASK]:  95%|▉| 19/20 [Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1119.20 ms /    10 runs   (  111.92 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1121.37 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:   0%| | 0/20Llama.generate: 33 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     781.15 ms /    15 tokens (   52.08 ms per token,    19.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1379.49 ms /    12 runs   (  114.96 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    2163.80 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:   5%| | 1/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1967.62 ms /    17 runs   (  115.74 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1971.66 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  10%| | 2/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1869.77 ms /    17 runs   (  109.99 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1874.04 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  15%|▏| 3/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1860.77 ms /    17 runs   (  109.46 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1865.27 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  20%|▏| 4/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1640.20 ms /    15 runs   (  109.35 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1643.69 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  25%|▎| 5/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1431.97 ms /    13 runs   (  110.15 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1434.98 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  30%|▎| 6/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1850.88 ms /    17 runs   (  108.88 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1855.05 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  35%|▎| 7/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1310.75 ms /    12 runs   (  109.23 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1313.55 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  40%|▍| 8/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1430.69 ms /    13 runs   (  110.05 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1433.70 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  45%|▍| 9/20Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1421.02 ms /    13 runs   (  109.31 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1424.09 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  50%|▌| 10/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1998.88 ms /    17 runs   (  117.58 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    2002.79 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  55%|▌| 11/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2023.49 ms /    18 runs   (  112.42 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    2027.66 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  60%|▌| 12/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1489.49 ms /    13 runs   (  114.58 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1492.70 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  65%|▋| 13/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1717.77 ms /    15 runs   (  114.52 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1722.33 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  70%|▋| 14/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1896.25 ms /    17 runs   (  111.54 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1900.11 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  75%|▊| 15/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1342.74 ms /    12 runs   (  111.90 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1345.45 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  80%|▊| 16/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1563.42 ms /    15 runs   (  104.23 ms per token,     9.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1566.77 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  85%|▊| 17/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1624.63 ms /    15 runs   (  108.31 ms per token,     9.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1627.94 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  90%|▉| 18/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1276.40 ms /    12 runs   (  106.37 ms per token,     9.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1279.16 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: plants require sunlight and water [MASK] [MASK]:  95%|▉| 19/2Llama.generate: 47 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1885.34 ms /    17 runs   (  110.90 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1889.13 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • plants grow\n",
      " • plants grow\n",
      " • plants grow\n",
      " • plants grow\n",
      " • plants grow\n",
      " • plants grow\n",
      " • plants grow\n",
      " • plants grow\n",
      " • plants grow\n",
      " • plants grow\n",
      " • plants photosynthesize\n",
      " • plants grow\n",
      " • plants grow\n",
      " • plants grow\n",
      " • plants grow\n",
      " • plants grow\n",
      " • plants photosynthesize\n",
      " • plants grow\n",
      " • plants grow\n",
      " • plants photosynthesize\n",
      " • absorb photosynthesize\n",
      " • need grow\n",
      " • require grow\n",
      " • absorb grow and photosynthesize\n",
      " • absorb grow\n",
      " • absorb photosynthesize\n",
      " • require grow and thrive\n",
      " • absorb grow\n",
      " • require grow and thrive\n",
      " • need grow\n",
      " • use photosynthesize\n",
      " • need grow\n",
      " • absorb photosynthesize\n",
      " • absorb grow and thrive\n",
      " • absorb grow and thrive\n",
      " • absorb grow\n",
      " • absorb grow\n",
      " • absorb grow\n",
      " • require grow and thrive\n",
      " • photosynthesize grow and thrive\n",
      " • nutrients grow\n",
      " • sunlight grow\n",
      " • sunlight grow\n",
      " • sunlight grow\n",
      " • sunlight grow\n",
      " • sunlight grow\n",
      " • nutrients grow\n",
      " • sunlight grow\n",
      " • nutrients grow\n",
      " • sunlight grow\n",
      " • sunlight photosynthesize\n",
      " • nutrients grow\n",
      " • sunlight grow\n",
      " • sunlight grow\n",
      " • sunlight grow\n",
      " • nutrients grow\n",
      " • sunlight photosynthesize\n",
      " • sunlight grow\n",
      " • nutrients grow\n",
      " • nutrients grow\n",
      " •  nutrients to grow and thrive\n",
      " • adequate proper nutrients to grow and thrive\n",
      " •  nutrients to grow and thrive\n",
      " • adequate appropriate soil conditions to grow and thrive\n",
      " •  nutrients to grow and thrive\n",
      " •  soil nutrients to grow and thrive\n",
      " • sufficient appropriate nutrients to grow and thrive\n",
      " • adequate nutrients to grow and thrive\n",
      " • ample adequate nutrients to grow and thrive\n",
      " • adequate proper soil nutrition to thrive\n",
      " • ample proper nutrients to grow and thrive\n",
      " •  [root] to grow and thrive\n",
      " •  nutrients to grow and thrive\n",
      " •  nutrients to grow and thrive\n",
      " • ample nutrients to grow and thrive\n",
      " •  nutrients to grow and thrive\n",
      " • and adequate grow and thrive\n",
      " • and grow and thrive\n",
      " • ample proper nutrients to grow and thrive\n",
      " • adequate proper nutrients to grow and thrive\n",
      " • water grow\n",
      " • water grow\n",
      " • water grow\n",
      " • water grow\n",
      " • water grow\n",
      " • water grow\n",
      " • water photosynthesize\n",
      " • water grow\n",
      " • water photosynthesize\n",
      " • water photosynthesize\n",
      " • water grow\n",
      " • water grow\n",
      " • water grow\n",
      " • water photosynthesize\n",
      " • water grow\n",
      " • water grow\n",
      " • water grow\n",
      " • water photosynthesize\n",
      " • water grow\n",
      " • water grow\n",
      " • for growth and reproduction\n",
      " • for photosynthesis and nutrient absorption\n",
      " • for photosynthesis and nutrient absorption\n",
      " • for photosynthesis and nutrient absorption\n",
      " • in order to grow and thrive\n",
      " • to grow and thrive\n",
      " • for photosynthesis and nutrient absorption\n",
      " • regularly for optimal growth\n",
      " • to grow and thrive\n",
      " • to grow and thrive\n",
      " • for photosynthesis and nutrient absorption\n",
      " • in order to photosynthesize and grow\n",
      " • to grow and thrive\n",
      " • for photosynthesis and growth\n",
      " • for photosynthesis and nutrient absorption\n",
      " • for growth and survival\n",
      " • for growth and photosynthesis\n",
      " • for photosynthesis and absorption\n",
      " • to grow and survive\n",
      " • for photosynthesis and nutrient absorption\n",
      "\n",
      "Total generated: 120\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "plants          PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "require         PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "sunlight        PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "and             PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "water           PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "to              PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "\n",
      "##############################################################################################################\n",
      "ANALYZING SENTENCE:\n",
      "   'the government announced new policies to support healthcare'\n",
      "##############################################################################################################\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 0   |   Anchor word: 'the'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 29 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     801.43 ms /    20 tokens (   40.07 ms per token,    24.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1155.11 ms /    11 runs   (  105.01 ms per token,     9.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1959.40 ms /    31 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1527.64 ms /    14 runs   (  109.12 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1530.99 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1834.67 ms /    17 runs   (  107.92 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1838.50 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1262.76 ms /    12 runs   (  105.23 ms per token,     9.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1265.36 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1409.87 ms /    13 runs   (  108.45 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1412.73 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1493.60 ms /    14 runs   (  106.69 ms per token,     9.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1496.80 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1231.78 ms /    12 runs   (  102.65 ms per token,     9.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1234.43 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1517.94 ms /    14 runs   (  108.42 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1521.03 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1279.08 ms /    12 runs   (  106.59 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1281.76 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1538.94 ms /    15 runs   (  102.60 ms per token,     9.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1542.47 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1734.59 ms /    16 runs   (  108.41 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1738.19 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1684.97 ms /    16 runs   (  105.31 ms per token,     9.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1688.86 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1302.68 ms /    12 runs   (  108.56 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1305.33 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1706.00 ms /    16 runs   (  106.62 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1709.47 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1633.02 ms /    16 runs   (  102.06 ms per token,     9.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1636.50 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1518.09 ms /    14 runs   (  108.44 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1521.15 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1360.12 ms /    13 runs   (  104.62 ms per token,     9.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1362.99 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1952.25 ms /    18 runs   (  108.46 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1956.21 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1195.31 ms /    11 runs   (  108.66 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1197.75 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1248.40 ms /    12 runs   (  104.03 ms per token,     9.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1251.06 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 32 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     455.18 ms /    17 tokens (   26.78 ms per token,    37.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1695.88 ms /    16 runs   (  105.99 ms per token,     9.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    2154.94 ms /    33 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1150.48 ms /    11 runs   (  104.59 ms per token,     9.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1152.95 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1148.44 ms /    11 runs   (  104.40 ms per token,     9.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1150.93 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1179.16 ms /    11 runs   (  107.20 ms per token,     9.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1181.62 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1153.97 ms /    11 runs   (  104.91 ms per token,     9.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1156.46 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1333.20 ms /    13 runs   (  102.55 ms per token,     9.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1336.21 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1421.82 ms /    14 runs   (  101.56 ms per token,     9.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1425.05 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1254.48 ms /    12 runs   (  104.54 ms per token,     9.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1257.18 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1131.38 ms /    11 runs   (  102.85 ms per token,     9.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1133.85 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1469.45 ms /    14 runs   (  104.96 ms per token,     9.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1472.79 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1373.05 ms /    13 runs   (  105.62 ms per token,     9.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1375.97 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1232.40 ms /    12 runs   (  102.70 ms per token,     9.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1235.05 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1580.31 ms /    15 runs   (  105.35 ms per token,     9.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1583.68 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1464.55 ms /    14 runs   (  104.61 ms per token,     9.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1467.60 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1122.75 ms /    11 runs   (  102.07 ms per token,     9.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1125.35 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1365.68 ms /    13 runs   (  105.05 ms per token,     9.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1368.61 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1151.99 ms /    11 runs   (  104.73 ms per token,     9.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1154.74 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1131.61 ms /    11 runs   (  102.87 ms per token,     9.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1134.08 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1475.52 ms /    14 runs   (  105.39 ms per token,     9.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1478.64 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1153.81 ms /    11 runs   (  104.89 ms per token,     9.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1156.27 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 33 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     474.17 ms /    16 tokens (   29.64 ms per token,    33.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1290.59 ms /    11 runs   (  117.33 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1767.82 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1804.50 ms /    14 runs   (  128.89 ms per token,     7.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1807.73 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1791.36 ms /    13 runs   (  137.80 ms per token,     7.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1794.49 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1415.97 ms /    11 runs   (  128.72 ms per token,     7.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1418.91 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1532.99 ms /    12 runs   (  127.75 ms per token,     7.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1535.73 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1317.47 ms /    11 runs   (  119.77 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1320.00 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1536.69 ms /    12 runs   (  128.06 ms per token,     7.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1539.41 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1416.03 ms /    11 runs   (  128.73 ms per token,     7.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1418.49 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1652.59 ms /    14 runs   (  118.04 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1655.70 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1766.07 ms /    15 runs   (  117.74 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1769.61 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1337.90 ms /    12 runs   (  111.49 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1340.67 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1266.65 ms /    11 runs   (  115.15 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1269.10 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1272.79 ms /    11 runs   (  115.71 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1275.47 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1257.35 ms /    11 runs   (  114.30 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1259.86 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1290.83 ms /    11 runs   (  117.35 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1293.38 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1528.70 ms /    13 runs   (  117.59 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1531.69 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1495.07 ms /    13 runs   (  115.01 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1497.97 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1273.64 ms /    11 runs   (  115.79 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1276.17 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1269.77 ms /    11 runs   (  115.43 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1272.41 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1223.57 ms /    11 runs   (  111.23 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1226.08 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 34 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     535.44 ms /    15 tokens (   35.70 ms per token,    28.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =     976.98 ms /     9 runs   (  108.55 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1514.67 ms /    24 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1212.12 ms /    11 runs   (  110.19 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1215.14 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1146.67 ms /    10 runs   (  114.67 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1149.01 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1154.86 ms /    10 runs   (  115.49 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1157.19 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1124.50 ms /    10 runs   (  112.45 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1126.92 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1131.26 ms /    10 runs   (  113.13 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1133.57 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1113.90 ms /    10 runs   (  111.39 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1116.41 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1254.63 ms /    11 runs   (  114.06 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1257.13 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1259.39 ms /    11 runs   (  114.49 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1261.91 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1218.21 ms /    11 runs   (  110.75 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1220.70 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1286.12 ms /    11 runs   (  116.92 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1288.65 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'government', 'announced', 'new', '[mask]', 'to', 'support', 'healthcare'] ['the', 'biden', 'administration', 'announced', 'new', 'measures', 'to', 'support', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1613.85 ms /    14 runs   (  115.28 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1617.05 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1241.20 ms /    11 runs   (  112.84 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1243.73 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1258.47 ms /    11 runs   (  114.41 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1261.23 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1115.83 ms /    10 runs   (  111.58 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1118.07 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1223.97 ms /    11 runs   (  111.27 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1226.42 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1131.61 ms /    10 runs   (  113.16 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1133.91 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1351.85 ms /    12 runs   (  112.65 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1354.58 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1227.34 ms /    11 runs   (  111.58 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1229.84 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1200.53 ms /    11 runs   (  109.14 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1202.90 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 35 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     418.09 ms /    14 tokens (   29.86 ms per token,    33.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1154.85 ms /     9 runs   (  128.32 ms per token,     7.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1575.33 ms /    23 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1629.38 ms /    13 runs   (  125.34 ms per token,     7.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1632.32 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1567.48 ms /    12 runs   (  130.62 ms per token,     7.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1570.41 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1434.19 ms /    11 runs   (  130.38 ms per token,     7.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1437.53 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1478.79 ms /    12 runs   (  123.23 ms per token,     8.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1481.51 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'government', 'announced', 'new', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'united', 'states', 'government', 'announced', 'new', 'policies', 'to', 'strengthen', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1287.77 ms /    10 runs   (  128.78 ms per token,     7.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1289.97 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1630.98 ms /    11 runs   (  148.27 ms per token,     6.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1633.54 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1234.54 ms /    10 runs   (  123.45 ms per token,     8.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1236.88 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'government', 'announced', 'new', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'strengthen', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1432.71 ms /    12 runs   (  119.39 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1435.67 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1424.04 ms /    12 runs   (  118.67 ms per token,     8.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1427.11 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1347.51 ms /    12 runs   (  112.29 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1350.33 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1232.50 ms /    11 runs   (  112.05 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1235.04 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1439.94 ms /    13 runs   (  110.76 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1442.98 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1411.35 ms /    12 runs   (  117.61 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1414.30 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'government', 'announced', 'new', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'increase', 'funding', 'for', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1201.90 ms /    10 runs   (  120.19 ms per token,     8.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1204.69 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1406.85 ms /    12 runs   (  117.24 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1409.64 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1316.67 ms /    11 runs   (  119.70 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1319.33 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'government', 'announced', 'new', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'biden', 'administration', 'announced', 'new', 'policies', 'to', 'support', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1270.60 ms /    11 runs   (  115.51 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1273.40 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1402.21 ms /    12 runs   (  116.85 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1404.98 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1436.88 ms /    12 runs   (  119.74 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1439.71 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 36 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     419.79 ms /    13 tokens (   32.29 ms per token,    30.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1196.30 ms /    10 runs   (  119.63 ms per token,     8.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1618.65 ms /    23 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1259.62 ms /    11 runs   (  114.51 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1262.20 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1226.75 ms /    10 runs   (  122.67 ms per token,     8.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1229.45 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1198.19 ms /    10 runs   (  119.82 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1200.45 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1280.42 ms /    11 runs   (  116.40 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1282.91 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1135.09 ms /    10 runs   (  113.51 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1137.47 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1142.91 ms /    10 runs   (  114.29 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1145.21 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1118.33 ms /    10 runs   (  111.83 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1120.74 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1215.92 ms /    11 runs   (  110.54 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1218.38 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1100.95 ms /    10 runs   (  110.10 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1103.22 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1129.55 ms /    10 runs   (  112.95 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1131.79 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1348.99 ms /    12 runs   (  112.42 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1351.67 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1147.41 ms /    10 runs   (  114.74 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1149.63 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1250.54 ms /    11 runs   (  113.69 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1253.01 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1127.96 ms /    10 runs   (  112.80 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1130.37 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1468.66 ms /    13 runs   (  112.97 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1471.77 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1145.08 ms /    10 runs   (  114.51 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1147.35 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1563.88 ms /    14 runs   (  111.71 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1566.99 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1132.39 ms /    10 runs   (  113.24 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1134.64 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1571.43 ms /    14 runs   (  112.25 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1574.51 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 37 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     513.99 ms /    12 tokens (   42.83 ms per token,    23.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1340.92 ms /    12 runs   (  111.74 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1857.85 ms /    24 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1356.09 ms /    12 runs   (  113.01 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1358.73 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1230.00 ms /    11 runs   (  111.82 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1232.81 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1464.00 ms /    13 runs   (  112.62 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1466.90 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1130.87 ms /    10 runs   (  113.09 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1133.11 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1331.77 ms /    12 runs   (  110.98 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1334.41 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1213.39 ms /    11 runs   (  110.31 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1215.82 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1137.73 ms /    10 runs   (  113.77 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1140.16 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1707.79 ms /    15 runs   (  113.85 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1711.11 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1325.55 ms /    12 runs   (  110.46 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1328.41 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1371.40 ms /    12 runs   (  114.28 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1374.03 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1131.50 ms /    10 runs   (  113.15 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1133.74 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1554.26 ms /    12 runs   (  129.52 ms per token,     7.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1556.94 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1373.94 ms /    12 runs   (  114.50 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1376.63 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1482.59 ms /    13 runs   (  114.05 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1485.84 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'government', 'announced', 'new', 'policies', 'to', 'support', '[mask]'] ['the', 'biden', 'administration', 'announced', 'new', 'policies', 'to', 'support', 'renewable', 'energy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1238.38 ms /    11 runs   (  112.58 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1240.89 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1334.85 ms /    12 runs   (  111.24 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1337.58 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1410.17 ms /    12 runs   (  117.51 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1413.10 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1336.84 ms /    12 runs   (  111.40 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1339.60 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1308.74 ms /    11 runs   (  118.98 ms per token,     8.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1311.26 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • the united states government\n",
      " • the united states [mask]\n",
      " • the white house [mask] [mask]\n",
      " • the world health organization\n",
      " • the government [mask]\n",
      " • the united states [mask]\n",
      " • the united states government\n",
      " • the white house the white house\n",
      " • the united states government\n",
      " • the united states [mask] congress\n",
      " • the world health organization who\n",
      " • the world health organization who\n",
      " • the world health organization\n",
      " • the world health organization who\n",
      " • the world health organization who\n",
      " • the biden [mask] administration\n",
      " • the government [mask]\n",
      " • the united states [mask] government [mask]\n",
      " • the white house\n",
      " • the world health organization\n",
      " • the us is planning to introduce\n",
      " • the is implementing\n",
      " • the will introduce\n",
      " • the is implementing\n",
      " • the is implementing\n",
      " • the united states is introducing\n",
      " • the [mask] is implementing\n",
      " • the australian is introducing\n",
      " • the is implementing\n",
      " • the [mask] is implementing\n",
      " • the has unveiled\n",
      " • the canadian has introduced\n",
      " • the us is implementing\n",
      " • the us is planning to implement\n",
      " • the is implementing\n",
      " • the united states is introducing\n",
      " • the is implementing\n",
      " • the is implementing\n",
      " • the [mask] is implementing\n",
      " • the is introducing\n",
      " • the south korean comprehensive\n",
      " • the us comprehensive\n",
      " • the us a series of\n",
      " • the british new\n",
      " • the south korean comprehensive\n",
      " • the british comprehensive\n",
      " • the south korean comprehensive\n",
      " • the italian new\n",
      " • the us comprehensive\n",
      " • the united states a series of comprehensive\n",
      " • the brazilian progressive\n",
      " • the italian new\n",
      " • the british comprehensive\n",
      " • the british comprehensive\n",
      " • the italian comprehensive\n",
      " • the french a series of\n",
      " • the australian a range of\n",
      " • the uk comprehensive\n",
      " • the australian new\n",
      " • the british comprehensive\n",
      " • the regulations\n",
      " • the australian measures\n",
      " • the policies\n",
      " • the initiatives\n",
      " • the policies\n",
      " • the regulations\n",
      " • the policies\n",
      " • the us regulations\n",
      " • the australian measures\n",
      " • the australian measures\n",
      " •  \n",
      " • the us regulations\n",
      " • the federal policies\n",
      " • the federal policies\n",
      " • the regulations\n",
      " • the federal initiatives\n",
      " • the regulations\n",
      " • the united states initiatives\n",
      " • the australian initiatives\n",
      " • the australian initiatives\n",
      " • the to\n",
      " • the australian to strengthen\n",
      " • the ukrainian to\n",
      " • the to strengthen healthcare\n",
      " •  \n",
      " • the to\n",
      " • the british to\n",
      " •  \n",
      " • the united states to\n",
      " • the united states to\n",
      " • the united states to\n",
      " • the to strengthen healthcare\n",
      " • the chinese to strengthen\n",
      " •  \n",
      " • the to\n",
      " • the to strengthen\n",
      " •  \n",
      " • the to strengthen healthcare\n",
      " • the chinese to strengthen healthcare\n",
      " • the federal to strengthen healthcare\n",
      " • the uk improve\n",
      " • the british improve\n",
      " • the new \n",
      " • the improve\n",
      " • the british improve\n",
      " • the improve\n",
      " • the improve\n",
      " • the improve\n",
      " • the us improve\n",
      " • the improve\n",
      " • the improve\n",
      " • the united states improve\n",
      " • the improve\n",
      " • the british improve\n",
      " • the improve\n",
      " • the us improve\n",
      " • the improve\n",
      " • the us improve\n",
      " • the improve\n",
      " • the us improve\n",
      " • the iranian small businesses\n",
      " • the french small businesses\n",
      " • the small businesses\n",
      " • the united states small businesses\n",
      " • the businesses\n",
      " • the australian small businesses\n",
      " • the entrepreneurs\n",
      " • the citizens\n",
      " • the us small businesses\n",
      " • the australian small businesses\n",
      " • the us small businesses\n",
      " • the agriculture\n",
      " • the japanese small businesses\n",
      " • the british small businesses\n",
      " •  \n",
      " • the small businesses\n",
      " • the british small businesses\n",
      " • the australian small businesses\n",
      " • the iranian artists\n",
      " • the local businesses\n",
      "\n",
      "Total generated: 140\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "government      PMI=   -inf   P(x)=1.000  P(y)=0.000  P(xy)=0.000\n",
      "announced       PMI=   -inf   P(x)=0.500  P(y)=0.000  P(xy)=0.000\n",
      "new             PMI=   -inf   P(x)=0.000  P(y)=0.200  P(xy)=0.000\n",
      "policies        PMI=  0.585   P(x)=0.400  P(y)=0.250  P(xy)=0.150\n",
      "to              PMI=  0.100   P(x)=0.350  P(y)=0.400  P(xy)=0.150\n",
      "support         PMI=   -inf   P(x)=0.500  P(y)=0.000  P(xy)=0.000\n",
      "healthcare      PMI=   -inf   P(x)=0.350  P(y)=0.000  P(xy)=0.000\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 1   |   Anchor word: 'government'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 32 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     444.93 ms /    17 tokens (   26.17 ms per token,    38.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1749.04 ms /    15 runs   (  116.60 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    2197.56 ms /    32 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1318.88 ms /    11 runs   (  119.90 ms per token,     8.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1321.35 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1801.20 ms /    16 runs   (  112.58 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1805.06 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1492.05 ms /    13 runs   (  114.77 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1495.09 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1879.10 ms /    16 runs   (  117.44 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1882.67 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1790.03 ms /    16 runs   (  111.88 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1793.68 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1924.83 ms /    17 runs   (  113.23 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1928.84 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1344.41 ms /    12 runs   (  112.03 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1347.05 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1177.02 ms /    10 runs   (  117.70 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1179.32 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1358.16 ms /    12 runs   (  113.18 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1360.80 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1233.08 ms /    11 runs   (  112.10 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1235.60 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1139.75 ms /    10 runs   (  113.97 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1141.93 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1137.14 ms /    10 runs   (  113.71 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1139.37 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1128.57 ms /    10 runs   (  112.86 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1130.78 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1346.27 ms /    12 runs   (  112.19 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1349.13 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1367.15 ms /    12 runs   (  113.93 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1369.74 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1143.05 ms /    10 runs   (  114.31 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1145.26 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1117.90 ms /    10 runs   (  111.79 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1120.27 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1553.22 ms /    14 runs   (  110.94 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1556.26 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] [MASK] announced new policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1804.17 ms /    16 runs   (  112.76 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1807.95 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:   0%| |Llama.generate: 29 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     593.42 ms /    20 tokens (   29.67 ms per token,    33.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1340.35 ms /    11 runs   (  121.85 ms per token,     8.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1937.37 ms /    31 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:   5%| |Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1193.15 ms /    10 runs   (  119.32 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1195.76 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  10%| |Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1220.39 ms /    10 runs   (  122.04 ms per token,     8.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1222.93 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  15%|▏|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1265.45 ms /    10 runs   (  126.55 ms per token,     7.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1267.92 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  20%|▏|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1268.10 ms /    10 runs   (  126.81 ms per token,     7.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1270.64 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  25%|▎|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1225.91 ms /    10 runs   (  122.59 ms per token,     8.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1228.71 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  30%|▎|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1404.41 ms /    12 runs   (  117.03 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1407.47 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  35%|▎|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1599.68 ms /    13 runs   (  123.05 ms per token,     8.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1603.62 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  40%|▍|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1305.93 ms /    11 runs   (  118.72 ms per token,     8.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1308.44 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  45%|▍|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1649.14 ms /    14 runs   (  117.80 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1652.17 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  50%|▌|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1203.59 ms /    10 runs   (  120.36 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1205.96 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  55%|▌|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1184.81 ms /    10 runs   (  118.48 ms per token,     8.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1186.98 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  60%|▌|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1160.26 ms /    10 runs   (  116.03 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1162.67 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  65%|▋|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1339.85 ms /    11 runs   (  121.80 ms per token,     8.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1342.23 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  70%|▋|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1599.80 ms /    11 runs   (  145.44 ms per token,     6.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1602.33 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  75%|▊|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1210.54 ms /    10 runs   (  121.05 ms per token,     8.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1212.73 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  80%|▊|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1314.32 ms /    11 runs   (  119.48 ms per token,     8.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1317.03 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  85%|▊|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1145.33 ms /    10 runs   (  114.53 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1147.59 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  90%|▉|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1070.51 ms /    10 runs   (  107.05 ms per token,     9.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1072.72 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  95%|▉|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1026.09 ms /     9 runs   (  114.01 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1028.09 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 33 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     526.59 ms /    16 tokens (   32.91 ms per token,    30.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1217.39 ms /     9 runs   (  135.27 ms per token,     7.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1747.46 ms /    25 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1207.28 ms /    10 runs   (  120.73 ms per token,     8.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1209.94 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1379.93 ms /    12 runs   (  114.99 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1382.52 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1177.79 ms /    10 runs   (  117.78 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1180.00 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1171.14 ms /    10 runs   (  117.11 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1173.53 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1158.52 ms /    10 runs   (  115.85 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1160.75 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1376.04 ms /    12 runs   (  114.67 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1378.64 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1143.69 ms /    10 runs   (  114.37 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1145.86 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1449.76 ms /    12 runs   (  120.81 ms per token,     8.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1452.50 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1192.98 ms /    10 runs   (  119.30 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1195.25 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1132.48 ms /    10 runs   (  113.25 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1134.70 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1152.56 ms /    10 runs   (  115.26 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1154.74 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1134.42 ms /    10 runs   (  113.44 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1136.89 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1110.40 ms /    10 runs   (  111.04 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1112.62 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1370.43 ms /    12 runs   (  114.20 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1373.08 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1108.34 ms /    10 runs   (  110.83 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1110.52 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1130.68 ms /    10 runs   (  113.07 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1132.86 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1112.56 ms /    10 runs   (  111.26 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1114.85 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1129.21 ms /    10 runs   (  112.92 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1131.39 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1092.83 ms /    10 runs   (  109.28 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1095.14 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:   0%| Llama.generate: 34 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     582.56 ms /    15 tokens (   38.84 ms per token,    25.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1193.06 ms /     9 runs   (  132.56 ms per token,     7.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1777.97 ms /    24 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:   5%| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1373.32 ms /    10 runs   (  137.33 ms per token,     7.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1375.54 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  10%| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1349.76 ms /    10 runs   (  134.98 ms per token,     7.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1352.20 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  15%|▏Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1322.37 ms /    10 runs   (  132.24 ms per token,     7.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1324.61 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  20%|▏Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1332.21 ms /    10 runs   (  133.22 ms per token,     7.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1334.51 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  25%|▎Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1292.81 ms /    10 runs   (  129.28 ms per token,     7.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1295.31 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  30%|▎Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1246.51 ms /    10 runs   (  124.65 ms per token,     8.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1248.72 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  35%|▎Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1315.76 ms /    10 runs   (  131.58 ms per token,     7.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1317.97 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  40%|▍Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1327.71 ms /    10 runs   (  132.77 ms per token,     7.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1329.93 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  45%|▍Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1297.08 ms /    10 runs   (  129.71 ms per token,     7.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1299.29 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  50%|▌Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1283.80 ms /    10 runs   (  128.38 ms per token,     7.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1286.04 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  55%|▌Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1257.06 ms /    10 runs   (  125.71 ms per token,     7.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1259.28 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  60%|▌Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1180.07 ms /    10 runs   (  118.01 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1182.53 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  65%|▋Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1171.18 ms /    10 runs   (  117.12 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1173.66 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  70%|▋Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1147.56 ms /    10 runs   (  114.76 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1149.82 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  75%|▊Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1153.24 ms /    10 runs   (  115.32 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1155.47 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  80%|▊Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1157.84 ms /    10 runs   (  115.78 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1160.02 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  85%|▊Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1167.99 ms /    10 runs   (  116.80 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1170.29 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  90%|▉Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1149.01 ms /    10 runs   (  114.90 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1151.21 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  95%|▉Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1138.17 ms /    10 runs   (  113.82 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1140.48 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 35 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     619.98 ms /    14 tokens (   44.28 ms per token,    22.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1308.14 ms /     9 runs   (  145.35 ms per token,     6.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1930.64 ms /    23 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1384.53 ms /    10 runs   (  138.45 ms per token,     7.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1387.82 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1626.03 ms /    10 runs   (  162.60 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1628.29 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', '[mask]', 'announced', 'new', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'strongly', 'supporting', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1668.03 ms /    12 runs   (  139.00 ms per token,     7.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1670.76 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1203.72 ms /     9 runs   (  133.75 ms per token,     7.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1206.00 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', '[mask]', 'announced', 'new', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'supporting', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1556.47 ms /    12 runs   (  129.71 ms per token,     7.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1559.14 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1250.21 ms /    10 runs   (  125.02 ms per token,     8.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1252.46 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1158.35 ms /    10 runs   (  115.83 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1160.62 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1032.18 ms /     9 runs   (  114.69 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1034.21 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', '[mask]', 'announced', 'new', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'supporting', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1115.86 ms /    10 runs   (  111.59 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1118.37 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1116.35 ms /    10 runs   (  111.63 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1118.72 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1077.35 ms /    10 runs   (  107.74 ms per token,     9.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1079.55 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', '[mask]', 'announced', 'new', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'strongly', 'supporting', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1335.61 ms /    12 runs   (  111.30 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1338.27 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1115.19 ms /    10 runs   (  111.52 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1117.44 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1086.02 ms /    10 runs   (  108.60 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1088.28 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1066.53 ms /    10 runs   (  106.65 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1068.75 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', '[mask]', 'announced', 'new', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'strongly', 'supporting', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1278.36 ms /    12 runs   (  106.53 ms per token,     9.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1281.10 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1110.54 ms /    10 runs   (  111.05 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1112.80 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1311.62 ms /    12 runs   (  109.30 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1314.49 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1114.88 ms /    10 runs   (  111.49 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1117.32 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:   0%|Llama.generate: 36 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     368.72 ms /    13 tokens (   28.36 ms per token,    35.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1199.32 ms /     9 runs   (  133.26 ms per token,     7.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1570.32 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:   5%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1326.91 ms /    10 runs   (  132.69 ms per token,     7.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1329.11 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  10%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1291.58 ms /    10 runs   (  129.16 ms per token,     7.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1293.95 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  15%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1261.45 ms /    10 runs   (  126.14 ms per token,     7.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1263.75 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  20%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1252.16 ms /    10 runs   (  125.22 ms per token,     7.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1254.36 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  25%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1165.58 ms /     9 runs   (  129.51 ms per token,     7.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1167.78 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  30%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1270.76 ms /    10 runs   (  127.08 ms per token,     7.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1272.97 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  35%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1252.99 ms /    10 runs   (  125.30 ms per token,     7.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1255.18 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  40%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1294.74 ms /    10 runs   (  129.47 ms per token,     7.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1296.94 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  45%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1280.53 ms /    10 runs   (  128.05 ms per token,     7.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1282.72 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  50%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1204.57 ms /    10 runs   (  120.46 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1206.74 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  55%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1173.22 ms /    10 runs   (  117.32 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1175.38 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  60%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     987.06 ms /     9 runs   (  109.67 ms per token,     9.12 tokens per second)\n",
      "llama_perf_context_print:       total time =     989.16 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  65%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1097.30 ms /    10 runs   (  109.73 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1099.50 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  70%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1044.90 ms /    10 runs   (  104.49 ms per token,     9.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1047.03 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  75%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1089.37 ms /    10 runs   (  108.94 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1091.50 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  80%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1125.89 ms /    10 runs   (  112.59 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1128.95 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  85%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1078.46 ms /    10 runs   (  107.85 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1080.88 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  90%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1084.42 ms /    10 runs   (  108.44 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1086.72 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  95%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1097.16 ms /    10 runs   (  109.72 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1099.31 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:   0%| | Llama.generate: 37 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     424.36 ms /    12 tokens (   35.36 ms per token,    28.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1132.19 ms /    10 runs   (  113.22 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1559.19 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:   5%| | Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1227.05 ms /    11 runs   (  111.55 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1229.74 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  10%| | Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1121.62 ms /    10 runs   (  112.16 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1123.92 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  15%|▏| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1122.57 ms /    10 runs   (  112.26 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1124.77 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  20%|▏| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1208.35 ms /    11 runs   (  109.85 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1211.01 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  25%|▎| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1245.35 ms /    11 runs   (  113.21 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1247.71 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  30%|▎| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1235.57 ms /    11 runs   (  112.32 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1238.14 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  35%|▎| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1095.49 ms /    10 runs   (  109.55 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1097.64 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  40%|▍| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1233.08 ms /    11 runs   (  112.10 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1235.52 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  45%|▍| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1120.88 ms /    10 runs   (  112.09 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1123.97 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  50%|▌| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1456.80 ms /    10 runs   (  145.68 ms per token,     6.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1458.97 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  55%|▌| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1114.76 ms /    10 runs   (  111.48 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1116.98 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  60%|▌| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1124.91 ms /    10 runs   (  112.49 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1127.09 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  65%|▋| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1218.16 ms /    11 runs   (  110.74 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1220.65 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  70%|▋| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1138.36 ms /    11 runs   (  103.49 ms per token,     9.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1140.80 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  75%|▊| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1209.95 ms /    11 runs   (  110.00 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1212.56 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  80%|▊| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1090.30 ms /    10 runs   (  109.03 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1092.47 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  85%|▊| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1075.73 ms /    10 runs   (  107.57 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1077.83 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  90%|▉| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1208.40 ms /    11 runs   (  109.85 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1210.75 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  95%|▉| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1205.45 ms /    11 runs   (  109.59 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1207.83 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • the world health organization who\n",
      " • the white house\n",
      " • the world health organization who\n",
      " • the biden [mask]\n",
      " • the world health organization who\n",
      " • the world health organization who\n",
      " • the united states [mask] government [mask]\n",
      " • the world health organization\n",
      " • the government\n",
      " • the world health organization\n",
      " • the white house\n",
      " • the government\n",
      " • the government\n",
      " • the government\n",
      " • the world health organization\n",
      " • the world health organization\n",
      " • the company\n",
      " • the government\n",
      " • the united states [mask]\n",
      " • the world health organization who\n",
      " • world health organization introduced\n",
      " • government introduced\n",
      " • government is implementing\n",
      " • government introduced\n",
      " • government will implement\n",
      " • government introduces\n",
      " • world health organization introduced\n",
      " • world health organization introduces\n",
      " • government is implementing\n",
      " • government unveiled the ambitious\n",
      " • government recently implemented\n",
      " • government is implementing\n",
      " • government introduced\n",
      " • government introduces\n",
      " • government has implemented\n",
      " • government recently introduced\n",
      " • government has implemented\n",
      " • government has introduced\n",
      " • government introduces\n",
      " • government implemented\n",
      " • government comprehensive\n",
      " • government comprehensive\n",
      " • world health organization comprehensive\n",
      " • government new\n",
      " • government comprehensive\n",
      " • government comprehensive\n",
      " • world health organization comprehensive\n",
      " • government comprehensive\n",
      " • world health organization comprehensive\n",
      " • government new\n",
      " • government comprehensive\n",
      " • government comprehensive\n",
      " • government comprehensive\n",
      " • government comprehensive\n",
      " • world health organization comprehensive\n",
      " • government comprehensive\n",
      " • government comprehensive\n",
      " • government comprehensive\n",
      " • government new\n",
      " • government new\n",
      " • government initiatives\n",
      " • government regulations\n",
      " • government initiatives\n",
      " • government policies\n",
      " • government policies\n",
      " • government measures\n",
      " • government initiatives\n",
      " • government initiatives\n",
      " • government policies\n",
      " • government measures\n",
      " • government policies\n",
      " • government initiatives\n",
      " • government programs\n",
      " • government initiatives\n",
      " • government initiatives\n",
      " • government initiatives\n",
      " • government regulations\n",
      " • government initiatives\n",
      " • government initiatives\n",
      " • government measures\n",
      " • government to\n",
      " • government to\n",
      " •  \n",
      " • government to strengthen\n",
      " •  \n",
      " • world health organization to\n",
      " • government to\n",
      " • government to\n",
      " •  \n",
      " • government to\n",
      " • government to\n",
      " •  \n",
      " • government to increase\n",
      " • government to\n",
      " • government to\n",
      " •  \n",
      " • government to increase\n",
      " • government to\n",
      " • government to strengthen\n",
      " • government to\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government small businesses\n",
      " • government small businesses\n",
      " • government businesses\n",
      " • government education\n",
      " • government small businesses\n",
      " • government small businesses\n",
      " • government small businesses\n",
      " • government businesses\n",
      " • government small businesses\n",
      " • government businesses\n",
      " • government businesses\n",
      " • government businesses\n",
      " • government businesses\n",
      " • government small businesses\n",
      " • government small businesses\n",
      " • government small businesses\n",
      " • government businesses\n",
      " • government citizens\n",
      " • government small businesses\n",
      " • government small businesses\n",
      "\n",
      "Total generated: 140\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "the             PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "announced       PMI=   -inf   P(x)=0.850  P(y)=0.000  P(xy)=0.000\n",
      "new             PMI=  0.322   P(x)=0.800  P(y)=0.200  P(xy)=0.200\n",
      "policies        PMI=  0.000   P(x)=1.000  P(y)=0.200  P(xy)=0.200\n",
      "to              PMI=  0.377   P(x)=0.700  P(y)=0.550  P(xy)=0.500\n",
      "support         PMI=   -inf   P(x)=1.000  P(y)=0.000  P(xy)=0.000\n",
      "healthcare      PMI=   -inf   P(x)=1.000  P(y)=0.000  P(xy)=0.000\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 2   |   Anchor word: 'announced'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 29 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     516.76 ms /    20 tokens (   25.84 ms per token,    38.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1438.69 ms /    14 runs   (  102.76 ms per token,     9.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1959.07 ms /    34 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1899.37 ms /    17 runs   (  111.73 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1903.18 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1220.77 ms /    11 runs   (  110.98 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1223.21 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1193.74 ms /    11 runs   (  108.52 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1196.13 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1231.35 ms /    11 runs   (  111.94 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1233.73 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1219.16 ms /    11 runs   (  110.83 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1221.55 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1376.99 ms /    13 runs   (  105.92 ms per token,     9.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1379.90 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1309.36 ms /    12 runs   (  109.11 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1312.35 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1196.51 ms /    11 runs   (  108.77 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1198.90 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1168.25 ms /    11 runs   (  106.20 ms per token,     9.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1170.63 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1527.46 ms /    14 runs   (  109.10 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1530.47 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1195.25 ms /    11 runs   (  108.66 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1197.63 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1621.56 ms /    15 runs   (  108.10 ms per token,     9.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1624.79 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1305.18 ms /    12 runs   (  108.77 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1307.81 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1304.90 ms /    12 runs   (  108.74 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1307.53 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1620.73 ms /    15 runs   (  108.05 ms per token,     9.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1623.97 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1208.30 ms /    11 runs   (  109.85 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1210.82 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1190.92 ms /    11 runs   (  108.27 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1193.74 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1362.63 ms /    13 runs   (  104.82 ms per token,     9.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1365.64 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] government [MASK] new policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1190.88 ms /    11 runs   (  108.26 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1193.26 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:   0%| |Llama.generate: 29 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     523.74 ms /    20 tokens (   26.19 ms per token,    38.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =     894.80 ms /     8 runs   (  111.85 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1420.57 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:   5%| |Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1031.16 ms /    10 runs   (  103.12 ms per token,     9.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1033.43 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  10%| |Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1691.36 ms /    16 runs   (  105.71 ms per token,     9.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1694.85 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  15%|▏|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1051.31 ms /    10 runs   (  105.13 ms per token,     9.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1053.52 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  20%|▏|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1053.62 ms /    10 runs   (  105.36 ms per token,     9.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1055.86 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  25%|▎|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1204.97 ms /    11 runs   (  109.54 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1207.47 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  30%|▎|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1264.53 ms /    12 runs   (  105.38 ms per token,     9.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1267.28 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  35%|▎|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1104.67 ms /    10 runs   (  110.47 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1107.15 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  40%|▍|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1008.12 ms /    10 runs   (  100.81 ms per token,     9.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1010.46 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  45%|▍|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1298.65 ms /    12 runs   (  108.22 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1301.29 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  50%|▌|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1085.32 ms /    10 runs   (  108.53 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1087.65 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  55%|▌|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     942.82 ms /     9 runs   (  104.76 ms per token,     9.55 tokens per second)\n",
      "llama_perf_context_print:       total time =     944.83 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  60%|▌|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1195.59 ms /    11 runs   (  108.69 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1198.03 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  65%|▋|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1086.57 ms /    10 runs   (  108.66 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1088.86 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  70%|▋|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     875.08 ms /     8 runs   (  109.39 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =     876.91 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  75%|▊|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1049.28 ms /    10 runs   (  104.93 ms per token,     9.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1051.50 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  80%|▊|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1414.57 ms /    13 runs   (  108.81 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1417.52 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  85%|▊|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1071.60 ms /    10 runs   (  107.16 ms per token,     9.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1073.91 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  90%|▉|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1425.53 ms /    11 runs   (  129.59 ms per token,     7.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1428.10 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] [MASK] new policies to support healthcare:  95%|▉|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1269.53 ms /    11 runs   (  115.41 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1272.20 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 30 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     552.98 ms /    19 tokens (   29.10 ms per token,    34.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1475.93 ms /    13 runs   (  113.53 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    2032.46 ms /    32 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1008.65 ms /     9 runs   (  112.07 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1010.74 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1806.40 ms /    16 runs   (  112.90 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1810.17 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1634.96 ms /    14 runs   (  116.78 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1638.31 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1313.95 ms /    11 runs   (  119.45 ms per token,     8.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1316.45 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1446.95 ms /    12 runs   (  120.58 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1449.69 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1836.53 ms /    16 runs   (  114.78 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1839.97 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1049.19 ms /     9 runs   (  116.58 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1051.35 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     907.67 ms /     8 runs   (  113.46 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =     909.55 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1026.58 ms /     9 runs   (  114.06 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1028.76 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1532.43 ms /    14 runs   (  109.46 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1535.45 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1017.61 ms /     9 runs   (  113.07 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1019.58 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1362.79 ms /    12 runs   (  113.57 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1365.36 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     971.13 ms /     9 runs   (  107.90 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =     973.09 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1024.47 ms /     9 runs   (  113.83 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1026.43 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     903.28 ms /     8 runs   (  112.91 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =     905.07 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1749.98 ms /    16 runs   (  109.37 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1753.38 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1016.47 ms /     9 runs   (  112.94 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1018.47 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1017.06 ms /     9 runs   (  113.01 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1019.01 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     869.54 ms /     8 runs   (  108.69 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =     871.36 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:   0%|Llama.generate: 34 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     628.98 ms /    15 tokens (   41.93 ms per token,    23.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1100.60 ms /     9 runs   (  122.29 ms per token,     8.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1731.88 ms /    24 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:   5%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1074.61 ms /     9 runs   (  119.40 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1076.69 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  10%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1251.03 ms /    10 runs   (  125.10 ms per token,     7.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1253.25 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  15%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1189.55 ms /    10 runs   (  118.96 ms per token,     8.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1191.75 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  20%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1190.61 ms /    10 runs   (  119.06 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1193.01 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  25%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1210.59 ms /    10 runs   (  121.06 ms per token,     8.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1212.75 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  30%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1174.93 ms /    10 runs   (  117.49 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1177.12 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  35%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1236.36 ms /    10 runs   (  123.64 ms per token,     8.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1238.49 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  40%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1223.76 ms /    10 runs   (  122.38 ms per token,     8.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1225.94 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  45%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1253.63 ms /    10 runs   (  125.36 ms per token,     7.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1255.81 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  50%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1138.32 ms /    10 runs   (  113.83 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1140.61 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  55%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1178.94 ms /    10 runs   (  117.89 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1181.27 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  60%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1302.01 ms /    11 runs   (  118.36 ms per token,     8.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1304.35 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  65%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1167.68 ms /    10 runs   (  116.77 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1170.08 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  70%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1297.91 ms /    11 runs   (  117.99 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1300.32 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  75%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1170.18 ms /    10 runs   (  117.02 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1172.38 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  80%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1144.45 ms /    10 runs   (  114.45 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1146.61 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  85%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1386.52 ms /    12 runs   (  115.54 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1389.33 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  90%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1171.50 ms /    10 runs   (  117.15 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1173.71 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  95%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1152.50 ms /    10 runs   (  115.25 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1154.71 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 35 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     672.54 ms /    14 tokens (   48.04 ms per token,    20.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1271.23 ms /     9 runs   (  141.25 ms per token,     7.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1946.13 ms /    23 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1543.29 ms /    11 runs   (  140.30 ms per token,     7.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1546.21 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1530.42 ms /    11 runs   (  139.13 ms per token,     7.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1532.90 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1390.36 ms /    10 runs   (  139.04 ms per token,     7.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1392.61 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1736.64 ms /    11 runs   (  157.88 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1739.40 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1963.69 ms /    15 runs   (  130.91 ms per token,     7.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1967.01 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1229.67 ms /    10 runs   (  122.97 ms per token,     8.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1231.84 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1206.04 ms /    10 runs   (  120.60 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1208.31 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1415.21 ms /    12 runs   (  117.93 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1418.01 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1265.04 ms /    11 runs   (  115.00 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1267.40 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1120.35 ms /    10 runs   (  112.04 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1122.56 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1106.92 ms /    10 runs   (  110.69 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1109.09 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1128.19 ms /    10 runs   (  112.82 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1130.37 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1218.55 ms /    11 runs   (  110.78 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1221.18 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1206.00 ms /    11 runs   (  109.64 ms per token,     9.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1208.59 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1200.57 ms /    11 runs   (  109.14 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1202.98 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1087.25 ms /    10 runs   (  108.73 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1089.43 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1072.49 ms /    10 runs   (  107.25 ms per token,     9.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1074.67 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1341.84 ms /    12 runs   (  111.82 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1344.43 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1222.70 ms /    11 runs   (  111.15 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1225.10 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:   0%Llama.generate: 36 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     738.84 ms /    13 tokens (   56.83 ms per token,    17.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1188.69 ms /     9 runs   (  132.08 ms per token,     7.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1929.96 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:   5%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1348.32 ms /    11 runs   (  122.57 ms per token,     8.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1351.22 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  10%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1189.33 ms /    10 runs   (  118.93 ms per token,     8.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1191.55 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  15%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1351.34 ms /    11 runs   (  122.85 ms per token,     8.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1353.73 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  20%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1211.25 ms /    10 runs   (  121.13 ms per token,     8.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1213.42 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  25%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1219.99 ms /    10 runs   (  122.00 ms per token,     8.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1222.16 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  30%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1091.04 ms /     9 runs   (  121.23 ms per token,     8.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1093.07 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  35%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1284.89 ms /    11 runs   (  116.81 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1287.39 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  40%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1207.74 ms /    10 runs   (  120.77 ms per token,     8.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    1210.06 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  45%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1176.36 ms /    10 runs   (  117.64 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1178.53 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  50%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1189.42 ms /    10 runs   (  118.94 ms per token,     8.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1191.81 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  55%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1220.18 ms /    10 runs   (  122.02 ms per token,     8.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1222.46 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  60%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1175.08 ms /    10 runs   (  117.51 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1177.32 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  65%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1108.21 ms /    10 runs   (  110.82 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1110.45 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  70%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1138.26 ms /    10 runs   (  113.83 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1140.84 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  75%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1057.21 ms /     9 runs   (  117.47 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1059.27 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  80%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1056.82 ms /     9 runs   (  117.42 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1058.82 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  85%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1054.70 ms /     9 runs   (  117.19 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1056.76 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  90%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1173.75 ms /    10 runs   (  117.38 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1175.97 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  95%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1023.93 ms /     9 runs   (  113.77 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1025.84 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:   0%| |Llama.generate: 37 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     745.51 ms /    12 tokens (   62.13 ms per token,    16.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1016.85 ms /     9 runs   (  112.98 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1764.70 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:   5%| |Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1280.08 ms /    11 runs   (  116.37 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1282.49 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  10%| |Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1227.19 ms /    11 runs   (  111.56 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1229.60 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  15%|▏|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1325.70 ms /    12 runs   (  110.48 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1328.25 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  20%|▏|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1217.62 ms /    11 runs   (  110.69 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1219.97 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  25%|▎|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1006.89 ms /     9 runs   (  111.88 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1009.58 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  30%|▎|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1211.30 ms /    11 runs   (  110.12 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1213.65 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  35%|▎|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1228.10 ms /    11 runs   (  111.65 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1230.45 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  40%|▍|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1113.20 ms /    10 runs   (  111.32 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1116.09 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  45%|▍|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1331.57 ms /    12 runs   (  110.96 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1334.29 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  50%|▌|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1004.35 ms /     9 runs   (  111.59 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1006.58 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  55%|▌|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1260.99 ms /    11 runs   (  114.64 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1263.63 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  60%|▌|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1171.67 ms /    10 runs   (  117.17 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1173.98 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  65%|▋|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1138.40 ms /    10 runs   (  113.84 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1140.96 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  70%|▋|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1170.31 ms /    10 runs   (  117.03 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1172.78 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  75%|▊|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1183.31 ms /    10 runs   (  118.33 ms per token,     8.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1185.55 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  80%|▊|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1269.05 ms /    11 runs   (  115.37 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1271.43 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  85%|▊|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1254.06 ms /    11 runs   (  114.01 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1256.43 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  90%|▉|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1004.12 ms /     9 runs   (  111.57 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1006.10 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  95%|▉|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1219.15 ms /    11 runs   (  110.83 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1221.50 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • the us is implementing\n",
      " • the [mask] is implementing [mask]\n",
      " • the is implementing\n",
      " • the is implementing\n",
      " • the is implementing\n",
      " • the is implementing\n",
      " • the is planning to implement\n",
      " • the us has introduced\n",
      " • the is introducing\n",
      " • the is implementing\n",
      " • the [mask] is implementing\n",
      " • the is implementing\n",
      " • the us is implementing\n",
      " • the australian is introducing\n",
      " • the british is implementing\n",
      " • the us is implementing\n",
      " • the is implementing\n",
      " • the is implementing\n",
      " • the united states is implementing\n",
      " • the has announced\n",
      " • government implements\n",
      " • government introduced\n",
      " • world health organization who introduced\n",
      " • government implemented\n",
      " • government introduced\n",
      " • government recently implemented\n",
      " • world health organization introduced\n",
      " • government has introduced\n",
      " • government introduced\n",
      " • government unveiled\n",
      " • government introduced\n",
      " • government implemented\n",
      " • innovative [mask]\n",
      " • government has implemented\n",
      " • innovative \n",
      " • government is implementing\n",
      " • world health organization has implemented\n",
      " • government introduced\n",
      " • government introduces\n",
      " • government is implementing\n",
      " • implements [mask] [mask]\n",
      " • implements \n",
      " • enforces stringent [mask]\n",
      " • implements [mask] [mask]\n",
      " • implements [mask]\n",
      " • develops and implements\n",
      " • enforces [mask] [mask]\n",
      " • implements \n",
      " • implements \n",
      " • implements \n",
      " • enforces [mask]\n",
      " • implemented \n",
      " • implements [mask]\n",
      " • implements \n",
      " • implements \n",
      " • implements \n",
      " • enforces stringent [mask]\n",
      " • implements \n",
      " • implements \n",
      " • implements \n",
      " • introduced policies\n",
      " • established policies\n",
      " • introduced initiatives\n",
      " • has introduced policies\n",
      " • announced initiatives\n",
      " • allocated funding\n",
      " • announced initiatives\n",
      " • implemented measures\n",
      " • launched initiatives\n",
      " • introduced policies\n",
      " • introduced policies\n",
      " • introduced initiatives\n",
      " • decided to implement initiatives\n",
      " • implemented initiatives\n",
      " • announces measures\n",
      " • introduced measures\n",
      " • announced initiatives\n",
      " • enacted legislation\n",
      " • established initiatives\n",
      " • implemented policies\n",
      " • introduced to\n",
      " • introduces to\n",
      " • is implementing to\n",
      " • implements to\n",
      " • enacts to\n",
      " • unveiled aimed at strengthening healthcare\n",
      " • introduces to\n",
      " • implements to\n",
      " • enacted to\n",
      " • has implemented to\n",
      " • implements to\n",
      " • implemented to\n",
      " • implements to\n",
      " • introduces to\n",
      " • has implemented to\n",
      " • introduces to\n",
      " • implements to\n",
      " • introduces to\n",
      " • enacted to\n",
      " • is implementing to\n",
      " • announced improve\n",
      " • has announced improve\n",
      " • introduced improve\n",
      " • is planning improve\n",
      " • implemented improve\n",
      " • introduced improve\n",
      " • implements improve\n",
      " • devised improve\n",
      " • introduced improve\n",
      " • introduced improve\n",
      " • introduced improve\n",
      " • proposed reform\n",
      " • introduced improve\n",
      " • introduced improve\n",
      " • implemented improve\n",
      " • announced improve\n",
      " • introduced improve\n",
      " • implemented improve\n",
      " • implements improve\n",
      " • introduced reform\n",
      " • implemented economic growth\n",
      " • implemented the economy\n",
      " • implements the economy\n",
      " • introduces the economy\n",
      " • introduced small businesses\n",
      " • implements businesses\n",
      " • implemented small businesses\n",
      " • announced small businesses\n",
      " • implemented small businesses\n",
      " • devised the economy\n",
      " • introduced education\n",
      " • implemented small businesses\n",
      " • implemented education\n",
      " • implemented small businesses\n",
      " • implemented education\n",
      " • implemented businesses\n",
      " • announced small businesses\n",
      " • implemented small businesses\n",
      " • announced businesses\n",
      " • introduced small businesses\n",
      "\n",
      "Total generated: 140\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "the             PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "government      PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "new             PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "policies        PMI=   -inf   P(x)=0.150  P(y)=0.300  P(xy)=0.000\n",
      "to              PMI=   -inf   P(x)=0.000  P(y)=0.950  P(xy)=0.000\n",
      "support         PMI=   -inf   P(x)=0.100  P(y)=0.000  P(xy)=0.000\n",
      "healthcare      PMI=   -inf   P(x)=0.150  P(y)=0.000  P(xy)=0.000\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 3   |   Anchor word: 'new'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 29 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     889.23 ms /    20 tokens (   44.46 ms per token,    22.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1108.78 ms /     9 runs   (  123.20 ms per token,     8.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    2000.25 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1371.16 ms /    11 runs   (  124.65 ms per token,     8.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1373.50 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1334.33 ms /    11 runs   (  121.30 ms per token,     8.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1337.10 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1610.17 ms /    13 runs   (  123.86 ms per token,     8.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1613.12 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1321.30 ms /    11 runs   (  120.12 ms per token,     8.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1324.53 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1606.21 ms /    14 runs   (  114.73 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1609.19 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1255.73 ms /    11 runs   (  114.16 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1258.12 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1157.45 ms /    10 runs   (  115.74 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1159.61 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1241.70 ms /    11 runs   (  112.88 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1244.26 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1604.11 ms /    14 runs   (  114.58 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1607.16 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1250.87 ms /    11 runs   (  113.72 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1253.28 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'government', 'announced', '[mask]', 'policies', 'to', 'support', 'healthcare'] ['the', 'biden', 'administration', 'announced', 'comprehensive', 'policies', 'to', 'support', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1235.26 ms /    11 runs   (  112.30 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1237.70 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1367.23 ms /    12 runs   (  113.94 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1369.87 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1492.01 ms /    13 runs   (  114.77 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1495.00 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1570.85 ms /    14 runs   (  112.20 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1573.93 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1363.98 ms /    12 runs   (  113.66 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1366.67 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1259.12 ms /    11 runs   (  114.47 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1261.51 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1149.32 ms /    10 runs   (  114.93 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1151.54 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1144.82 ms /    10 runs   (  114.48 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1147.02 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced [MASK] policies to support healthLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1264.88 ms /    11 runs   (  114.99 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1267.34 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 29 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     455.82 ms /    20 tokens (   22.79 ms per token,    43.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1014.23 ms /     9 runs   (  112.69 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1472.35 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1367.57 ms /    12 runs   (  113.96 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1370.15 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1007.85 ms /     9 runs   (  111.98 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1009.85 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1112.33 ms /    10 runs   (  111.23 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1114.51 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1042.57 ms /     9 runs   (  115.84 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1044.54 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1342.44 ms /    12 runs   (  111.87 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1345.01 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1110.67 ms /    10 runs   (  111.07 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1112.85 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1123.60 ms /    10 runs   (  112.36 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1125.81 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1110.23 ms /    10 runs   (  111.02 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1112.46 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1105.12 ms /    10 runs   (  110.51 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1107.29 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1100.49 ms /    10 runs   (  110.05 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1102.65 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1136.58 ms /    10 runs   (  113.66 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1139.09 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1096.70 ms /    10 runs   (  109.67 ms per token,     9.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1099.07 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1094.42 ms /    10 runs   (  109.44 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1096.60 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1101.03 ms /    10 runs   (  110.10 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1103.23 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1103.24 ms /    10 runs   (  110.32 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1105.61 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1099.08 ms /    10 runs   (  109.91 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1101.31 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1089.68 ms /    10 runs   (  108.97 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1091.86 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1093.98 ms /    10 runs   (  109.40 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1096.22 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced [MASK] policies to support healthcare:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1111.65 ms /    10 runs   (  111.16 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1113.86 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 30 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     573.94 ms /    19 tokens (   30.21 ms per token,    33.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1414.41 ms /    10 runs   (  141.44 ms per token,     7.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1991.33 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1209.15 ms /     9 runs   (  134.35 ms per token,     7.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1212.50 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1611.81 ms /    12 runs   (  134.32 ms per token,     7.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1614.71 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1605.41 ms /    12 runs   (  133.78 ms per token,     7.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1608.15 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1496.05 ms /    11 runs   (  136.00 ms per token,     7.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1498.71 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2063.05 ms /    15 runs   (  137.54 ms per token,     7.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    2066.56 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1937.76 ms /    15 runs   (  129.18 ms per token,     7.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1941.90 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1531.36 ms /    12 runs   (  127.61 ms per token,     7.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1534.15 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1637.93 ms /    13 runs   (  125.99 ms per token,     7.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1640.87 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1272.87 ms /    10 runs   (  127.29 ms per token,     7.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1275.22 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1124.44 ms /     9 runs   (  124.94 ms per token,     8.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1126.54 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1492.96 ms /    12 runs   (  124.41 ms per token,     8.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1495.62 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1117.31 ms /     9 runs   (  124.15 ms per token,     8.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1119.34 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1467.40 ms /    12 runs   (  122.28 ms per token,     8.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1470.08 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2098.55 ms /    17 runs   (  123.44 ms per token,     8.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    2102.40 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1108.75 ms /     9 runs   (  123.19 ms per token,     8.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1110.89 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1445.31 ms /    12 runs   (  120.44 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1448.03 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1111.26 ms /     9 runs   (  123.47 ms per token,     8.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1113.34 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1434.09 ms /    12 runs   (  119.51 ms per token,     8.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1436.89 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government [MASK] [MASK] policies to support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1421.10 ms /    12 runs   (  118.43 ms per token,     8.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1423.77 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 31 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     572.72 ms /    18 tokens (   31.82 ms per token,    31.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1189.03 ms /    10 runs   (  118.90 ms per token,     8.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1764.40 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', '[mask]', '[mask]', 'to', 'support', 'healthcare'] ['the', 'government', 'announced', 'a', 'comprehensive', 'plan', 'to', 'improve', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1174.43 ms /    10 runs   (  117.44 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1176.75 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     935.44 ms /     8 runs   (  116.93 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =     937.76 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', '[mask]', '[mask]', 'to', 'support', 'healthcare'] ['the', 'government', 'announced', 'subsidies', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1443.97 ms /    12 runs   (  120.33 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1447.12 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1320.77 ms /    11 runs   (  120.07 ms per token,     8.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1323.49 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1581.80 ms /    13 runs   (  121.68 ms per token,     8.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1584.89 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1342.64 ms /    11 runs   (  122.06 ms per token,     8.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1345.31 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1425.06 ms /    11 runs   (  129.55 ms per token,     7.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1427.83 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1341.17 ms /    11 runs   (  121.92 ms per token,     8.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1343.81 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1326.13 ms /    11 runs   (  120.56 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1328.73 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1362.30 ms /    11 runs   (  123.85 ms per token,     8.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1365.69 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1357.75 ms /    11 runs   (  123.43 ms per token,     8.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1360.43 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1199.94 ms /    10 runs   (  119.99 ms per token,     8.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1202.36 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1082.34 ms /     9 runs   (  120.26 ms per token,     8.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1084.53 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1255.98 ms /    11 runs   (  114.18 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1258.76 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1817.15 ms /    16 runs   (  113.57 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1821.30 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1246.10 ms /    11 runs   (  113.28 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1248.59 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1321.13 ms /    11 runs   (  120.10 ms per token,     8.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1323.55 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1440.53 ms /    12 runs   (  120.04 ms per token,     8.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1443.45 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1347.68 ms /    11 runs   (  122.52 ms per token,     8.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1350.83 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 35 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =    4443.11 ms /    14 tokens (  317.36 ms per token,     3.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1336.44 ms /     9 runs   (  148.49 ms per token,     6.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    5783.50 ms /    23 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1532.34 ms /    11 runs   (  139.30 ms per token,     7.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1535.13 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1398.16 ms /    11 runs   (  127.11 ms per token,     7.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1400.97 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', '[mask]', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'comprehensive', 'policies', 'to', 'bolster', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1318.84 ms /    10 runs   (  131.88 ms per token,     7.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1321.21 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', '[mask]', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'improve', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1523.02 ms /    12 runs   (  126.92 ms per token,     7.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1525.72 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1162.91 ms /    10 runs   (  116.29 ms per token,     8.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1165.20 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1189.02 ms /    10 runs   (  118.90 ms per token,     8.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1191.60 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1398.70 ms /    12 runs   (  116.56 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1401.42 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1582.28 ms /    14 runs   (  113.02 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1585.39 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1496.18 ms /    13 runs   (  115.09 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1499.05 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1170.88 ms /    10 runs   (  117.09 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1173.12 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1328.99 ms /    12 runs   (  110.75 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1331.88 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1375.41 ms /    12 runs   (  114.62 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1378.13 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1266.82 ms /    11 runs   (  115.17 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1269.36 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1323.49 ms /    12 runs   (  110.29 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1326.49 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1143.85 ms /    10 runs   (  114.39 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1146.33 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1489.70 ms /    13 runs   (  114.59 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1492.64 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1211.49 ms /    11 runs   (  110.14 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1213.95 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1141.52 ms /    10 runs   (  114.15 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1143.74 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1369.27 ms /    12 runs   (  114.11 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1371.95 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 36 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     512.27 ms /    13 tokens (   39.41 ms per token,    25.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1012.94 ms /     9 runs   (  112.55 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1527.59 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1155.15 ms /    10 runs   (  115.52 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1157.56 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1150.85 ms /    10 runs   (  115.08 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1153.25 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1238.16 ms /    11 runs   (  112.56 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1240.70 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1113.89 ms /    10 runs   (  111.39 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1116.20 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1106.94 ms /    10 runs   (  110.69 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1109.26 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1120.72 ms /    10 runs   (  112.07 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1123.04 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1236.60 ms /    11 runs   (  112.42 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1239.10 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1117.31 ms /    10 runs   (  111.73 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1119.75 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1107.05 ms /    10 runs   (  110.70 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1109.67 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1122.71 ms /    10 runs   (  112.27 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1125.03 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1140.82 ms /    10 runs   (  114.08 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1143.16 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1127.48 ms /    10 runs   (  112.75 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1129.82 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1122.06 ms /    10 runs   (  112.21 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1124.35 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1147.15 ms /    10 runs   (  114.72 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1149.49 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1121.93 ms /    10 runs   (  112.19 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1124.44 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1134.25 ms /    10 runs   (  113.42 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1137.17 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1110.07 ms /    10 runs   (  111.01 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1112.38 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1217.28 ms /    11 runs   (  110.66 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1220.01 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1115.36 ms /    10 runs   (  111.54 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1117.68 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 37 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     485.24 ms /    12 tokens (   40.44 ms per token,    24.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1145.09 ms /    10 runs   (  114.51 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1632.84 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1300.36 ms /    11 runs   (  118.21 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1302.82 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1287.80 ms /    11 runs   (  117.07 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1290.39 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1037.46 ms /     9 runs   (  115.27 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1039.77 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1284.86 ms /    11 runs   (  116.81 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1287.38 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1293.21 ms /    11 runs   (  117.56 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1295.70 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1547.66 ms /    11 runs   (  140.70 ms per token,     7.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1550.26 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1229.73 ms /    10 runs   (  122.97 ms per token,     8.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1232.00 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1345.56 ms /    11 runs   (  122.32 ms per token,     8.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1348.05 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1317.17 ms /    11 runs   (  119.74 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1319.78 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1356.42 ms /    11 runs   (  123.31 ms per token,     8.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1359.14 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1716.34 ms /    14 runs   (  122.60 ms per token,     8.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1719.46 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1287.59 ms /    11 runs   (  117.05 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1290.11 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1146.15 ms /    10 runs   (  114.61 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1148.44 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1272.46 ms /    11 runs   (  115.68 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1274.94 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1182.61 ms /    10 runs   (  118.26 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1185.35 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1045.02 ms /     9 runs   (  116.11 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1047.33 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1286.58 ms /    11 runs   (  116.96 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1289.13 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1445.44 ms /    12 runs   (  120.45 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1448.25 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1138.03 ms /    10 runs   (  113.80 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1140.77 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • the new\n",
      " • the chinese new\n",
      " • the australian new\n",
      " • the australian a series of\n",
      " • the australian comprehensive\n",
      " • the indonesian a series of\n",
      " • the chinese comprehensive\n",
      " • the comprehensive\n",
      " • the italian new\n",
      " • the us comprehensive\n",
      " •  \n",
      " • the british comprehensive\n",
      " • the brazilian several\n",
      " • the australian a range of\n",
      " • the iranian a series of\n",
      " • the united states comprehensive\n",
      " • the british comprehensive\n",
      " • the comprehensive\n",
      " • the several\n",
      " • the british comprehensive\n",
      " • government comprehensive\n",
      " • world health organization comprehensive\n",
      " • government comprehensive\n",
      " • government comprehensive\n",
      " • government comprehensive\n",
      " • world health organization comprehensive\n",
      " • government comprehensive\n",
      " • government comprehensive\n",
      " • government comprehensive\n",
      " • government comprehensive\n",
      " • government comprehensive\n",
      " • government comprehensive\n",
      " • government comprehensive\n",
      " • government comprehensive\n",
      " • government comprehensive\n",
      " • government new\n",
      " • government comprehensive\n",
      " • government comprehensive\n",
      " • government comprehensive\n",
      " • government comprehensive\n",
      " • implements [mask]\n",
      " • implements \n",
      " • implements [mask]\n",
      " • formulates and implements\n",
      " • implements [mask]\n",
      " • implemented [mask] [mask]\n",
      " • enacts [mask] [mask]\n",
      " • implements and adjusts\n",
      " • should implement and prioritize\n",
      " • enforces \n",
      " • implements \n",
      " • implements and strengthens\n",
      " • implements \n",
      " • formulates and implements\n",
      " • enforces [mask] [mask]\n",
      " • implements \n",
      " • implements [mask]\n",
      " • implemented \n",
      " • adopts [mask]\n",
      " • implements [mask]\n",
      " •  \n",
      " • an initiative\n",
      " •  \n",
      " • new policies and funds\n",
      " • an investment initiative\n",
      " • a significant increase in funding\n",
      " • a new initiative\n",
      " • new funding initiatives\n",
      " • a comprehensive plan\n",
      " • a new initiative\n",
      " • a comprehensive package\n",
      " • a new initiative\n",
      " • new initiatives\n",
      " • new initiatives\n",
      " • a comprehensive plan\n",
      " • a new initiative offering financial assistance\n",
      " • a comprehensive plan\n",
      " • a new initiative\n",
      " • a financial assistance package\n",
      " • a major initiative\n",
      " • new to\n",
      " • new to strengthen healthcare\n",
      " •  \n",
      " •  \n",
      " • new to increase\n",
      " • new to\n",
      " • comprehensive to\n",
      " • comprehensive to increase\n",
      " • new emphasizing their\n",
      " • new emphasizing\n",
      " • new to\n",
      " • new to strengthen\n",
      " • comprehensive to expand\n",
      " • new to further\n",
      " • new to strengthen\n",
      " • new to\n",
      " • new to strengthen their\n",
      " • new that significantly\n",
      " • new to\n",
      " • comprehensive to bolster healthcare\n",
      " • comprehensive transform\n",
      " • new improve\n",
      " • comprehensive improve\n",
      " • comprehensive revolutionize\n",
      " • comprehensive transform\n",
      " • comprehensive improve\n",
      " • progressive improve\n",
      " • comprehensive revolutionize\n",
      " • comprehensive improve\n",
      " • comprehensive transform\n",
      " • comprehensive improve\n",
      " • comprehensive improve\n",
      " • comprehensive improve\n",
      " • comprehensive transform\n",
      " • comprehensive transform\n",
      " • comprehensive improve\n",
      " • comprehensive improve\n",
      " • comprehensive improve\n",
      " • comprehensive revolutionize\n",
      " • progressive improve\n",
      " • economic small businesses\n",
      " • new small businesses\n",
      " • progressive the environment\n",
      " • specific vulnerable\n",
      " • comprehensive the economy\n",
      " • clear small businesses\n",
      " • clear small businesses\n",
      " • bold businesses\n",
      " • comprehensive small businesses\n",
      " • comprehensive economic growth\n",
      " • new small businesses\n",
      " • progressive lowincome communities\n",
      " • new the environment\n",
      " • comprehensive businesses\n",
      " • comprehensive small businesses\n",
      " • specific businesses\n",
      " • progressive healthcare\n",
      " • comprehensive small businesses\n",
      " • new economic small businesses\n",
      " • aggressive businesses\n",
      "\n",
      "Total generated: 140\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "the             PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "government      PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "announced       PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "policies        PMI=   -inf   P(x)=0.200  P(y)=0.000  P(xy)=0.000\n",
      "to              PMI=  0.252   P(x)=0.700  P(y)=0.300  P(xy)=0.250\n",
      "support         PMI=   -inf   P(x)=0.050  P(y)=0.000  P(xy)=0.000\n",
      "healthcare      PMI=   -inf   P(x)=0.150  P(y)=0.050  P(xy)=0.000\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 4   |   Anchor word: 'policies'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 29 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     594.39 ms /    20 tokens (   29.72 ms per token,    33.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1287.30 ms /    11 runs   (  117.03 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1884.68 ms /    31 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1192.69 ms /    10 runs   (  119.27 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1194.93 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1192.01 ms /    10 runs   (  119.20 ms per token,     8.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1194.32 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1170.59 ms /    10 runs   (  117.06 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1172.89 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1145.71 ms /    10 runs   (  114.57 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1147.99 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1091.05 ms /    10 runs   (  109.10 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1093.24 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1223.08 ms /    11 runs   (  111.19 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1225.72 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1154.82 ms /    10 runs   (  115.48 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1157.29 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1244.18 ms /    11 runs   (  113.11 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1246.57 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1124.88 ms /    10 runs   (  112.49 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1127.05 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1287.29 ms /    11 runs   (  117.03 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1289.73 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1573.27 ms /    14 runs   (  112.38 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1576.32 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1145.27 ms /    10 runs   (  114.53 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1147.58 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1135.75 ms /    10 runs   (  113.58 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1137.95 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1224.40 ms /    11 runs   (  111.31 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1226.78 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1134.70 ms /    10 runs   (  113.47 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1136.92 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1284.38 ms /    11 runs   (  116.76 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1287.44 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1152.25 ms /    10 runs   (  115.23 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1154.50 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1193.08 ms /    11 runs   (  108.46 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1195.46 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1131.32 ms /    10 runs   (  113.13 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1133.51 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:   0%| Llama.generate: 29 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     477.57 ms /    20 tokens (   23.88 ms per token,    41.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1098.05 ms /     9 runs   (  122.01 ms per token,     8.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1577.86 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:   5%| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1197.28 ms /    10 runs   (  119.73 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1199.51 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  10%| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1287.69 ms /    10 runs   (  128.77 ms per token,     7.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1289.91 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  15%|▏Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1266.45 ms /    10 runs   (  126.65 ms per token,     7.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1268.67 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  20%|▏Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1209.92 ms /    10 runs   (  120.99 ms per token,     8.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1212.19 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  25%|▎Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1267.45 ms /    10 runs   (  126.74 ms per token,     7.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1269.63 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  30%|▎Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1262.93 ms /    10 runs   (  126.29 ms per token,     7.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1265.13 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  35%|▎Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1241.45 ms /    10 runs   (  124.15 ms per token,     8.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1243.63 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  40%|▍Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1259.82 ms /    10 runs   (  125.98 ms per token,     7.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1261.99 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  45%|▍Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1273.68 ms /    10 runs   (  127.37 ms per token,     7.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1275.90 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  50%|▌Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1255.09 ms /    10 runs   (  125.51 ms per token,     7.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1257.34 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  55%|▌Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1274.00 ms /    10 runs   (  127.40 ms per token,     7.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1276.32 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  60%|▌Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1254.43 ms /    10 runs   (  125.44 ms per token,     7.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1256.88 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  65%|▋Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1418.51 ms /    10 runs   (  141.85 ms per token,     7.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1420.79 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  70%|▋Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1256.84 ms /    10 runs   (  125.68 ms per token,     7.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1259.35 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  75%|▊Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1267.34 ms /    10 runs   (  126.73 ms per token,     7.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1269.61 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  80%|▊Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1267.34 ms /    10 runs   (  126.73 ms per token,     7.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1269.60 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  85%|▊Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1161.55 ms /    10 runs   (  116.15 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1163.80 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  90%|▉Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1212.89 ms /    10 runs   (  121.29 ms per token,     8.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1215.13 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new [MASK] to support healthcare:  95%|▉Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1177.11 ms /    10 runs   (  117.71 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1179.43 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:   0%|Llama.generate: 30 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     907.47 ms /    19 tokens (   47.76 ms per token,    20.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1128.27 ms /    10 runs   (  112.83 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    2038.37 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:   5%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1040.26 ms /     9 runs   (  115.58 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1042.28 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  10%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1147.60 ms /    10 runs   (  114.76 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1149.79 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  15%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1158.56 ms /    10 runs   (  115.86 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1161.28 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  20%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1340.22 ms /    12 runs   (  111.69 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1342.87 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  25%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1241.08 ms /    11 runs   (  112.83 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1244.22 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  30%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     970.68 ms /     9 runs   (  107.85 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =     972.99 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  35%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1101.08 ms /    10 runs   (  110.11 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1103.31 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  40%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1102.81 ms /    10 runs   (  110.28 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1105.06 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  45%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1118.90 ms /    10 runs   (  111.89 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1121.11 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  50%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     949.65 ms /     9 runs   (  105.52 ms per token,     9.48 tokens per second)\n",
      "llama_perf_context_print:       total time =     951.65 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  55%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1224.51 ms /    11 runs   (  111.32 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1227.21 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  60%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1112.52 ms /    10 runs   (  111.25 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1114.80 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  65%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     988.17 ms /     9 runs   (  109.80 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =     990.24 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  70%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1084.74 ms /    10 runs   (  108.47 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1086.99 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  75%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1105.14 ms /    10 runs   (  110.51 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1107.39 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  80%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1105.91 ms /    10 runs   (  110.59 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1108.21 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  85%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1182.95 ms /    11 runs   (  107.54 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1185.39 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  90%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     992.12 ms /     9 runs   (  110.24 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =     994.12 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] new [MASK] to support healthcare:  95%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1114.33 ms /    10 runs   (  111.43 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1116.89 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 31 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     586.60 ms /    18 tokens (   32.59 ms per token,    30.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1242.40 ms /    10 runs   (  124.24 ms per token,     8.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1831.53 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2140.11 ms /    16 runs   (  133.76 ms per token,     7.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    2143.69 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1842.25 ms /    14 runs   (  131.59 ms per token,     7.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1845.38 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1335.92 ms /    10 runs   (  133.59 ms per token,     7.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1338.16 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1607.92 ms /    12 runs   (  133.99 ms per token,     7.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1610.59 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1403.16 ms /    11 runs   (  127.56 ms per token,     7.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1405.62 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1470.54 ms /    11 runs   (  133.69 ms per token,     7.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1473.03 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1732.25 ms /    13 runs   (  133.25 ms per token,     7.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1735.16 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1470.10 ms /    11 runs   (  133.65 ms per token,     7.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1472.58 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1203.15 ms /     9 runs   (  133.68 ms per token,     7.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1205.15 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', '[mask]', '[mask]', 'to', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'improve', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1454.09 ms /    11 runs   (  132.19 ms per token,     7.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1456.59 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1470.12 ms /    11 runs   (  133.65 ms per token,     7.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1472.58 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1352.03 ms /    10 runs   (  135.20 ms per token,     7.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1354.28 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1629.36 ms /    12 runs   (  135.78 ms per token,     7.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1632.47 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1213.64 ms /     9 runs   (  134.85 ms per token,     7.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1215.95 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1488.84 ms /    11 runs   (  135.35 ms per token,     7.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1491.30 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1292.22 ms /    10 runs   (  129.22 ms per token,     7.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1294.50 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', '[mask]', '[mask]', 'to', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'measures', 'to', 'improve', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1634.72 ms /    12 runs   (  136.23 ms per token,     7.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1637.38 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1716.09 ms /    11 runs   (  156.01 ms per token,     6.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1718.62 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] [MASK] to support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', '[mask]', '[mask]', 'to', 'support', 'healthcare'] ['the', 'government', 'announced', 'an', 'increase', 'in', 'funding', 'for', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1345.27 ms /    11 runs   (  122.30 ms per token,     8.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1347.89 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 32 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     408.66 ms /    17 tokens (   24.04 ms per token,    41.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1203.48 ms /    10 runs   (  120.35 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1614.84 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1345.47 ms /    11 runs   (  122.32 ms per token,     8.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1348.08 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1548.37 ms /    13 runs   (  119.11 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1551.33 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1209.25 ms /    10 runs   (  120.92 ms per token,     8.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1211.72 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', 'new', '[mask]', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'improve', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1159.06 ms /    10 runs   (  115.91 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1161.29 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1502.06 ms /    13 runs   (  115.54 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1504.95 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1269.98 ms /    11 runs   (  115.45 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1272.44 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1460.56 ms /    13 runs   (  112.35 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1463.74 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1376.17 ms /    12 runs   (  114.68 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1379.00 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', 'new', '[mask]', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'increase', 'funding', 'for', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1257.82 ms /    11 runs   (  114.35 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1260.25 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1250.37 ms /    11 runs   (  113.67 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1252.82 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', 'new', '[mask]', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'increase', 'healthcare', 'funding']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1264.19 ms /    11 runs   (  114.93 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1266.77 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1257.05 ms /    11 runs   (  114.28 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1259.67 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1248.18 ms /    11 runs   (  113.47 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1250.70 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1248.65 ms /    11 runs   (  113.51 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1251.11 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1358.38 ms /    12 runs   (  113.20 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1361.40 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1464.01 ms /    13 runs   (  112.62 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1467.13 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1239.58 ms /    11 runs   (  112.69 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1242.05 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1487.64 ms /    13 runs   (  114.43 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1490.51 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1238.13 ms /    11 runs   (  112.56 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1240.60 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:   0Llama.generate: 36 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     415.02 ms /    13 tokens (   31.92 ms per token,    31.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1152.74 ms /     9 runs   (  128.08 ms per token,     7.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1570.46 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:   5Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1170.34 ms /    10 runs   (  117.03 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1172.83 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  10Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1144.49 ms /    10 runs   (  114.45 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1147.06 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  15Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1128.50 ms /    10 runs   (  112.85 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1130.79 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  20Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1142.30 ms /    10 runs   (  114.23 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1144.56 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  25Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1142.12 ms /    10 runs   (  114.21 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1144.42 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  30Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1102.83 ms /    10 runs   (  110.28 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1105.13 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  35Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1153.67 ms /    10 runs   (  115.37 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1155.95 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  40Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1127.69 ms /    10 runs   (  112.77 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1129.99 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  45Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1137.86 ms /    10 runs   (  113.79 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1140.19 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  50Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1137.31 ms /    10 runs   (  113.73 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1139.64 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  55Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1147.94 ms /    10 runs   (  114.79 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1150.40 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  60Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1133.09 ms /    10 runs   (  113.31 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1135.41 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  65Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1101.87 ms /    10 runs   (  110.19 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1104.13 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  70Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1138.61 ms /    10 runs   (  113.86 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1140.87 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  75Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1143.66 ms /    10 runs   (  114.37 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1146.10 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  80Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1132.82 ms /    10 runs   (  113.28 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1135.14 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  85Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1144.61 ms /    10 runs   (  114.46 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1146.87 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  90Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1150.47 ms /    10 runs   (  115.05 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1152.73 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  95Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1135.03 ms /    10 runs   (  113.50 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1137.28 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:   0%| Llama.generate: 37 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     440.00 ms /    12 tokens (   36.67 ms per token,    27.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1324.05 ms /    12 runs   (  110.34 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1766.91 ms /    24 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:   5%| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1098.21 ms /    10 runs   (  109.82 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1100.39 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  10%| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1118.72 ms /    10 runs   (  111.87 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1120.91 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  15%|▏Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1097.59 ms /    10 runs   (  109.76 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1099.78 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  20%|▏Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1235.57 ms /    11 runs   (  112.32 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1237.96 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  25%|▎Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1108.67 ms /    10 runs   (  110.87 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1110.85 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  30%|▎Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1217.17 ms /    11 runs   (  110.65 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1219.56 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  35%|▎Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1555.74 ms /    12 runs   (  129.64 ms per token,     7.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1558.40 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  40%|▍Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1244.21 ms /    11 runs   (  113.11 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1246.91 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  45%|▍Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1101.61 ms /    10 runs   (  110.16 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1103.86 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  50%|▌Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1331.42 ms /    12 runs   (  110.95 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1334.07 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  55%|▌Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1128.71 ms /    10 runs   (  112.87 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1130.93 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  60%|▌Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1118.11 ms /    10 runs   (  111.81 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1120.33 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  65%|▋Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1096.29 ms /    10 runs   (  109.63 ms per token,     9.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1098.48 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  70%|▋Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1128.26 ms /    10 runs   (  112.83 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1130.56 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  75%|▊Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1235.76 ms /    11 runs   (  112.34 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1238.20 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  80%|▊Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1198.02 ms /    11 runs   (  108.91 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1200.40 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  85%|▊Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1204.60 ms /    11 runs   (  109.51 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1206.98 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  90%|▉Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1238.05 ms /    11 runs   (  112.55 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1241.09 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  95%|▉Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1217.18 ms /    11 runs   (  110.65 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1219.61 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • the united states policies\n",
      " • the taxes\n",
      " • the initiatives\n",
      " • the us initiatives\n",
      " • the policies\n",
      " • the policies\n",
      " • the us initiatives\n",
      " • the policies\n",
      " • the british policies\n",
      " • the regulations\n",
      " • the australian regulations\n",
      " • the us policies\n",
      " • the policies\n",
      " • the regulations\n",
      " • the british initiatives\n",
      " • the measures\n",
      " • the federal regulations\n",
      " • the measures\n",
      " • the canadian initiatives\n",
      " • the regulations\n",
      " • government regulations\n",
      " • company initiatives\n",
      " • government programs\n",
      " • government initiatives\n",
      " • government initiatives\n",
      " • company initiatives\n",
      " • government regulations\n",
      " • government initiatives\n",
      " • government policies\n",
      " • government initiatives\n",
      " • government policies\n",
      " • government initiatives\n",
      " • government initiatives\n",
      " • government programs\n",
      " • company programs\n",
      " • government initiatives\n",
      " • government initiatives\n",
      " • government initiatives\n",
      " • government initiatives\n",
      " • government measures\n",
      " • is introducing initiatives\n",
      " • implemented initiatives\n",
      " • implements initiatives\n",
      " • introduced policies\n",
      " • enacted legislation\n",
      " • announces initiatives\n",
      " • announced initiatives\n",
      " • implements regulations\n",
      " • launched initiatives\n",
      " • introduced initiatives\n",
      " • implements policies\n",
      " • has announced measures\n",
      " • implemented measures\n",
      " • implemented policies\n",
      " • expanded initiatives\n",
      " • introduced initiatives\n",
      " • implemented policies\n",
      " • will introduce initiatives\n",
      " • introduced policies\n",
      " • introduced measures\n",
      " • a comprehensive plan\n",
      " • a comprehensive package of financial and policy measures\n",
      " • new tax incentives and funding\n",
      " • additional funding\n",
      " • a substantial financial increase\n",
      " • new tax credits\n",
      " • an ambitious plan\n",
      " • a significant increase in funding\n",
      " • a financial package\n",
      " •  \n",
      " • a new initiative\n",
      " • a comprehensive plan\n",
      " • additional funding\n",
      " • new programs and funding\n",
      " • new policies\n",
      " • a significant investment\n",
      " •  \n",
      " • new policies and funding\n",
      " •  \n",
      " • a substantial investment\n",
      " • policies to increase healthcare\n",
      " • policies to strengthen healthcare\n",
      " • policies to increase financial\n",
      " •  \n",
      " • policies to increase healthcare\n",
      " • healthcare initiatives to boost\n",
      " • policies to increase healthcare\n",
      " • policies to increase financial\n",
      " •  \n",
      " • initiatives to strengthen healthcare\n",
      " •  \n",
      " • initiatives to strengthen healthcare\n",
      " • tax credits to\n",
      " • policies to enhance healthcare\n",
      " • initiatives to strengthen healthcare\n",
      " • healthcare initiatives to provide increased\n",
      " • policies to increase financial\n",
      " • policies to increase healthcare\n",
      " • policies to increase financial\n",
      " • policies to increase healthcare\n",
      " • regulations improve\n",
      " • regulations improve\n",
      " • policies improve\n",
      " • regulations improve\n",
      " • policies improve\n",
      " • regulations improve\n",
      " • policies improve\n",
      " • policies improve\n",
      " • policies improve\n",
      " • policies improve\n",
      " • regulations improve\n",
      " • regulations improve\n",
      " • initiatives improve\n",
      " • regulations improve\n",
      " • policies improve\n",
      " • policies improve\n",
      " • regulations improve\n",
      " • policies improve\n",
      " • regulations improve\n",
      " • policies improve\n",
      " • economic stimulus measures businesses\n",
      " • measures businesses\n",
      " • policies citizens\n",
      " • policies citizens\n",
      " • initiatives the economy\n",
      " • policies businesses\n",
      " • policies small businesses\n",
      " • incentives small businesses\n",
      " • initiatives small businesses\n",
      " • measures businesses\n",
      " • incentives entrepreneurs\n",
      " • measures businesses\n",
      " • policies healthcare\n",
      " • policies businesses\n",
      " • policies businesses\n",
      " • incentives businesses\n",
      " • policies small businesses\n",
      " • policies small businesses\n",
      " • policies small businesses\n",
      " • policies small businesses\n",
      "\n",
      "Total generated: 140\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "the             PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "government      PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "announced       PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "new             PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "to              PMI=   -inf   P(x)=0.550  P(y)=0.000  P(xy)=0.000\n",
      "support         PMI=   -inf   P(x)=0.500  P(y)=0.000  P(xy)=0.000\n",
      "healthcare      PMI=  0.862   P(x)=0.550  P(y)=0.050  P(xy)=0.050\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 5   |   Anchor word: 'to'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 29 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     520.97 ms /    20 tokens (   26.05 ms per token,    38.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1266.17 ms /    11 runs   (  115.11 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1789.98 ms /    31 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1339.18 ms /    11 runs   (  121.74 ms per token,     8.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1341.60 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1608.35 ms /    14 runs   (  114.88 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1611.73 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1216.21 ms /    10 runs   (  121.62 ms per token,     8.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1218.68 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1317.85 ms /    11 runs   (  119.80 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1320.29 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'government', 'announced', 'new', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'australian', 'government', 'announced', 'new', 'policies', 'to', 'strengthen', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1143.54 ms /    10 runs   (  114.35 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1145.74 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1333.40 ms /    11 runs   (  121.22 ms per token,     8.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1335.81 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'government', 'announced', 'new', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'federal', 'government', 'announced', 'new', 'policies', 'to', 'strengthen', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1286.15 ms /    11 runs   (  116.92 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1288.56 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'government', 'announced', 'new', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'biden', 'administration', 'announced', 'new', 'policies', 'to', 'support', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1161.47 ms /    10 runs   (  116.15 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1163.71 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1166.85 ms /    10 runs   (  116.69 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1169.07 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'government', 'announced', 'new', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'strengthen', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1291.63 ms /    11 runs   (  117.42 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1294.07 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1331.65 ms /    12 runs   (  110.97 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1334.43 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'government', 'announced', 'new', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'united', 'states', 'government', 'announced', 'new', 'policies', 'to', 'strengthen', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1126.47 ms /    10 runs   (  112.65 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1128.93 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1473.45 ms /    13 runs   (  113.34 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1476.34 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1519.06 ms /    14 runs   (  108.50 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1522.14 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1113.04 ms /    10 runs   (  111.30 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1115.21 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'government', 'announced', 'new', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'strengthen', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1207.54 ms /    11 runs   (  109.78 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1209.99 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1203.39 ms /    11 runs   (  109.40 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1205.77 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1095.43 ms /    10 runs   (  109.54 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1097.67 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1189.32 ms /    11 runs   (  108.12 ms per token,     9.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1191.71 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 29 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     419.09 ms /    20 tokens (   20.95 ms per token,    47.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1060.66 ms /     9 runs   (  117.85 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1482.05 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1156.51 ms /    10 runs   (  115.65 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1158.69 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1153.87 ms /    10 runs   (  115.39 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1156.15 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', '[mask]', 'announced', 'new', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'strongly', 'supporting', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1041.13 ms /     9 runs   (  115.68 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1043.16 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', '[mask]', 'announced', 'new', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'supporting', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1509.34 ms /    13 runs   (  116.10 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1512.39 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1157.17 ms /    10 runs   (  115.72 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1159.56 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1054.29 ms /     9 runs   (  117.14 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1056.27 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', '[mask]', 'announced', 'new', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'supporting', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1436.99 ms /    12 runs   (  119.75 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1439.60 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', '[mask]', 'announced', 'new', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'world', 'health', 'organization', 'announced', 'new', 'policies', 'strongly', 'supporting', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1394.41 ms /    12 runs   (  116.20 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1397.09 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1181.17 ms /    10 runs   (  118.12 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1183.39 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1164.87 ms /    10 runs   (  116.49 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1167.18 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', '[mask]', 'announced', 'new', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'strongly', 'supporting', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1399.17 ms /    12 runs   (  116.60 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1401.77 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1164.61 ms /    10 runs   (  116.46 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1166.79 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1310.27 ms /     9 runs   (  145.59 ms per token,     6.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1312.40 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', '[mask]', 'announced', 'new', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'supporting', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1227.38 ms /    10 runs   (  122.74 ms per token,     8.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1229.58 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', '[mask]', 'announced', 'new', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'strengthen', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1521.39 ms /    12 runs   (  126.78 ms per token,     7.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1524.00 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1392.34 ms /    12 runs   (  116.03 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1395.07 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1136.82 ms /    10 runs   (  113.68 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1139.07 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1135.14 ms /    10 runs   (  113.51 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1137.36 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies [MASK] support healthcare: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1135.81 ms /    10 runs   (  113.58 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1138.01 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', '[mask]', 'announced', 'new', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'strengthen', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 30 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     465.10 ms /    19 tokens (   24.48 ms per token,    40.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =     913.40 ms /     8 runs   (  114.18 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1380.65 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1227.68 ms /    10 runs   (  122.77 ms per token,     8.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1229.90 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1358.96 ms /    11 runs   (  123.54 ms per token,     8.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1361.62 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1227.37 ms /    10 runs   (  122.74 ms per token,     8.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1229.80 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1236.28 ms /    10 runs   (  123.63 ms per token,     8.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1238.56 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1522.55 ms /    13 runs   (  117.12 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1525.42 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1247.37 ms /    11 runs   (  113.40 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1249.81 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1120.27 ms /    10 runs   (  112.03 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1122.80 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1105.88 ms /    10 runs   (  110.59 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1108.25 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1090.71 ms /    10 runs   (  109.07 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    1092.93 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1148.23 ms /    10 runs   (  114.82 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1150.52 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1147.16 ms /    10 runs   (  114.72 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1149.43 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1150.90 ms /    10 runs   (  115.09 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1153.23 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1444.01 ms /    13 runs   (  111.08 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1446.83 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     988.05 ms /     9 runs   (  109.78 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =     990.06 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     985.08 ms /     9 runs   (  109.45 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =     987.06 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1604.46 ms /    15 runs   (  106.96 ms per token,     9.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1607.90 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1419.51 ms /    13 runs   (  109.19 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1422.41 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1155.16 ms /    10 runs   (  115.52 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1157.40 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies [MASK] support healthcare:Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1201.48 ms /    11 runs   (  109.23 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1203.94 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 31 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     959.17 ms /    18 tokens (   53.29 ms per token,    18.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1033.15 ms /     9 runs   (  114.79 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1995.00 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', '[mask]', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'improve', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1201.38 ms /    10 runs   (  120.14 ms per token,     8.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1203.71 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1396.64 ms /    12 runs   (  116.39 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1399.53 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1582.25 ms /    13 runs   (  121.71 ms per token,     8.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1585.49 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1222.32 ms /    10 runs   (  122.23 ms per token,     8.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1224.70 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1161.85 ms /    10 runs   (  116.19 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1164.15 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', '[mask]', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'comprehensive', 'policies', 'to', 'strengthen', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1433.63 ms /    12 runs   (  119.47 ms per token,     8.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1436.45 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1191.05 ms /    10 runs   (  119.10 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1193.41 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1174.12 ms /    10 runs   (  117.41 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1176.46 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1384.74 ms /    12 runs   (  115.39 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1387.50 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1339.09 ms /    12 runs   (  111.59 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1341.95 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1236.66 ms /    11 runs   (  112.42 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1239.42 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1374.39 ms /    12 runs   (  114.53 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1377.16 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1560.00 ms /    14 runs   (  111.43 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1563.21 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1222.89 ms /    11 runs   (  111.17 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1225.47 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', '[mask]', 'policies', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'strongly', 'supporting', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1243.12 ms /    11 runs   (  113.01 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1245.66 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1101.21 ms /    10 runs   (  110.12 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1103.62 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1111.94 ms /    10 runs   (  111.19 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1114.27 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1256.16 ms /    11 runs   (  114.20 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1258.65 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies [MASK] support healtLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1480.48 ms /    13 runs   (  113.88 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1483.49 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 32 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     519.59 ms /    17 tokens (   30.56 ms per token,    32.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1523.36 ms /    10 runs   (  152.34 ms per token,     6.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    2045.77 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1434.41 ms /    11 runs   (  130.40 ms per token,     7.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1437.39 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1438.02 ms /    11 runs   (  130.73 ms per token,     7.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1440.68 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', 'new', '[mask]', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'increase', 'healthcare', 'funding']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1309.58 ms /    10 runs   (  130.96 ms per token,     7.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1312.01 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', 'new', '[mask]', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'improve', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1534.11 ms /    12 runs   (  127.84 ms per token,     7.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1537.33 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', 'new', '[mask]', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'increase', 'funding', 'for', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1415.97 ms /    11 runs   (  128.72 ms per token,     7.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1419.88 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1547.14 ms /    12 runs   (  128.93 ms per token,     7.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1549.80 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1811.92 ms /    14 runs   (  129.42 ms per token,     7.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1815.62 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1463.92 ms /    12 runs   (  121.99 ms per token,     8.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1466.55 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1617.66 ms /    13 runs   (  124.44 ms per token,     8.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1620.49 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1340.47 ms /    11 runs   (  121.86 ms per token,     8.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1343.09 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1423.48 ms /    12 runs   (  118.62 ms per token,     8.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1426.24 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', 'new', '[mask]', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'increase', 'funding', 'for', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1281.28 ms /    11 runs   (  116.48 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1283.78 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1221.24 ms /    11 runs   (  111.02 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1223.76 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1209.45 ms /    11 runs   (  109.95 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1212.10 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1309.40 ms /    12 runs   (  109.12 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1312.07 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', 'new', '[mask]', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'increase', 'funding', 'for', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1190.62 ms /    11 runs   (  108.24 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1193.33 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1192.69 ms /    11 runs   (  108.43 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1195.15 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     867.13 ms /     8 runs   (  108.39 ms per token,     9.23 tokens per second)\n",
      "llama_perf_context_print:       total time =     868.92 ms /     9 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the government announced new [MASK] [MASK] support healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', 'new', '[mask]', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'healthcare', 'policies']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1075.83 ms /    10 runs   (  107.58 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1078.12 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', 'new', '[mask]', '[mask]', 'support', 'healthcare'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'enhance', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 33 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     447.62 ms /    16 tokens (   27.98 ms per token,    35.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1386.86 ms /    12 runs   (  115.57 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1837.52 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1439.62 ms /    12 runs   (  119.97 ms per token,     8.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1442.37 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1408.58 ms /    12 runs   (  117.38 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1411.21 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2266.89 ms /    20 runs   (  113.34 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    2271.50 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =         19\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1418.91 ms /    12 runs   (  118.24 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1421.57 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1778.49 ms /    15 runs   (  118.57 ms per token,     8.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1781.76 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1486.14 ms /    12 runs   (  123.84 ms per token,     8.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1488.84 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1420.78 ms /    12 runs   (  118.40 ms per token,     8.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1423.62 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1548.92 ms /    13 runs   (  119.15 ms per token,     8.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1551.91 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1821.90 ms /    15 runs   (  121.46 ms per token,     8.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1825.47 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1385.35 ms /    12 runs   (  115.45 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1387.95 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1632.18 ms /    13 runs   (  125.55 ms per token,     7.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1635.03 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1384.64 ms /    12 runs   (  115.39 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1387.75 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1360.39 ms /    12 runs   (  113.37 ms per token,     8.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    1363.18 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1350.75 ms /    12 runs   (  112.56 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1353.69 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1315.97 ms /    12 runs   (  109.66 ms per token,     9.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1318.73 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1380.44 ms /    12 runs   (  115.04 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1383.22 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1459.00 ms /    13 runs   (  112.23 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1461.95 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1351.04 ms /    12 runs   (  112.59 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1353.74 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1345.28 ms /    12 runs   (  112.11 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1347.99 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 37 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     502.10 ms /    12 tokens (   41.84 ms per token,    23.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1408.11 ms /    10 runs   (  140.81 ms per token,     7.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1912.99 ms /    22 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1841.46 ms /    15 runs   (  122.76 ms per token,     8.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1845.21 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', 'new', 'policies', '[mask]', 'support', '[mask]'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'promote', 'transparency', 'and', 'protect', 'public', 'interests']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1462.38 ms /    12 runs   (  121.87 ms per token,     8.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1465.45 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1595.65 ms /    13 runs   (  122.74 ms per token,     8.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1599.95 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1802.72 ms /    15 runs   (  120.18 ms per token,     8.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1806.29 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', 'new', 'policies', '[mask]', 'support', '[mask]'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'strengthen', 'national', 'security', 'and', 'improve', 'the', 'economy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2144.47 ms /    18 runs   (  119.14 ms per token,     8.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    2148.45 ms /    19 tokens\n",
      "llama_perf_context_print:    graphs reused =         17\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1319.63 ms /    11 runs   (  119.97 ms per token,     8.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1322.06 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1528.49 ms /    13 runs   (  117.58 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1531.34 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1757.46 ms /    15 runs   (  117.16 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1760.74 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', 'new', 'policies', '[mask]', 'support', '[mask]'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'strengthen', 'economic', 'growth', 'and', 'ensure', 'food', 'security']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1639.47 ms /    14 runs   (  117.11 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1642.57 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1311.23 ms /    11 runs   (  119.20 ms per token,     8.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1313.72 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1723.25 ms /    15 runs   (  114.88 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1726.61 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1498.67 ms /    13 runs   (  115.28 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1502.51 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1274.02 ms /    11 runs   (  115.82 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1276.46 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1642.99 ms /    14 runs   (  117.36 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1646.14 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1300.67 ms /    11 runs   (  118.24 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1303.18 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1235.69 ms /    11 runs   (  112.34 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1238.15 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1506.42 ms /    13 runs   (  115.88 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1509.37 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2185.43 ms /    19 runs   (  115.02 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    2189.98 ms /    20 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2650.20 ms /    22 runs   (  120.46 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    2655.35 ms /    23 tokens\n",
      "llama_perf_context_print:    graphs reused =         20\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', 'new', 'policies', '[mask]', 'support', '[mask]'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'ensure', '[support]', 'and', '[mask]', 'the', 'wellbeing', 'of', 'citizens']\n",
      "\n",
      "Generated Sentences:\n",
      " • the united states to\n",
      " • the australian to\n",
      " • the us to\n",
      " • the to\n",
      " •  \n",
      " • the to\n",
      " •  \n",
      " •  \n",
      " • the to\n",
      " •  \n",
      " • the australian to\n",
      " •  \n",
      " • the to\n",
      " • the british to increase\n",
      " • the to increase healthcare funding and\n",
      " •  \n",
      " • the chinese to\n",
      " • the australian to\n",
      " • the to\n",
      " • the british to\n",
      " • government to\n",
      " • government to\n",
      " •  \n",
      " •  \n",
      " • government to increase funding and\n",
      " • government to\n",
      " •  \n",
      " •  \n",
      " • government to increase\n",
      " • government to\n",
      " •  \n",
      " • government to increase\n",
      " • government to\n",
      " •  \n",
      " •  \n",
      " • government to increase\n",
      " • government to increase\n",
      " • government to\n",
      " • government to\n",
      " •  \n",
      " • implements to\n",
      " • announces to\n",
      " • will implement to\n",
      " • implements to\n",
      " • implements to\n",
      " • announced to [mask]\n",
      " • has implemented to\n",
      " • implements to\n",
      " • implements to\n",
      " • implements to\n",
      " • implements to\n",
      " • implemented to\n",
      " • introduced to\n",
      " • has implemented to increase\n",
      " • implements to\n",
      " • implements to\n",
      " • unveils that receive widespread\n",
      " • has implemented to increase\n",
      " • implements to\n",
      " • implements that strongly\n",
      " •  \n",
      " • new to\n",
      " • progressive to increase\n",
      " • comprehensive to ensure public\n",
      " • new to\n",
      " •  \n",
      " • new to expand\n",
      " • comprehensive to\n",
      " • comprehensive to\n",
      " • new to ensure\n",
      " • new to increase\n",
      " • new to significantly\n",
      " • new to strengthen\n",
      " • these comprehensive to bolster\n",
      " •  \n",
      " • new to further\n",
      " • progressive to\n",
      " • new to\n",
      " • new to strengthen healthcare\n",
      " • new emphasizing\n",
      " • policies to strengthen healthcare\n",
      " • measures to strengthen healthcare\n",
      " •  \n",
      " •  \n",
      " •  \n",
      " • policies to increase healthcare\n",
      " • policies to increase\n",
      " • health insurance subsidies to\n",
      " • policies to increase\n",
      " • initiatives to increase financial\n",
      " • measures to improve healthcare\n",
      " •  \n",
      " • policies to strengthen healthcare\n",
      " • policies to increase healthcare\n",
      " • policies to enhance healthcare\n",
      " •  \n",
      " • policies to increase healthcare\n",
      " • initiatives to enhance healthcare\n",
      " •  \n",
      " •  \n",
      " • to improve and prioritize\n",
      " • to improve and expand\n",
      " • to improve and expand\n",
      " • aimed at [expanding] [reforming]\n",
      " • to expand and improve\n",
      " • to increase funding and improve access to\n",
      " • to improve and expand\n",
      " • to improve and expand\n",
      " • aimed at improving and expanding\n",
      " • aimed at improving and expanding access to\n",
      " • to improve and expand\n",
      " • to improve and prioritize\n",
      " • to improve and expand\n",
      " • to expand and improve\n",
      " • to improve and expand\n",
      " • to improve and expand\n",
      " • to improve and expand\n",
      " • to expand [mask]\n",
      " • to expand and reform\n",
      " • to improve and expand\n",
      " • to strengthen public \n",
      " •  \n",
      " • to bolster public \n",
      " • to increase for small businesses\n",
      " •  \n",
      " • to strengthen [economic] [for small businesses]\n",
      " • to increase public \n",
      " • to increase for small businesses\n",
      " •  \n",
      " • to increase public for the environment\n",
      " • to strengthen public \n",
      " • to increase public and promote economic growth\n",
      " • to increase for small businesses\n",
      " • to small businesses\n",
      " • to provide better for small businesses\n",
      " • to small businesses\n",
      " • to small businesses\n",
      " • to increase for small businesses\n",
      " • to [enhance] [citizens]\n",
      " •  \n",
      "\n",
      "Total generated: 140\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "the             PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "government      PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "announced       PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "new             PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "policies        PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "support         PMI=   -inf   P(x)=0.850  P(y)=0.000  P(xy)=0.000\n",
      "healthcare      PMI=   -inf   P(x)=0.150  P(y)=0.000  P(xy)=0.000\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 6   |   Anchor word: 'support'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 29 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     554.27 ms /    20 tokens (   27.71 ms per token,    36.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1133.96 ms /     9 runs   (  126.00 ms per token,     7.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1690.68 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1322.95 ms /    10 runs   (  132.30 ms per token,     7.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1325.50 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1227.67 ms /    10 runs   (  122.77 ms per token,     8.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1230.14 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1330.72 ms /    12 runs   (  110.89 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1333.70 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1546.90 ms /    14 runs   (  110.49 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1550.06 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1217.07 ms /    11 runs   (  110.64 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1219.46 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1208.95 ms /    11 runs   (  109.90 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1211.39 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1115.52 ms /    10 runs   (  111.55 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1117.74 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1117.27 ms /    10 runs   (  111.73 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1119.51 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1201.55 ms /    11 runs   (  109.23 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1203.95 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'government', 'announced', 'new', 'policies', 'to', '[mask]', 'healthcare'] ['the', 'biden', 'administration', 'announced', 'new', 'policies', 'to', 'improve', 'healthcare']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1575.36 ms /    14 runs   (  112.53 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1578.41 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1182.27 ms /    11 runs   (  107.48 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1184.71 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1189.77 ms /    11 runs   (  108.16 ms per token,     9.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1192.36 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1149.82 ms /    11 runs   (  104.53 ms per token,     9.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1152.26 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1047.62 ms /    10 runs   (  104.76 ms per token,     9.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1049.84 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1044.72 ms /    10 runs   (  104.47 ms per token,     9.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1046.90 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1480.11 ms /    14 runs   (  105.72 ms per token,     9.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1483.36 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1202.13 ms /    11 runs   (  109.28 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1204.60 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1074.31 ms /    10 runs   (  107.43 ms per token,     9.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1076.60 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1067.61 ms /    10 runs   (  106.76 ms per token,     9.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1069.85 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:   0%|Llama.generate: 29 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     531.19 ms /    20 tokens (   26.56 ms per token,    37.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =     988.45 ms /     9 runs   (  109.83 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1521.94 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:   5%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1082.68 ms /    10 runs   (  108.27 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1084.95 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  10%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     941.38 ms /     9 runs   (  104.60 ms per token,     9.56 tokens per second)\n",
      "llama_perf_context_print:       total time =     943.52 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  15%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1046.57 ms /    10 runs   (  104.66 ms per token,     9.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1048.89 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  20%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1064.61 ms /    10 runs   (  106.46 ms per token,     9.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1067.02 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  25%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1049.64 ms /    10 runs   (  104.96 ms per token,     9.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1051.90 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  30%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1078.44 ms /    10 runs   (  107.84 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1080.66 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  35%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1056.38 ms /    10 runs   (  105.64 ms per token,     9.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1058.60 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  40%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1058.41 ms /    10 runs   (  105.84 ms per token,     9.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1060.59 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  45%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1047.29 ms /    10 runs   (  104.73 ms per token,     9.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1049.55 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  50%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1076.66 ms /    10 runs   (  107.67 ms per token,     9.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1078.86 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  55%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1056.80 ms /    10 runs   (  105.68 ms per token,     9.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1059.02 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  60%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1061.55 ms /    10 runs   (  106.16 ms per token,     9.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1063.79 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  65%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1061.27 ms /    10 runs   (  106.13 ms per token,     9.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1063.50 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  70%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1045.27 ms /    10 runs   (  104.53 ms per token,     9.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1047.46 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  75%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1047.88 ms /    10 runs   (  104.79 ms per token,     9.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1050.04 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  80%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1051.23 ms /    10 runs   (  105.12 ms per token,     9.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1053.44 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  85%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1052.44 ms /    10 runs   (  105.24 ms per token,     9.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1054.77 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  90%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1050.34 ms /    10 runs   (  105.03 ms per token,     9.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1052.85 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to [MASK] healthcare:  95%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1041.88 ms /    10 runs   (  104.19 ms per token,     9.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1044.06 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:   0%Llama.generate: 30 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     781.85 ms /    19 tokens (   41.15 ms per token,    24.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =     972.22 ms /     9 runs   (  108.02 ms per token,     9.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1756.72 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:   5%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1099.38 ms /    10 runs   (  109.94 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1101.71 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  10%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1085.04 ms /    10 runs   (  108.50 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1087.45 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  15%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1078.76 ms /    10 runs   (  107.88 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1081.09 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  20%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1048.02 ms /    10 runs   (  104.80 ms per token,     9.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1050.28 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  25%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     968.05 ms /     9 runs   (  107.56 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =     970.12 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  30%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1089.50 ms /    10 runs   (  108.95 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1092.22 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  35%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1067.77 ms /    10 runs   (  106.78 ms per token,     9.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1070.12 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  40%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     963.37 ms /     9 runs   (  107.04 ms per token,     9.34 tokens per second)\n",
      "llama_perf_context_print:       total time =     965.54 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  45%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1066.70 ms /    10 runs   (  106.67 ms per token,     9.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1068.93 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  50%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1063.63 ms /    10 runs   (  106.36 ms per token,     9.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1065.96 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  55%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1059.68 ms /    10 runs   (  105.97 ms per token,     9.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1062.03 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  60%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1052.26 ms /    10 runs   (  105.23 ms per token,     9.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1054.59 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  65%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1080.42 ms /    10 runs   (  108.04 ms per token,     9.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1082.78 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  70%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1167.82 ms /    11 runs   (  106.17 ms per token,     9.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1170.36 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  75%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     989.90 ms /     9 runs   (  109.99 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =     991.99 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  80%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1065.89 ms /    10 runs   (  106.59 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1068.35 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  85%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     987.84 ms /     9 runs   (  109.76 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =     990.21 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  90%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1085.48 ms /    10 runs   (  108.55 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1087.82 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to [MASK] healthcare:  95%Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1093.54 ms /    10 runs   (  109.35 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1096.18 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 31 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     563.92 ms /    18 tokens (   31.33 ms per token,    31.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1061.07 ms /     9 runs   (  117.90 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1627.37 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1226.46 ms /    10 runs   (  122.65 ms per token,     8.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1228.66 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1216.00 ms /    10 runs   (  121.60 ms per token,     8.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1218.52 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1243.24 ms /    10 runs   (  124.32 ms per token,     8.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1245.57 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1235.01 ms /    10 runs   (  123.50 ms per token,     8.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1237.25 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1205.25 ms /    10 runs   (  120.53 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1207.53 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1114.05 ms /    10 runs   (  111.40 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1116.42 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1151.42 ms /    10 runs   (  115.14 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1154.05 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1154.90 ms /    10 runs   (  115.49 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1157.16 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1140.19 ms /    10 runs   (  114.02 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1142.68 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1149.41 ms /    10 runs   (  114.94 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1151.66 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1151.24 ms /    10 runs   (  115.12 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1153.52 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1163.22 ms /    10 runs   (  116.32 ms per token,     8.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1165.47 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1098.05 ms /    10 runs   (  109.80 ms per token,     9.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1100.30 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1149.39 ms /    10 runs   (  114.94 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1151.62 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1126.52 ms /    10 runs   (  112.65 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1128.73 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1120.89 ms /    10 runs   (  112.09 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1123.11 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1134.85 ms /    10 runs   (  113.49 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1137.52 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1089.46 ms /    10 runs   (  108.95 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1091.72 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to [MASK] healthcareLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1130.64 ms /    10 runs   (  113.06 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1132.89 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:   0Llama.generate: 32 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     683.96 ms /    17 tokens (   40.23 ms per token,    24.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =     983.71 ms /     9 runs   (  109.30 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1670.03 ms /    26 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:   5Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1158.45 ms /    10 runs   (  115.84 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1160.79 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  10Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1151.22 ms /    10 runs   (  115.12 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1153.50 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  15Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1108.74 ms /    10 runs   (  110.87 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1111.02 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  20Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1160.90 ms /    10 runs   (  116.09 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1163.27 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  25Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1145.67 ms /    10 runs   (  114.57 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1148.13 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  30Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1137.84 ms /    10 runs   (  113.78 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1140.20 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  35Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1139.83 ms /    10 runs   (  113.98 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1142.18 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  40Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1166.16 ms /    10 runs   (  116.62 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1168.51 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  45Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1149.04 ms /    10 runs   (  114.90 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1151.37 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  50Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1102.72 ms /    10 runs   (  110.27 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1105.04 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  55Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1134.56 ms /    10 runs   (  113.46 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1136.87 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  60Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1148.94 ms /    10 runs   (  114.89 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1151.18 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  65Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1138.33 ms /    10 runs   (  113.83 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1141.26 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  70Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1147.74 ms /    10 runs   (  114.77 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1150.07 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  75Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1169.53 ms /    10 runs   (  116.95 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1171.87 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  80Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1160.89 ms /    10 runs   (  116.09 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1163.40 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  85Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1111.14 ms /    10 runs   (  111.11 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1113.74 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  90Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1160.01 ms /    10 runs   (  116.00 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1162.31 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to [MASK] healthcare:  95Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1153.77 ms /    10 runs   (  115.38 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1156.07 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 33 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     661.11 ms /    16 tokens (   41.32 ms per token,    24.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1491.71 ms /    13 runs   (  114.75 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    2156.10 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1793.09 ms /    15 runs   (  119.54 ms per token,     8.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1796.81 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1365.35 ms /    12 runs   (  113.78 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1368.03 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1851.95 ms /    16 runs   (  115.75 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1855.43 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1634.06 ms /    14 runs   (  116.72 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1637.12 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1401.13 ms /    12 runs   (  116.76 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1403.74 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1752.55 ms /    15 runs   (  116.84 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1755.94 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1352.44 ms /    12 runs   (  112.70 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1355.39 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1702.09 ms /    15 runs   (  113.47 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1705.74 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1371.05 ms /    12 runs   (  114.25 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1373.86 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1481.79 ms /    13 runs   (  113.98 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1484.67 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1369.66 ms /    12 runs   (  114.14 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1372.25 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1702.48 ms /    15 runs   (  113.50 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1705.90 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1942.84 ms /    17 runs   (  114.28 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1947.22 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1366.50 ms /    12 runs   (  113.87 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1369.29 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1318.06 ms /    12 runs   (  109.84 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1320.72 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1369.47 ms /    12 runs   (  114.12 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1372.07 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1486.91 ms /    13 runs   (  114.38 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1490.01 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1440.98 ms /    13 runs   (  110.84 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1443.85 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new policies [MASK] [MASK] healthcarLlama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1372.83 ms /    12 runs   (  114.40 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1375.48 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:   0%|Llama.generate: 34 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =    1108.19 ms /    15 tokens (   73.88 ms per token,    13.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1274.64 ms /    11 runs   (  115.88 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    2385.78 ms /    26 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:   5%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1741.71 ms /    15 runs   (  116.11 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1746.47 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  10%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1595.32 ms /    14 runs   (  113.95 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1598.70 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  15%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1644.96 ms /    15 runs   (  109.66 ms per token,     9.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1648.53 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  20%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1808.79 ms /    16 runs   (  113.05 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1812.96 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  25%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1581.88 ms /    14 runs   (  112.99 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1585.82 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  30%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1687.20 ms /    15 runs   (  112.48 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1691.81 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  35%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1421.20 ms /    13 runs   (  109.32 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1424.07 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  40%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1705.02 ms /    16 runs   (  106.56 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1708.46 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  45%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1382.08 ms /    13 runs   (  106.31 ms per token,     9.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1384.86 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  50%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1287.25 ms /    12 runs   (  107.27 ms per token,     9.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1289.83 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  55%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1193.62 ms /    11 runs   (  108.51 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1196.06 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  60%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1498.49 ms /    14 runs   (  107.03 ms per token,     9.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1501.55 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  65%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1239.26 ms /    11 runs   (  112.66 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1242.08 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  70%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1833.98 ms /    15 runs   (  122.27 ms per token,     8.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1837.98 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  75%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1765.73 ms /    15 runs   (  117.72 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1770.23 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  80%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1344.99 ms /    12 runs   (  112.08 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1348.10 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  85%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1686.45 ms /    15 runs   (  112.43 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1690.42 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  90%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1799.89 ms /    16 runs   (  112.49 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1803.97 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  95%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1361.53 ms /    12 runs   (  113.46 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1364.55 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • the improve\n",
      " • the improve\n",
      " • the improve\n",
      " • the united states improve\n",
      " • the us improve\n",
      " • the australian improve\n",
      " • the australian improve\n",
      " • the improve\n",
      " • the improve\n",
      " •  \n",
      " • the us improve\n",
      " • the uk improve\n",
      " • the australian improve\n",
      " • the australian improve\n",
      " • the improve\n",
      " • the british improve\n",
      " • the us improve\n",
      " • the australian improve\n",
      " • the improve\n",
      " • the improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • government improve\n",
      " • implemented improve\n",
      " • introduced improve\n",
      " • introduced improve\n",
      " • implemented improve\n",
      " • introduced improve\n",
      " • introduced reform\n",
      " • implemented improve\n",
      " • introduced improve\n",
      " • introduced reform\n",
      " • implemented improve\n",
      " • implemented improve\n",
      " • implemented improve\n",
      " • introduced reform\n",
      " • developed improve\n",
      " • devised improve\n",
      " • implemented improve\n",
      " • implemented improve\n",
      " • implemented improve\n",
      " • introduced improve\n",
      " • implemented improve\n",
      " • comprehensive improve\n",
      " • comprehensive transform\n",
      " • comprehensive improve\n",
      " • comprehensive improve\n",
      " • comprehensive improve\n",
      " • comprehensive improve\n",
      " • comprehensive reform\n",
      " • comprehensive reform\n",
      " • comprehensive improve\n",
      " • comprehensive reform\n",
      " • comprehensive improve\n",
      " • new improve\n",
      " • comprehensive improve\n",
      " • comprehensive improve\n",
      " • comprehensive improve\n",
      " • comprehensive improve\n",
      " • comprehensive improve\n",
      " • comprehensive improve\n",
      " • comprehensive improve\n",
      " • comprehensive improve\n",
      " • regulations improve\n",
      " • regulations improve\n",
      " • policies improve\n",
      " • policies improve\n",
      " • policies improve\n",
      " • policies improve\n",
      " • measures improve\n",
      " • initiatives improve\n",
      " • policies improve\n",
      " • initiatives improve\n",
      " • regulations improve\n",
      " • policies improve\n",
      " • policies improve\n",
      " • initiatives improve\n",
      " • regulations improve\n",
      " • policies improve\n",
      " • policies improve\n",
      " • regulations improve\n",
      " • regulations improve\n",
      " • policies improve\n",
      " • aimed at improving and expanding\n",
      " • designed to improve and expand access to\n",
      " • to improve and expand\n",
      " • designed to improve and expand [mask]\n",
      " • to improve and expand\n",
      " • to improve and expand\n",
      " • to increase funding and expand access in\n",
      " • to improve and expand\n",
      " • aimed at improving and expanding access to\n",
      " • to improve and expand\n",
      " • to improve and prioritize\n",
      " • to improve and expand\n",
      " • designed to improve and expand access to\n",
      " • aimed at improving access to and affordability of\n",
      " • to improve and expand\n",
      " • to improve and expand\n",
      " • to improve and expand\n",
      " • to improve and expand\n",
      " • to improve and prioritize\n",
      " • to improve and expand\n",
      " • mitigate economic inequality\n",
      " • combat climate change and improve public health\n",
      " • alleviate economic disparity\n",
      " • encourage economic growth and address climate change\n",
      " • mitigate climate change and boost the economy\n",
      " • reduce deficit and boost economy\n",
      " • address climate change and promote economic growth\n",
      " • mitigate economic disparities\n",
      " • mitigate climate change and reduce carbon emissions\n",
      " • mitigate economic disparities\n",
      " • mitigate climate change\n",
      " • implement and address\n",
      " • address and mitigate the crisis\n",
      " • improve the economy\n",
      " • address climate change and boost the economy\n",
      " • address and resolve economic imbalances\n",
      " • mitigate economic inequality\n",
      " • combat climate change and improve public health\n",
      " • reduce carbon emissions and promote renewable energy\n",
      " • curb global warming\n",
      "\n",
      "Total generated: 140\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "the             PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "government      PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "announced       PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "new             PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "policies        PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "to              PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "healthcare      PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "\n",
      "==========================================================================================\n",
      "Anchor index: 7   |   Anchor word: 'healthcare'\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 29 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     783.39 ms /    20 tokens (   39.17 ms per token,    25.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1474.18 ms /    11 runs   (  134.02 ms per token,     7.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    2260.87 ms /    31 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1621.15 ms /    12 runs   (  135.10 ms per token,     7.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1624.21 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1456.49 ms /    11 runs   (  132.41 ms per token,     7.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1459.27 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1506.84 ms /    11 runs   (  136.99 ms per token,     7.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1510.44 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1490.64 ms /    11 runs   (  135.51 ms per token,     7.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1493.30 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1447.83 ms /    11 runs   (  131.62 ms per token,     7.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1450.40 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1622.32 ms /    12 runs   (  135.19 ms per token,     7.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1625.86 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1595.25 ms /    12 runs   (  132.94 ms per token,     7.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1598.20 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1365.65 ms /    11 runs   (  124.15 ms per token,     8.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1368.24 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1670.75 ms /    13 runs   (  128.52 ms per token,     7.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    1673.75 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1470.25 ms /    12 runs   (  122.52 ms per token,     8.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1473.10 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1208.47 ms /    10 runs   (  120.85 ms per token,     8.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1210.89 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1400.77 ms /    12 runs   (  116.73 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1403.71 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1264.90 ms /    11 runs   (  114.99 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1267.67 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1279.92 ms /    11 runs   (  116.36 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1282.48 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1392.14 ms /    12 runs   (  116.01 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1394.94 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1371.98 ms /    12 runs   (  114.33 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    1374.73 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1158.11 ms /    10 runs   (  115.81 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1160.35 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1283.23 ms /    11 runs   (  116.66 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1285.78 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: [MASK] government announced new policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1454.47 ms /    11 runs   (  132.22 ms per token,     7.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1457.04 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:   0%| | Llama.generate: 29 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     590.74 ms /    20 tokens (   29.54 ms per token,    33.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1187.38 ms /     9 runs   (  131.93 ms per token,     7.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1780.60 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:   5%| | Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1417.52 ms /    11 runs   (  128.87 ms per token,     7.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1420.44 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  10%| | Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1258.92 ms /    10 runs   (  125.89 ms per token,     7.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1261.37 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  15%|▏| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1438.91 ms /    11 runs   (  130.81 ms per token,     7.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1441.51 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  20%|▏| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1439.14 ms /    11 runs   (  130.83 ms per token,     7.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1441.76 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  25%|▎| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1288.71 ms /    10 runs   (  128.87 ms per token,     7.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1291.13 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  30%|▎| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1438.88 ms /    11 runs   (  130.81 ms per token,     7.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1441.50 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  35%|▎| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1397.09 ms /    11 runs   (  127.01 ms per token,     7.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1399.96 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  40%|▍| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1387.47 ms /    11 runs   (  126.13 ms per token,     7.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1390.13 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  45%|▍| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1322.80 ms /    11 runs   (  120.25 ms per token,     8.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1325.64 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  50%|▌| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1185.20 ms /    10 runs   (  118.52 ms per token,     8.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1187.68 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  55%|▌| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1268.91 ms /    11 runs   (  115.36 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1271.41 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  60%|▌| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1165.12 ms /    10 runs   (  116.51 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1167.37 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  65%|▋| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1281.20 ms /    11 runs   (  116.47 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1283.68 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  70%|▋| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1196.92 ms /    10 runs   (  119.69 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1200.20 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  75%|▊| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1193.40 ms /    10 runs   (  119.34 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1195.77 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  80%|▊| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1273.66 ms /    11 runs   (  115.79 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1276.00 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  85%|▊| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1151.89 ms /    10 runs   (  115.19 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1154.10 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  90%|▉| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1244.07 ms /    11 runs   (  113.10 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1246.42 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the [MASK] announced new policies to support [MASK]:  95%|▉| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1240.13 ms /    11 runs   (  112.74 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1242.49 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:   0%| |Llama.generate: 30 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     567.51 ms /    19 tokens (   29.87 ms per token,    33.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1112.19 ms /     9 runs   (  123.58 ms per token,     8.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1682.19 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:   5%| |Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1376.90 ms /    11 runs   (  125.17 ms per token,     7.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1379.56 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  10%| |Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1386.74 ms /    11 runs   (  126.07 ms per token,     7.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1389.45 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  15%|▏|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1360.15 ms /    11 runs   (  123.65 ms per token,     8.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1362.97 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  20%|▏|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1345.49 ms /    11 runs   (  122.32 ms per token,     8.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1348.28 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  25%|▎|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1350.29 ms /    11 runs   (  122.75 ms per token,     8.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1353.24 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  30%|▎|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1366.21 ms /    11 runs   (  124.20 ms per token,     8.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1368.93 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  35%|▎|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1282.62 ms /    10 runs   (  128.26 ms per token,     7.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1285.11 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  40%|▍|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1412.21 ms /    11 runs   (  128.38 ms per token,     7.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1414.99 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  45%|▍|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1470.04 ms /    12 runs   (  122.50 ms per token,     8.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1473.02 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  50%|▌|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1317.72 ms /    11 runs   (  119.79 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    1320.70 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  55%|▌|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1335.42 ms /    11 runs   (  121.40 ms per token,     8.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    1338.21 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  60%|▌|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1300.01 ms /    11 runs   (  118.18 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1302.59 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  65%|▋|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1314.81 ms /    11 runs   (  119.53 ms per token,     8.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1318.30 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  70%|▋|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1168.52 ms /    10 runs   (  116.85 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1170.76 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  75%|▊|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1285.85 ms /    11 runs   (  116.90 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1288.27 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  80%|▊|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1365.91 ms /    11 runs   (  124.17 ms per token,     8.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1368.36 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  85%|▊|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1492.60 ms /    12 runs   (  124.38 ms per token,     8.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1495.85 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  90%|▉|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1157.63 ms /    10 runs   (  115.76 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1159.88 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government [MASK] new policies to support [MASK]:  95%|▉|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1013.79 ms /     9 runs   (  112.64 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1016.45 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 31 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     667.81 ms /    18 tokens (   37.10 ms per token,    26.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1126.99 ms /    10 runs   (  112.70 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    1797.42 ms /    28 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1241.64 ms /    11 runs   (  112.88 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1244.77 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1287.48 ms /    11 runs   (  117.04 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1289.97 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1289.93 ms /    11 runs   (  117.27 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1292.50 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1462.63 ms /    11 runs   (  132.97 ms per token,     7.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1465.58 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1608.95 ms /    14 runs   (  114.93 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    1612.10 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1144.21 ms /    10 runs   (  114.42 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1146.43 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1226.30 ms /    11 runs   (  111.48 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1228.72 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1200.57 ms /    11 runs   (  109.14 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1203.10 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1136.78 ms /    10 runs   (  113.68 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1139.33 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1221.31 ms /    11 runs   (  111.03 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1223.75 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1209.05 ms /    11 runs   (  109.91 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1211.55 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1236.88 ms /    11 runs   (  112.44 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1239.33 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1109.30 ms /    10 runs   (  110.93 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1111.55 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1216.72 ms /    11 runs   (  110.61 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1219.15 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1225.00 ms /    11 runs   (  111.36 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1227.44 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1318.70 ms /    12 runs   (  109.89 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1321.40 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1210.68 ms /    11 runs   (  110.06 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1213.10 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1216.59 ms /    11 runs   (  110.60 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1218.98 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced [MASK] policies to support [MASK]:  Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1224.35 ms /    11 runs   (  111.30 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1226.86 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:   0%| Llama.generate: 32 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     739.81 ms /    17 tokens (   43.52 ms per token,    22.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1098.89 ms /    10 runs   (  109.89 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1841.14 ms /    27 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:   5%| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1412.71 ms /    12 runs   (  117.73 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    1415.52 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  10%| Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1301.32 ms /    11 runs   (  118.30 ms per token,     8.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1303.91 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  15%|▏Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1087.84 ms /    10 runs   (  108.78 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1090.02 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  20%|▏Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1155.94 ms /    10 runs   (  115.59 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1158.15 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  25%|▎Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1357.68 ms /    12 runs   (  113.14 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1360.53 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  30%|▎Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1108.74 ms /    10 runs   (  110.87 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1110.97 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  35%|▎Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1100.40 ms /    10 runs   (  110.04 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1102.57 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  40%|▍Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1126.33 ms /    10 runs   (  112.63 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1128.58 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  45%|▍Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1253.82 ms /    11 runs   (  113.98 ms per token,     8.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1256.26 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  50%|▌Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1119.50 ms /    10 runs   (  111.95 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1121.70 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  55%|▌Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1106.42 ms /    10 runs   (  110.64 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1108.60 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  60%|▌Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1496.57 ms /    13 runs   (  115.12 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1499.74 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  65%|▋Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1083.18 ms /    10 runs   (  108.32 ms per token,     9.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1085.66 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  70%|▋Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1195.12 ms /    11 runs   (  108.65 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1197.56 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  75%|▊Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1280.43 ms /    11 runs   (  116.40 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1282.96 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  80%|▊Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1275.32 ms /    11 runs   (  115.94 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1277.85 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  85%|▊Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1357.61 ms /    12 runs   (  113.13 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1360.33 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  90%|▉Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1092.76 ms /    10 runs   (  109.28 ms per token,     9.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1094.93 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =         10\n",
      "Processing prompt: the government announced new [MASK] to support [MASK]:  95%|▉Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1196.91 ms /    11 runs   (  108.81 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1199.32 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 33 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     853.65 ms /    16 tokens (   53.35 ms per token,    18.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1268.93 ms /    10 runs   (  126.89 ms per token,     7.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    2125.39 ms /    26 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1691.66 ms /    13 runs   (  130.13 ms per token,     7.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1694.80 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1857.94 ms /    14 runs   (  132.71 ms per token,     7.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1861.27 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1486.75 ms /    11 runs   (  135.16 ms per token,     7.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1489.67 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1931.01 ms /    15 runs   (  128.73 ms per token,     7.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1934.96 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', 'new', 'policies', '[mask]', 'support', '[mask]'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'enhance', '[support]', 'and', '[mask]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2156.44 ms /    16 runs   (  134.78 ms per token,     7.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    2161.53 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', 'new', 'policies', '[mask]', 'support', '[mask]'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'increase', '[mask]', 'and', 'strengthen', '[mask]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1885.22 ms /    15 runs   (  125.68 ms per token,     7.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1888.69 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1705.71 ms /    13 runs   (  131.21 ms per token,     7.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1708.45 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2324.31 ms /    16 runs   (  145.27 ms per token,     6.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    2328.16 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', 'new', 'policies', '[mask]', 'support', '[mask]'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'strengthen', 'national', 'security', 'and', 'bolster', 'economic', 'growth']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1466.21 ms /    11 runs   (  133.29 ms per token,     7.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1468.75 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1059.12 ms /     9 runs   (  117.68 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1061.17 ms /    10 tokens\n",
      "llama_perf_context_print:    graphs reused =          9\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1757.02 ms /    15 runs   (  117.13 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1760.62 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1898.70 ms /    16 runs   (  118.67 ms per token,     8.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1902.29 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', 'new', 'policies', '[mask]', 'support', '[mask]'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'promote', 'environmental', 'sustainability', 'and', 'strengthen', 'national', 'security']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1285.56 ms /    11 runs   (  116.87 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1287.99 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1305.77 ms /    11 runs   (  118.71 ms per token,     8.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1308.16 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1848.93 ms /    16 runs   (  115.56 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    1852.52 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['the', 'government', 'announced', 'new', 'policies', '[mask]', 'support', '[mask]'] ['the', 'government', 'announced', 'new', 'policies', 'to', 'increase', '[mask]', 'and', 'enhance', '[mask]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1624.09 ms /    14 runs   (  116.01 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    1627.55 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1509.05 ms /    13 runs   (  116.08 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1511.97 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1497.81 ms /    13 runs   (  115.22 ms per token,     8.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1500.64 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new policies [MASK] support [MASK]: Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1268.74 ms /    11 runs   (  115.34 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1271.09 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:   0%|Llama.generate: 34 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =     480.77 ms /    15 tokens (   32.05 ms per token,    31.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1610.03 ms /    14 runs   (  115.00 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    2094.11 ms /    29 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:   5%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1571.02 ms /    14 runs   (  112.22 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1574.11 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  10%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1389.20 ms /    12 runs   (  115.77 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1391.86 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  15%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1457.60 ms /    13 runs   (  112.12 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    1460.45 ms /    14 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  20%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1698.28 ms /    15 runs   (  113.22 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1701.58 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  25%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1750.94 ms /    15 runs   (  116.73 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1754.40 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  30%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1759.29 ms /    15 runs   (  117.29 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1762.74 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  35%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1982.89 ms /    17 runs   (  116.64 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1986.56 ms /    18 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  40%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1662.27 ms /    14 runs   (  118.73 ms per token,     8.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1665.39 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  45%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1423.89 ms /    12 runs   (  118.66 ms per token,     8.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1426.59 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  50%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1812.35 ms /    16 runs   (  113.27 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1816.05 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =         16\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  55%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1776.20 ms /    15 runs   (  118.41 ms per token,     8.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1779.45 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  60%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1619.88 ms /    14 runs   (  115.71 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1622.95 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  65%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1826.35 ms /    15 runs   (  121.76 ms per token,     8.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1829.84 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  70%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1418.42 ms /    12 runs   (  118.20 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1421.32 ms /    13 tokens\n",
      "llama_perf_context_print:    graphs reused =         12\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  75%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1732.21 ms /    14 runs   (  123.73 ms per token,     8.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1735.31 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  80%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1930.51 ms /    15 runs   (  128.70 ms per token,     7.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1933.86 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  85%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1373.33 ms /    11 runs   (  124.85 ms per token,     8.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1375.78 ms /    12 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  90%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1900.64 ms /    15 runs   (  126.71 ms per token,     7.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1904.01 ms /    16 tokens\n",
      "llama_perf_context_print:    graphs reused =         15\n",
      "Processing prompt: the government announced new policies to [MASK] [MASK]:  95%|Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     976.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1752.10 ms /    14 runs   (  125.15 ms per token,     7.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1755.24 ms /    15 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      " • the japanese small businesses\n",
      " • the federal small businesses\n",
      " • the small businesses\n",
      " • the the environment\n",
      " • the french farmers\n",
      " • the small businesses\n",
      " • the australian small businesses\n",
      " • the british small businesses\n",
      " • the small businesses\n",
      " • the united states small businesses\n",
      " • the federal small businesses\n",
      " • the businesses\n",
      " • the mexican small businesses\n",
      " • the the economy\n",
      " • the british businesses\n",
      " • the japanese small businesses\n",
      " • the mexican small businesses\n",
      " • the agriculture\n",
      " • the irish farmers\n",
      " • the small businesses\n",
      " • government businesses\n",
      " • government small businesses\n",
      " • government businesses\n",
      " • government small businesses\n",
      " • government small businesses\n",
      " • government education\n",
      " • government small businesses\n",
      " • government small businesses\n",
      " • government the economy\n",
      " • government small businesses\n",
      " • government citizens\n",
      " • government small businesses\n",
      " • government farmers\n",
      " • government small businesses\n",
      " • government education\n",
      " • government businesses\n",
      " • government small businesses\n",
      " • government businesses\n",
      " • government small businesses\n",
      " • government small businesses\n",
      " • announced businesses\n",
      " • implemented small businesses\n",
      " • announced small businesses\n",
      " • implemented small businesses\n",
      " • developed small businesses\n",
      " • announced the environment\n",
      " • introduced small businesses\n",
      " • developed businesses\n",
      " • introduced small businesses\n",
      " • devised small businesses\n",
      " • announced small businesses\n",
      " • introduced small businesses\n",
      " • announced small businesses\n",
      " • introduced small businesses\n",
      " • implemented businesses\n",
      " • implements the economy\n",
      " • implemented small businesses\n",
      " • is implementing small businesses\n",
      " • introduced businesses\n",
      " • implemented employment\n",
      " • aggressive small businesses\n",
      " • comprehensive economic growth\n",
      " • progressive the environment\n",
      " • aggressive small businesses\n",
      " • ambitious small businesses\n",
      " • progressive lowincome families\n",
      " • new businesses\n",
      " • specific small businesses\n",
      " • progressive vulnerable communities\n",
      " • comprehensive businesses\n",
      " • economic small businesses\n",
      " • ambitious economic recovery\n",
      " • ambitious the economy\n",
      " • clear citizens\n",
      " • comprehensive small businesses\n",
      " • new small businesses\n",
      " • new economic small businesses\n",
      " • progressive the environment\n",
      " • comprehensive economic growth\n",
      " • ambitious sustainable development\n",
      " • regulations small businesses\n",
      " • incentives small businesses\n",
      " • policies small businesses\n",
      " • measures businesses\n",
      " • initiatives businesses\n",
      " • subsidies farmers\n",
      " • policies businesses\n",
      " • measures businesses\n",
      " • policies businesses\n",
      " • policies the economy\n",
      " • policies education\n",
      " • initiatives businesses\n",
      " • subsidies small businesses\n",
      " • policies citizens\n",
      " • economic measures businesses\n",
      " • policies small businesses\n",
      " • policies small businesses\n",
      " • economic stimulus businesses\n",
      " • policies businesses\n",
      " • measures small businesses\n",
      " • to strengthen public \n",
      " • to enhance for small businesses\n",
      " • to increase public for sustainability\n",
      " • to small businesses\n",
      " •  \n",
      " •  \n",
      " • to enhance public and improve public services\n",
      " • to increase for vulnerable populations\n",
      " •  \n",
      " • to strengthen public \n",
      " • to strengthen \n",
      " • to economic growth and environmental sustainability\n",
      " •  \n",
      " • to strengthen public \n",
      " • to small businesses\n",
      " •  \n",
      " • to increase public for the environment\n",
      " • to increase for small businesses\n",
      " • to strengthen for small businesses\n",
      " • to small businesses\n",
      " • address climate change and boost the economy\n",
      " • address and mitigate the crisis\n",
      " • implement and regulate\n",
      " • improve transparency in healthcare\n",
      " • address climate change and boost the economy\n",
      " • address climate change and improve public health\n",
      " • reduce carbon emissions and improve public transportation\n",
      " • reduce greenhouse gas emissions and promote sustainable development\n",
      " • address climate change and reduce emissions\n",
      " • mitigate economic inequality\n",
      " • reduce carbon emissions and promote renewable energy\n",
      " • tackle climate change and boost the economy\n",
      " • curb inflation and promote economic growth\n",
      " • address climate change and promote economic growth\n",
      " • mitigate climate change\n",
      " • alleviate economic disparity\n",
      " • reduce carbon emissions and improve public health\n",
      " • implement and improve\n",
      " • encourage economic growth and protect the environment\n",
      " • address climate change and economic growth\n",
      "\n",
      "Total generated: 140\n",
      "\n",
      "\n",
      "PMI Scores:\n",
      "the             PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "government      PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "announced       PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "new             PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "policies        PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "to              PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "support         PMI=   -inf   P(x)=0.000  P(y)=0.000  P(xy)=0.000\n",
      "\n",
      "=== EXERCISE 4 COMPLETE — READY FOR VISUALIZATION ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# EXERCISE 4\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from tools.command_generator import generate_prompts, prefix_prompt\n",
    "from tools.evaluate_response import get_replacements\n",
    "\n",
    "# PROMPT SAMPLING\n",
    "def run_prompts(model, sentence, anchor_idx, prompts_per_word=20):\n",
    "    prompts = generate_prompts(sentence, anchor_idx)\n",
    "    all_replacements = []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        replacements = []\n",
    "        for _ in tqdm(range(prompts_per_word),\n",
    "                       desc=f\"Processing prompt: {prompt}\",\n",
    "                       leave=False):\n",
    "\n",
    "            response = model.get_response(prefix_prompt(prompt)).strip()\n",
    "            if not response:\n",
    "                continue\n",
    "\n",
    "            replacement = get_replacements(prompt, response)\n",
    "            if replacement:\n",
    "                replacements.append(replacement)\n",
    "\n",
    "        if replacements:\n",
    "            all_replacements.append(replacements)\n",
    "\n",
    "    return all_replacements\n",
    "\n",
    "\n",
    "# VISUALIZE RAW GENERATED SENTENCES\n",
    "def visualize_generated_sentences(all_responses):\n",
    "    print(\"\\nGenerated Sentences:\")\n",
    "    flat = []\n",
    "\n",
    "    for group in all_responses:\n",
    "        for pair in group:\n",
    "            text = \" \".join(pair)\n",
    "            flat.append(text)\n",
    "            print(\" •\", text)\n",
    "\n",
    "    print(f\"\\nTotal generated: {len(flat)}\\n\")\n",
    "    return flat\n",
    "\n",
    "\n",
    "# RUN PMI FOR ALL ANCHOR WORDS IN ONE SENTENCE\n",
    "def run_sentence_experiment(sentence, prompts_per_word=20):\n",
    "    words = sentence.split()\n",
    "    anchor_indices = list(range(len(words)))\n",
    "\n",
    "    print(\"\\n\" + \"#\"*110)\n",
    "    print(f\"ANALYZING SENTENCE:\\n   '{sentence}'\")\n",
    "    print(\"#\"*110)\n",
    "\n",
    "    sentence_results = {}\n",
    "\n",
    "    for anchor_idx in anchor_indices:\n",
    "        anchor_word = words[anchor_idx]\n",
    "\n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(f\"Anchor index: {anchor_idx}   |   Anchor word: '{anchor_word}'\")\n",
    "        print(\"=\"*90)\n",
    "\n",
    "        # Generate model outputs\n",
    "        all_responses = run_prompts(model, sentence, anchor_idx, prompts_per_word)\n",
    "\n",
    "        # Show generated sentences\n",
    "        visualize_generated_sentences(all_responses)\n",
    "\n",
    "        # Compute PMI\n",
    "        pmi_scores = compute_pmi(sentence, all_responses, anchor_idx)\n",
    "\n",
    "        # Print PMI table (no color here)\n",
    "        print(\"\\nPMI Scores:\")\n",
    "        if len(pmi_scores) == 0:\n",
    "            print(\"No valid PMI scores (model did not regenerate expected words).\")\n",
    "        else:\n",
    "            for idx in sorted(pmi_scores.keys()):\n",
    "                d = pmi_scores[idx]\n",
    "                print(f\"{d['word']:<15} PMI={d['pmi']:7.3f}   \"\n",
    "                      f\"P(x)={d['P_x']:.3f}  P(y)={d['P_y']:.3f}  P(xy)={d['P_xy']:.3f}\")\n",
    "\n",
    "        # Save PMI so we can visualize later\n",
    "        sentence_results[anchor_word] = {\n",
    "            \"anchor_idx\": anchor_idx,\n",
    "            \"pmi_scores\": pmi_scores\n",
    "        }\n",
    "\n",
    "    return sentence_results\n",
    "\n",
    "\n",
    "# RUN EXPERIMENT ACROSS MULTIPLE SENTENCES\n",
    "experiment_sentences = [\n",
    "    \"doctors assess symptoms to diagnose diseases\",\n",
    "    \"artificial intelligence transforms modern industries\",\n",
    "    \"children love sweet ice cream on warm summer days\",\n",
    "    \"plants require sunlight and water to grow\",\n",
    "    \"the government announced new policies to support healthcare\",\n",
    "]\n",
    "\n",
    "all_sentence_results = {}\n",
    "\n",
    "for sent in experiment_sentences:\n",
    "    results = run_sentence_experiment(sent, prompts_per_word=20)\n",
    "    all_sentence_results[sent] = results\n",
    "\n",
    "print(\"\\n=== EXERCISE 4 COMPLETE — READY FOR VISUALIZATION ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########################################################################################################################\n",
      "VISUALIZATIONS FOR SENTENCE:\n",
      "  'doctors assess symptoms to diagnose diseases'\n",
      "########################################################################################################################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'doctors' (index 0)\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mdoctors\u001b[0m assess \u001b[33msymptoms(0.74)\u001b[0m \u001b[32mto(1.15)\u001b[0m \u001b[31mdiagnose(0.15)\u001b[0m \u001b[32mdiseases(0.84)\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'assess' (index 1)\n",
      "====================================================================================================\n",
      "\n",
      "No valid PMI scores\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'symptoms' (index 2)\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "doctors assess \u001b[1m\u001b[36msymptoms\u001b[0m to \u001b[32mdiagnose(0.00)\u001b[0m \u001b[31mdiseases(-0.26)\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'to' (index 3)\n",
      "====================================================================================================\n",
      "\n",
      "No valid PMI scores\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'diagnose' (index 4)\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "doctors assess symptoms to \u001b[1m\u001b[36mdiagnose\u001b[0m \u001b[31mdiseases(0.15)\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'diseases' (index 5)\n",
      "====================================================================================================\n",
      "\n",
      "No valid PMI scores\n",
      "\n",
      "\n",
      "\n",
      "########################################################################################################################\n",
      "VISUALIZATIONS FOR SENTENCE:\n",
      "  'artificial intelligence transforms modern industries'\n",
      "########################################################################################################################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'artificial' (index 0)\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36martificial\u001b[0m \u001b[31mintelligence(0.86)\u001b[0m transforms modern industries\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'intelligence' (index 1)\n",
      "====================================================================================================\n",
      "\n",
      "No valid PMI scores\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'transforms' (index 2)\n",
      "====================================================================================================\n",
      "\n",
      "No valid PMI scores\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'modern' (index 3)\n",
      "====================================================================================================\n",
      "\n",
      "No valid PMI scores\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'industries' (index 4)\n",
      "====================================================================================================\n",
      "\n",
      "No valid PMI scores\n",
      "\n",
      "\n",
      "\n",
      "########################################################################################################################\n",
      "VISUALIZATIONS FOR SENTENCE:\n",
      "  'children love sweet ice cream on warm summer days'\n",
      "########################################################################################################################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'children' (index 0)\n",
      "====================================================================================================\n",
      "\n",
      "No valid PMI scores\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'love' (index 1)\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "children \u001b[1m\u001b[36mlove\u001b[0m sweet ice \u001b[32mcream(0.38)\u001b[0m \u001b[32mon(0.18)\u001b[0m \u001b[33mwarm(0.00)\u001b[0m \u001b[31msummer(-0.49)\u001b[0m \u001b[33mdays(0.07)\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'sweet' (index 2)\n",
      "====================================================================================================\n",
      "\n",
      "No valid PMI scores\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'ice' (index 3)\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "children love sweet \u001b[1m\u001b[36mice\u001b[0m \u001b[32mcream(1.74)\u001b[0m \u001b[31mon(-0.08)\u001b[0m warm summer days\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'cream' (index 4)\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "children love sweet ice \u001b[1m\u001b[36mcream\u001b[0m \u001b[31mon(0.00)\u001b[0m warm \u001b[31msummer(0.00)\u001b[0m \u001b[31mdays(0.00)\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'on' (index 5)\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "children love sweet ice cream \u001b[1m\u001b[36mon\u001b[0m warm summer \u001b[31mdays(0.51)\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'warm' (index 6)\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "children love sweet ice cream on \u001b[1m\u001b[36mwarm\u001b[0m summer \u001b[31mdays(0.00)\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'summer' (index 7)\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "children love sweet ice cream on warm \u001b[1m\u001b[36msummer\u001b[0m \u001b[31mdays(-0.22)\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'days' (index 8)\n",
      "====================================================================================================\n",
      "\n",
      "No valid PMI scores\n",
      "\n",
      "\n",
      "\n",
      "########################################################################################################################\n",
      "VISUALIZATIONS FOR SENTENCE:\n",
      "  'plants require sunlight and water to grow'\n",
      "########################################################################################################################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'plants' (index 0)\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mplants\u001b[0m \u001b[32mrequire(0.62)\u001b[0m \u001b[31msunlight(0.00)\u001b[0m \u001b[31mand(0.00)\u001b[0m \u001b[31mwater(0.00)\u001b[0m \u001b[31mto(0.00)\u001b[0m \u001b[31mgrow(0.00)\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'require' (index 1)\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "plants \u001b[1m\u001b[36mrequire\u001b[0m \u001b[31msunlight(-0.07)\u001b[0m \u001b[32mand(0.86)\u001b[0m \u001b[31mwater(0.00)\u001b[0m \u001b[31mto(-0.04)\u001b[0m \u001b[31mgrow(-0.19)\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'sunlight' (index 2)\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "plants require \u001b[1m\u001b[36msunlight\u001b[0m \u001b[32mand(1.00)\u001b[0m water \u001b[31mto(-0.01)\u001b[0m \u001b[31mgrow(-0.03)\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'and' (index 3)\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "plants require sunlight \u001b[1m\u001b[36mand\u001b[0m water to \u001b[31mgrow(2.74)\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'water' (index 4)\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "plants require \u001b[31msunlight(0.07)\u001b[0m and \u001b[1m\u001b[36mwater\u001b[0m \u001b[32mto(0.42)\u001b[0m \u001b[31mgrow(0.00)\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'to' (index 5)\n",
      "====================================================================================================\n",
      "\n",
      "No valid PMI scores\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'grow' (index 6)\n",
      "====================================================================================================\n",
      "\n",
      "No valid PMI scores\n",
      "\n",
      "\n",
      "\n",
      "########################################################################################################################\n",
      "VISUALIZATIONS FOR SENTENCE:\n",
      "  'the government announced new policies to support healthcare'\n",
      "########################################################################################################################\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'the' (index 0)\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mthe\u001b[0m government announced new \u001b[32mpolicies(0.58)\u001b[0m \u001b[31mto(0.10)\u001b[0m support healthcare\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'government' (index 1)\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "the \u001b[1m\u001b[36mgovernment\u001b[0m announced \u001b[32mnew(0.32)\u001b[0m \u001b[31mpolicies(0.00)\u001b[0m \u001b[32mto(0.38)\u001b[0m support healthcare\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'announced' (index 2)\n",
      "====================================================================================================\n",
      "\n",
      "No valid PMI scores\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'new' (index 3)\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "the government announced \u001b[1m\u001b[36mnew\u001b[0m policies \u001b[31mto(0.25)\u001b[0m support healthcare\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'policies' (index 4)\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "the government announced new \u001b[1m\u001b[36mpolicies\u001b[0m to support \u001b[31mhealthcare(0.86)\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'to' (index 5)\n",
      "====================================================================================================\n",
      "\n",
      "No valid PMI scores\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'support' (index 6)\n",
      "====================================================================================================\n",
      "\n",
      "No valid PMI scores\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Anchor word: 'healthcare' (index 7)\n",
      "====================================================================================================\n",
      "\n",
      "No valid PMI scores\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VISUALIZATION FOR  SENTENCES\n",
    "\n",
    "for sentence, anchor_dict in all_sentence_results.items():\n",
    "\n",
    "    print(\"\\n\" + \"#\"*120)\n",
    "    print(f\"VISUALIZATIONS FOR SENTENCE:\\n  '{sentence}'\")\n",
    "    print(\"#\"*120 + \"\\n\")\n",
    "\n",
    "    for anchor_word, info in anchor_dict.items():\n",
    "        anchor_idx = info[\"anchor_idx\"]\n",
    "        pmi_scores = info[\"pmi_scores\"]\n",
    "\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(f\"Anchor word: '{anchor_word}' (index {anchor_idx})\")\n",
    "        print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "        visualize_pmi(sentence, pmi_scores, anchor_idx)\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection on PMI Results and Explanation Quality\n",
    "\n",
    "When we experimented with more sentences, different anchor words, and different numbers of model responses, I observed that the PMI values changed noticeably depending on how often the model regenerated particular word pairs. Words with clear semantic connections, such as doctor and diseases or ice and cream, tended to show higher PMI, while function words like to, and, or new often produced no valid PMI because the model did not reproduce them consistently. Increasing the number of responses made the PMI estimates more stable and less random.\n",
    "\n",
    "The explanation behind PMI is that it reflects how strongly the model associates two words by comparing the probability of generating them together versus independently. A high PMI therefore indicates that the model repeatedly regenerates those words in relation to each other, revealing an underlying learned association.\n",
    "\n",
    "Overall, PMI gives a simple and intuitive explanation because it highlights which words the model considers related. However, it is also limited, since many anchor words do not produce valid PMI, the results are sensitive to sampling noise, and the method does not explain the model’s internal reasoning processes. PMI is therefore helpful for intuition but should not be viewed as a complete explanation of model behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of the PMI-Based Explanation Method\n",
    "\n",
    "While PMI gives a simple way to estimate dependencies between words using a generative LLM, the method comes with several important limitations:\n",
    "\n",
    "**Exact word matching is too rigid**\n",
    "\n",
    "The method only counts a match if the model outputs the exact same word.\n",
    "But LLMs often generate synonyms or variations (e.g., kids vs children), which leads to underestimating true semantic relationships.\n",
    "\n",
    "**Naïve tokenization creates noise**\n",
    "\n",
    "Because the current implementation uses a simple .split(), the method struggles with contractions, punctuation, hyphenated words, and multi word expressions like ice cream or New York.\n",
    "This reduces the accuracy of the PMI associations.\n",
    "\n",
    "**Low sample size leads to unstable probabilities**\n",
    "\n",
    "With around 20 generated completions per masked pair, estimates of P(x), P(y), and P(x,y) can be noisy.\n",
    "A few lucky or unlucky generations can shift PMI ranks significantly.\n",
    "\n",
    "**PMI only captures pairwise relationships**\n",
    "\n",
    "Natural language meaning is often determined by interactions between several words or phrases.\n",
    "PMI cannot model multi word dependencies, syntax, or context beyond two word associations.\n",
    "\n",
    "**LLM biases influence the results**\n",
    "\n",
    "PMI reflects the model’s training distribution and biases.\n",
    "High PMI may reflect frequency biases in training data rather than genuine dependency in the sentence.\n",
    "\n",
    "Overall, PMI gives a simple and interpretable approximation of word dependencies, but its accuracy is limited by tokenization, sampling noise, synonym variation, masking artifacts, and the behavioral nature of the method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bonus Exercises\n",
    "### 5.1 Language pre-processing. \n",
    "In this exercise, we only lower the letters and split sentences into words; there's much more to do to pre-process the language. For example, contractions (*I'll*, *She's*, *world's*), suffix and prefix, compound words (*hard-working*). It's called word tokenization in NLP, and there are some Python packages that can do such work for us, e.g. [*TextBlob*](https://textblob.readthedocs.io/en/dev/). \n",
    "\n",
    "\n",
    "### 5.2 Better word matching\n",
    "In the above example of\n",
    "> Tokyo is the capital of Japan and a popular metropolis in the world.\n",
    "\n",
    "GenAI never gives the specific word 'metropolis' when masking it out; instead, sometimes it provides words like 'city', which is not the same word but has a similar meaning. Instead of measuring the exact matching of certain words (i.e. 0 or 1), we can also measure the similarity of two words, e.g. the cosine similarity in word embedding, which ranges from 0 to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
