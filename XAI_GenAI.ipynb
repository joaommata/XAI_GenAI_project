{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Responsible AI: XAI GenAI project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Background\n",
    "\n",
    "\n",
    "\n",
    "Based on the previous lessons on explainability, post-hoc methods are used to explain the model, such as saliency map, SmoothGrad, LRP, LIME, and SHAP. Take LRP (Layer Wise Relevance Propagation) as an example; it highlights the most relevant pixels to obtain a prediction of the class \"cat\" by backpropagating the relevance. (image source: [Montavon et. al (2016)](https://giorgiomorales.github.io/Layer-wise-Relevance-Propagation-in-Pytorch/))\n",
    "\n",
    "<!-- %%[markdown] -->\n",
    "![LRP example](images/catLRP.jpg)\n",
    "\n",
    "Another example is about text sentiment classification, here we show a case of visualizing the importance of words given the prediction of 'positive':\n",
    "\n",
    "![text example](images/textGradL2.png)\n",
    "\n",
    "where the words highlight with darker colours indicate to be more critical in predicting the sentence to be 'positive' in sentiment.\n",
    "More examples could be found [here](http://34.160.227.66/?models=sst2-tiny&dataset=sst_dev&hidden_modules=Explanations_Attention&layout=default).\n",
    "\n",
    "Both cases above require the class or the prediction of the model. But:\n",
    "\n",
    "***How do you explain a model that does not predict but generates?***\n",
    "\n",
    "In this project, we will work on explaining the generative model based on the dependency between words. We will first look at a simple example, and using Point-wise Mutual Information (PMI) to compute the saliency map of the sentence. After that we will contruct the expereiment step by step, followed by exercises and questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A simple example to start with\n",
    "Given a sample sentence: \n",
    "> *Tokyo is the capital city of Japan.* \n",
    "\n",
    "We are going to explain this sentence by finding the dependency using a saliency map between words.\n",
    "The dependency of two words in the sentence could be measured by [Point-wise mutual information (PMI)](https://en.wikipedia.org/wiki/Pointwise_mutual_information): \n",
    "\n",
    "\n",
    "Mask two words out, e.g. \n",
    "> \\[MASK-1\\] is the captial city of \\[MASK-2\\].\n",
    "\n",
    "\n",
    "Ask the generative model to fill in the sentence 10 times, and we have:\n",
    "\n",
    "| MASK-1      | MASK-2 |\n",
    "| ----------- | ----------- |\n",
    "|    tokyo   |     japan   |\n",
    "|  paris  |     france    |\n",
    "|  london  |     england    |\n",
    "|  paris  |     france    |\n",
    "|  beijing |  china |\n",
    "|    tokyo   |     japan   |\n",
    "|  paris  |     france    |\n",
    "|  paris  |     france    |\n",
    "|  london  |     england    |\n",
    "|  beijing |  china |\n",
    "\n",
    "PMI is calculated by: \n",
    "\n",
    "$PMI(x,y)=log_2⁡ \\frac{p(\\{x,y\\}| s-\\{x,y\\})}{P(\\{x\\}|s-\\{x,y\\})P(\\{y\\}|s-\\{x,y\\})}$\n",
    "\n",
    "where $x$, $y$ represents the words that we masked out, $s$ represents the setence, and $s-\\{x,y\\}$ represents the sentences tokens after removing the words $x$ and $y$.\n",
    "\n",
    "In this example we have $PMI(Tokyo, capital) = log_2 \\frac{0.2}{0.2 * 0.2} = 2.32$\n",
    "\n",
    "Select an interesting word in the sentences; we can now compute the PMI between all other words and the chosen word using the generative model:\n",
    "(Here, we use a longer sentence and run 20 responses per word.)\n",
    "![](images/resPMI.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparation\n",
    "### 2.1 Conda enviroment\n",
    "\n",
    "```\n",
    "conda env create -f environment.yml\n",
    "conda activate xai_llm\n",
    "```\n",
    "\n",
    "\n",
    "### 2.2 Download the offline LLM\n",
    "\n",
    "We use the offline LLM model from hugging face. It's approximately 5 GB.\n",
    "Download it using the comman below, and save it under `./models/`.\n",
    "```\n",
    "huggingface-cli download TheBloke/openchat-3.5-0106-GGUF openchat-3.5-0106.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n",
    "# credit to https://huggingface.co/TheBloke/openchat-3.5-0106-GGUF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mask the sentence and get the responses from LLM\n",
    "### 3.1 Get the input sentence\n",
    "\n",
    "**Remember to change the anchor word index when changing the input sentence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  doctors assess symptoms to diagnose diseases\n"
     ]
    }
   ],
   "source": [
    "# Removed for consistency, so that every run uses the same sentence\n",
    "# def get_input():\n",
    "    # ideally this reads inputs from a file, now it just takes an input\n",
    "    #return input(\"Enter a sentence: \")\n",
    "    \n",
    "# Cell 23 - Reset the sentence\n",
    "sentence = \"doctors assess symptoms to diagnose diseases\"\n",
    "\n",
    "anchor_word_idx = 0 # the index of the interested word\n",
    "prompts_per_word = 20 # number of generated responses  \n",
    "\n",
    "#sentence = get_input()\n",
    "print(\"Sentence: \", sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: openchat\n"
     ]
    }
   ],
   "source": [
    "from models.ChatModel import ChatModel\n",
    "model_name = \"openchat\"\n",
    "model = ChatModel(model_name)\n",
    "print(f\"Model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Run the prompts and get all the responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] symptoms to diagnose diseases:   5%|▌         | 1/20 [00:14<04:35, 14.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'symptoms', 'to', 'diagnose', 'diseases'] ['', 'doctors', 'use', 'various', 'tests']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] symptoms to diagnose diseases:  70%|███████   | 14/20 [00:26<00:06,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'symptoms', 'to', 'diagnose', 'diseases'] ['fever', 'and', 'headache', 'symptoms', 'help', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] symptoms to diagnose diseases:  80%|████████  | 16/20 [00:28<00:03,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'symptoms', 'to', 'diagnose', 'diseases'] ['mri', 'scans', 'show', 'clear', '[mask]', '[mask]', 'for', 'accurate', 'disease', 'diagnosis']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] symptoms to diagnose diseases: 100%|██████████| 20/20 [00:30<00:00,  1.53s/it]\n",
      "Input: [MASK] assess [MASK] to diagnose diseases:   5%|▌         | 1/20 [00:00<00:17,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['doctors', 'evaluate', 'patients', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess [MASK] to diagnose diseases:  15%|█▌        | 3/20 [00:02<00:14,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'patients', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess [MASK] to diagnose diseases:  25%|██▌       | 5/20 [00:04<00:13,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'uses', 'a', 'stethoscope', 'to', 'assess', 'lung', 'sounds', 'and', 'diagnoses', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess [MASK] to diagnose diseases:  35%|███▌      | 7/20 [00:05<00:11,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['doctors', 'examine', '[patients]', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess [MASK] to diagnose diseases:  40%|████      | 8/20 [00:06<00:09,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'the', 'patient', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess [MASK] to diagnose diseases:  50%|█████     | 10/20 [00:08<00:07,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'patient', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess [MASK] to diagnose diseases:  60%|██████    | 12/20 [00:09<00:05,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'patients', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess [MASK] to diagnose diseases:  70%|███████   | 14/20 [00:11<00:04,  1.21it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "pop from empty list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m             all_replacements\u001b[38;5;241m.\u001b[39mappend(replacements)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_replacements\n\u001b[0;32m---> 25\u001b[0m all_responses \u001b[38;5;241m=\u001b[39m \u001b[43mrun_prompts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchor_word_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts_per_word\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[60], line 18\u001b[0m, in \u001b[0;36mrun_prompts\u001b[0;34m(model, sentence, anchor_idx, prompts_per_word)\u001b[0m\n\u001b[1;32m     14\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[1;32m     15\u001b[0m     prefix_prompt(prompt),\n\u001b[1;32m     16\u001b[0m )\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m---> 18\u001b[0m     replacement \u001b[38;5;241m=\u001b[39m \u001b[43mget_replacements\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m replacement:\n\u001b[1;32m     20\u001b[0m         replacements\u001b[38;5;241m.\u001b[39mappend(replacement)\n",
      "File \u001b[0;32m~/Desktop/DTU/ResponsibleAI/XAI_GenAI_project/tools/evaluate_response.py:61\u001b[0m, in \u001b[0;36mget_replacements\u001b[0;34m(original_sentence, replaced_sentence)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m replaced_word \u001b[38;5;241m!=\u001b[39m stop_word:\n\u001b[1;32m     60\u001b[0m     replacements[replacement_idx]\u001b[38;5;241m.\u001b[39mappend(replaced_word)\n\u001b[0;32m---> 61\u001b[0m     replaced_word \u001b[38;5;241m=\u001b[39m \u001b[43mreplaced_words\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m replaced_words\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, replaced_word)\n\u001b[1;32m     64\u001b[0m replacement_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from empty list"
     ]
    }
   ],
   "source": [
    "from tools.command_generator import generate_prompts, prefix_prompt\n",
    "from tools.evaluate_response import get_replacements\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_prompts(model, sentence, anchor_idx, prompts_per_word=20):\n",
    "    prompts = generate_prompts(sentence, anchor_idx)\n",
    "    all_replacements = []\n",
    "    for prompt in prompts:\n",
    "        replacements = []\n",
    "        for _ in tqdm(\n",
    "            range(prompts_per_word),\n",
    "            desc=f\"Input: {prompt}\",\n",
    "        ):\n",
    "            response = model.get_response(\n",
    "                prefix_prompt(prompt),\n",
    "            ).strip()\n",
    "            if response:\n",
    "                replacement = get_replacements(prompt, response)\n",
    "                if replacement:\n",
    "                    replacements.append(replacement)\n",
    "        if len(replacements) > 0:\n",
    "            all_replacements.append(replacements)\n",
    "    return all_replacements\n",
    "\n",
    "all_responses = run_prompts(model, sentence, anchor_word_idx, prompts_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['', ''],\n",
       "  ['flu', '[mask]'],\n",
       "  ['', ''],\n",
       "  ['', ''],\n",
       "  ['', ''],\n",
       "  ['flulike', '[mask] [mask]'],\n",
       "  ['', ''],\n",
       "  ['common', '[mask]'],\n",
       "  ['patients', 'frequently exhibit classic [mask]'],\n",
       "  ['flulike', '[mask]'],\n",
       "  ['mumps', 'mumps'],\n",
       "  ['flu', '[mask]'],\n",
       "  ['fever', 'and chills'],\n",
       "  ['flulike', '[mask]'],\n",
       "  ['the', 'flu'],\n",
       "  ['', ''],\n",
       "  ['flulike', '[mask]'],\n",
       "  ['', ''],\n",
       "  ['flulike', ''],\n",
       "  ['', '']]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize responses\n",
    "all_responses[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load responses\n",
    "import json\n",
    "input_file = \"responses.json\"\n",
    "with open(input_file, \"r\") as f:\n",
    "    all_responses = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 EXERCISE: compute the PMI for each word\n",
    "\n",
    "$PMI(x,y)=log_2⁡ \\frac{p(\\{x,y\\}| s-\\{x,y\\})}{P(\\{x\\}|s-\\{x,y\\})P(\\{y\\}|s-\\{x,y\\})}$\n",
    "\n",
    "* Compute the $P(x)$, $P(y)$ and $P(x,y)$ first and print it out.\n",
    "* Compute the PMI for each word.\n",
    "* Visualize the result by coloring. Tips: you might need to normalize the result first. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from termcolor import colored\n",
    "\n",
    "def compute_pmi(sentence, all_responses, anchor_idx):\n",
    "    \"\"\"Compute PMI between anchor word and each other word.\"\"\"\n",
    "    words = sentence.lower().split()\n",
    "    anchor_word = words[anchor_idx]\n",
    "    pmi_scores = {}\n",
    "    \n",
    "    for other_idx in range(len(words)):\n",
    "        if other_idx == anchor_idx:\n",
    "            continue\n",
    "        \n",
    "        # Get pattern index (skips anchor position)\n",
    "        pattern_idx = other_idx if other_idx < anchor_idx else other_idx - 1\n",
    "        if pattern_idx >= len(all_responses):\n",
    "            continue\n",
    "            \n",
    "        responses = all_responses[pattern_idx]\n",
    "        if not responses:\n",
    "            continue\n",
    "        \n",
    "        # Extract anchor and other word replacements\n",
    "        anchor_replacements = [r[0].lower() for r in responses if len(r) == 2]\n",
    "        other_replacements = [r[1].lower() for r in responses if len(r) == 2]\n",
    "        total = len(anchor_replacements)\n",
    "        \n",
    "        # Calculate probabilities\n",
    "        count_x = sum(w == anchor_word for w in anchor_replacements)\n",
    "        count_y = sum(w == words[other_idx] for w in other_replacements)\n",
    "        count_xy = sum(anchor_replacements[i] == anchor_word and \n",
    "                      other_replacements[i] == words[other_idx] \n",
    "                      for i in range(total))\n",
    "        \n",
    "        P_x = count_x / total\n",
    "        P_y = count_y / total\n",
    "        P_xy = count_xy / total\n",
    "        \n",
    "        # Calculate PMI\n",
    "        if P_x > 0 and P_y > 0 and P_xy > 0:\n",
    "            pmi = math.log2(P_xy / (P_x * P_y))\n",
    "        else:\n",
    "            pmi = float('-inf')\n",
    "        \n",
    "        pmi_scores[other_idx] = {'word': words[other_idx], 'pmi': pmi, \n",
    "                                  'P_x': P_x, 'P_y': P_y, 'P_xy': P_xy}\n",
    "    \n",
    "    return pmi_scores\n",
    "\n",
    "def visualize_pmi(sentence, pmi_scores, anchor_idx):\n",
    "    \"\"\"Visualize PMI with colored words.\"\"\"\n",
    "    words = sentence.split()\n",
    "    \n",
    "    # Normalize PMI values\n",
    "    valid_pmis = [s['pmi'] for s in pmi_scores.values() if s['pmi'] != float('-inf')]\n",
    "    if not valid_pmis:\n",
    "        print(\"No valid PMI scores\")\n",
    "        return\n",
    "    \n",
    "    min_pmi, max_pmi = min(valid_pmis), max(valid_pmis)\n",
    "    pmi_range = max_pmi - min_pmi if max_pmi != min_pmi else 1\n",
    "    \n",
    "    # Color each word\n",
    "    colored_words = []\n",
    "    for i, word in enumerate(words):\n",
    "        if i == anchor_idx:\n",
    "            colored_words.append(colored(word, 'cyan', attrs=['bold']))\n",
    "        elif i in pmi_scores:\n",
    "            pmi = pmi_scores[i]['pmi']\n",
    "            if pmi != float('-inf'):\n",
    "                norm = (pmi - min_pmi) / pmi_range\n",
    "                color = 'green' if norm > 0.66 else 'yellow' if norm > 0.33 else 'red'\n",
    "                colored_words.append(colored(f\"{word}({pmi:.2f})\", color))\n",
    "            else:\n",
    "                colored_words.append(word)\n",
    "        else:\n",
    "            colored_words.append(word)\n",
    "    \n",
    "    print(\"\\n\" + \" \".join(colored_words) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor word: 'doctors'\n",
      "\n",
      "assess          PMI=   -inf  P(x)=0.000 P(y)=0.000 P(xy)=0.000\n",
      "symptoms        PMI=  1.737  P(x)=0.300 P(y)=0.200 P(xy)=0.200\n",
      "to              PMI=  1.152  P(x)=0.100 P(y)=0.450 P(xy)=0.100\n",
      "diagnose        PMI=  0.322  P(x)=0.100 P(y)=0.800 P(xy)=0.100\n",
      "diseases        PMI=  0.737  P(x)=0.200 P(y)=0.300 P(xy)=0.100\n",
      "\n",
      "\u001b[1m\u001b[36mdoctors\u001b[0m assess \u001b[32msymptoms(1.74)\u001b[0m \u001b[33mto(1.15)\u001b[0m \u001b[31mdiagnose(0.32)\u001b[0m \u001b[31mdiseases(0.74)\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute PMI scores\n",
    "pmi_scores = compute_pmi(sentence, all_responses, anchor_word_idx)\n",
    "\n",
    "# Print results\n",
    "words = sentence.lower().split()\n",
    "print(f\"Anchor word: '{words[anchor_word_idx]}'\\n\")\n",
    "for idx in sorted(pmi_scores.keys()):\n",
    "    data = pmi_scores[idx]\n",
    "    print(f\"{data['word']:<15} PMI={data['pmi']:7.3f}  \"\n",
    "          f\"P(x)={data['P_x']:.3f} P(y)={data['P_y']:.3f} P(xy)={data['P_xy']:.3f}\")\n",
    "\n",
    "# Visualize with colors\n",
    "visualize_pmi(sentence, pmi_scores, anchor_word_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PMI Results Interpretation (Higher PMI = stronger association)\n",
    "\n",
    "#### Results for \"doctors assess symptoms to diagnose diseases\"\n",
    "- The visualization shows that \"symptoms\" has the strongest semantic bond with \"doctors\" in this sentence\n",
    "\n",
    "**High PMI: Strong Dependency**\n",
    "- **symptoms (1.74)**: had the highest association with \"doctors\", this means that when both are amsked the model frequently generates them to fill the masked words.\n",
    "\n",
    "**Medium PMI: Moderate Dependency**  \n",
    "- **to (1.15)**: Moderate association.\n",
    "\n",
    "**Low PMI: Weak Dependency**\n",
    "- **diseases (0.74)**: Predictable from context but not uniquely tied to \"doctors\"\n",
    "- **diagnose (0.32)**: Despite being less associated with the word doctor (pmi=0.322), it is very predictable (P(y)=0.80). This means that the word itself (\"diagnose\") is very frequent but paired with alternatives to \"doctors\" (physicians, clinicians)\n",
    "\n",
    "**Negative PMI: No Dependency**\n",
    "- **assess (-inf)**: this means the model never generated \"doctors\" when both were masked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. EXERCISE: Try more examples; maybe come up with your own. Report the results.\n",
    "\n",
    "* Try to come up with more examples and, change the anchor word/number of responses, and observe the results. What does the explanation mean? Do you think it's a nice explanation? Why and why not? \n",
    "* What's the limitation of the current method? When does the method fail to explain? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bonus Exercises\n",
    "### 5.1 Language pre-processing. \n",
    "In this exercise, we only lower the letters and split sentences into words; there's much more to do to pre-process the language. For example, contractions (*I'll*, *She's*, *world's*), suffix and prefix, compound words (*hard-working*). It's called word tokenization in NLP, and there are some Python packages that can do such work for us, e.g. [*TextBlob*](https://textblob.readthedocs.io/en/dev/). \n",
    "\n",
    "\n",
    "### 5.2 Better word matching\n",
    "In the above example of\n",
    "> Tokyo is the capital of Japan and a popular metropolis in the world.\n",
    "\n",
    "GenAI never gives the specific word 'metropolis' when masking it out; instead, sometimes it provides words like 'city', which is not the same word but has a similar meaning. Instead of measuring the exact matching of certain words (i.e. 0 or 1), we can also measure the similarity of two words, e.g. the cosine similarity in word embedding, which ranges from 0 to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
