{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Responsible AI: XAI GenAI project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Background\n",
    "\n",
    "\n",
    "\n",
    "Based on the previous lessons on explainability, post-hoc methods are used to explain the model, such as saliency map, SmoothGrad, LRP, LIME, and SHAP. Take LRP (Layer Wise Relevance Propagation) as an example; it highlights the most relevant pixels to obtain a prediction of the class \"cat\" by backpropagating the relevance. (image source: [Montavon et. al (2016)](https://giorgiomorales.github.io/Layer-wise-Relevance-Propagation-in-Pytorch/))\n",
    "\n",
    "<!-- %%[markdown] -->\n",
    "![LRP example](images/catLRP.jpg)\n",
    "\n",
    "Another example is about text sentiment classification, here we show a case of visualizing the importance of words given the prediction of 'positive':\n",
    "\n",
    "![text example](images/textGradL2.png)\n",
    "\n",
    "where the words highlight with darker colours indicate to be more critical in predicting the sentence to be 'positive' in sentiment.\n",
    "More examples could be found [here](http://34.160.227.66/?models=sst2-tiny&dataset=sst_dev&hidden_modules=Explanations_Attention&layout=default).\n",
    "\n",
    "Both cases above require the class or the prediction of the model. But:\n",
    "\n",
    "***How do you explain a model that does not predict but generates?***\n",
    "\n",
    "In this project, we will work on explaining the generative model based on the dependency between words. We will first look at a simple example, and using Point-wise Mutual Information (PMI) to compute the saliency map of the sentence. After that we will contruct the expereiment step by step, followed by exercises and questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A simple example to start with\n",
    "Given a sample sentence: \n",
    "> *Tokyo is the capital city of Japan.* \n",
    "\n",
    "We are going to explain this sentence by finding the dependency using a saliency map between words.\n",
    "The dependency of two words in the sentence could be measured by [Point-wise mutual information (PMI)](https://en.wikipedia.org/wiki/Pointwise_mutual_information): \n",
    "\n",
    "\n",
    "Mask two words out, e.g. \n",
    "> \\[MASK-1\\] is the captial city of \\[MASK-2\\].\n",
    "\n",
    "\n",
    "Ask the generative model to fill in the sentence 10 times, and we have:\n",
    "\n",
    "| MASK-1      | MASK-2 |\n",
    "| ----------- | ----------- |\n",
    "|    tokyo   |     japan   |\n",
    "|  paris  |     france    |\n",
    "|  london  |     england    |\n",
    "|  paris  |     france    |\n",
    "|  beijing |  china |\n",
    "|    tokyo   |     japan   |\n",
    "|  paris  |     france    |\n",
    "|  paris  |     france    |\n",
    "|  london  |     england    |\n",
    "|  beijing |  china |\n",
    "\n",
    "PMI is calculated by: \n",
    "\n",
    "$PMI(x,y)=log_2⁡ \\frac{p(\\{x,y\\}| s-\\{x,y\\})}{P(\\{x\\}|s-\\{x,y\\})P(\\{y\\}|s-\\{x,y\\})}$\n",
    "\n",
    "where $x$, $y$ represents the words that we masked out, $s$ represents the setence, and $s-\\{x,y\\}$ represents the sentences tokens after removing the words $x$ and $y$.\n",
    "\n",
    "In this example we have $PMI(Tokyo, capital) = log_2 \\frac{0.2}{0.2 * 0.2} = 2.32$\n",
    "\n",
    "Select an interesting word in the sentences; we can now compute the PMI between all other words and the chosen word using the generative model:\n",
    "(Here, we use a longer sentence and run 20 responses per word.)\n",
    "![](images/resPMI.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparation\n",
    "### 2.1 Conda enviroment\n",
    "\n",
    "```\n",
    "conda env create -f environment.yml\n",
    "conda activate xai_llm\n",
    "```\n",
    "\n",
    "\n",
    "### 2.2 Download the offline LLM\n",
    "\n",
    "We use the offline LLM model from hugging face. It's approximately 5 GB.\n",
    "Download it using the comman below, and save it under `./models/`.\n",
    "```\n",
    "huggingface-cli download TheBloke/openchat-3.5-0106-GGUF openchat-3.5-0106.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n",
    "# credit to https://huggingface.co/TheBloke/openchat-3.5-0106-GGUF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mask the sentence and get the responses from LLM\n",
    "### 3.1 Get the input sentence\n",
    "\n",
    "**Remember to change the anchor word index when changing the input sentence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  doctors assess symptoms to diagnose diseases\n"
     ]
    }
   ],
   "source": [
    "# Removed for consistency, so that every run uses the same sentence\n",
    "# def get_input():\n",
    "    # ideally this reads inputs from a file, now it just takes an input\n",
    "    #return input(\"Enter a sentence: \")\n",
    "    \n",
    "# Cell 23 - Reset the sentence\n",
    "sentence = \"doctors assess symptoms to diagnose diseases\"\n",
    "\n",
    "anchor_word_idx = 0 # the index of the interested word\n",
    "prompts_per_word = 20 # number of generated responses  \n",
    "\n",
    "#sentence = get_input()\n",
    "print(\"Sentence: \", sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: openchat\n"
     ]
    }
   ],
   "source": [
    "from models.ChatModel import ChatModel\n",
    "model_name = \"openchat\"\n",
    "model = ChatModel(model_name)\n",
    "print(f\"Model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Run the prompts and get all the responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] symptoms to diagnose diseases:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] symptoms to diagnose diseases:  20%|██        | 4/20 [00:21<01:10,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'symptoms', 'to', 'diagnose', 'diseases'] ['medical', 'professionals', 'use', '[signs]', 'and', '[indicators]', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] symptoms to diagnose diseases: 100%|██████████| 20/20 [00:51<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'symptoms', 'to', 'diagnose', 'diseases'] ['fever', 'chills', 'and', '[mask]', 'symptoms', 'help', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess [MASK] to diagnose diseases:  10%|█         | 2/20 [00:04<00:42,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'the', 'patient', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess [MASK] to diagnose diseases:  15%|█▌        | 3/20 [00:07<00:41,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'physician', 'assesses', 'medical', 'history', 'and', 'physical', 'examination', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess [MASK] to diagnose diseases:  25%|██▌       | 5/20 [00:10<00:29,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess [MASK] to diagnose diseases:  30%|███       | 6/20 [00:12<00:27,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'the', 'patient', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess [MASK] to diagnose diseases:  45%|████▌     | 9/20 [00:17<00:19,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'the', 'patient', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess [MASK] to diagnose diseases:  50%|█████     | 10/20 [00:19<00:18,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'the', 'patient', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess [MASK] to diagnose diseases:  60%|██████    | 12/20 [00:23<00:14,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess [MASK] to diagnose diseases:  65%|██████▌   | 13/20 [00:25<00:12,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'physician', 'assesses', 'patients', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess [MASK] to diagnose diseases:  70%|███████   | 14/20 [00:27<00:10,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess [MASK] to diagnose diseases:  95%|█████████▌| 19/20 [00:35<00:01,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'the', 'patient', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess [MASK] to diagnose diseases: 100%|██████████| 20/20 [00:37<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', '[mask]', 'to', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'the', 'patient', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:   5%|▌         | 1/20 [00:03<00:58,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'and', 'diagnoses', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  15%|█▌        | 3/20 [00:07<00:38,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['she', 'assesses', 'symptoms', 'to', '[diagnose]', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  20%|██        | 4/20 [00:08<00:32,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['she', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  25%|██▌       | 5/20 [00:10<00:31,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['the', 'physician', 'assesses', 'symptoms', 'to', '[diagnose]', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  45%|████▌     | 9/20 [00:18<00:21,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['the', 'physician', 'assesses', 'symptoms', 'to', '[mask]', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  50%|█████     | 10/20 [00:19<00:18,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'and', 'diagnoses', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  55%|█████▌    | 11/20 [00:21<00:17,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['the', 'medical', 'professional', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  70%|███████   | 14/20 [00:26<00:10,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'and', 'diagnoses', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess symptoms [MASK] diagnose diseases:  75%|███████▌  | 15/20 [00:29<00:10,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', '[mask]', 'diagnose', 'diseases'] ['healthcare', 'professionals', 'assess', 'symptoms', 'in', 'order', 'to', '[diagnose]', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess symptoms [MASK] diagnose diseases: 100%|██████████| 20/20 [00:37<00:00,  1.89s/it]\n",
      "Input: [MASK] assess symptoms to [MASK] diseases:   5%|▌         | 1/20 [00:03<00:58,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', '[mask]', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess symptoms to [MASK] diseases:  15%|█▌        | 3/20 [00:06<00:34,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', '[mask]', 'diseases'] ['the', 'physician', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess symptoms to [MASK] diseases:  70%|███████   | 14/20 [00:23<00:09,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', '[mask]', 'diseases'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'diseases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess symptoms to [MASK] diseases: 100%|██████████| 20/20 [00:32<00:00,  1.64s/it]\n",
      "Input: [MASK] assess symptoms to diagnose [MASK]:   5%|▌         | 1/20 [00:02<00:53,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['a', 'physician', 'assesses', 'symptoms', 'to', 'diagnose', 'a', 'condition']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess symptoms to diagnose [MASK]:  10%|█         | 2/20 [00:05<00:48,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'medical', 'professional', 'assesses', 'symptoms', 'to', 'diagnose', 'a', 'potential', 'illness', 'or', 'condition']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess symptoms to diagnose [MASK]:  15%|█▌        | 3/20 [00:07<00:39,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'illness']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess symptoms to diagnose [MASK]:  30%|███       | 6/20 [00:12<00:27,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'a', 'potential', 'illness']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess symptoms to diagnose [MASK]:  35%|███▌      | 7/20 [00:14<00:25,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'a', 'condition']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess symptoms to diagnose [MASK]:  40%|████      | 8/20 [00:16<00:22,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'conditions']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess symptoms to diagnose [MASK]:  45%|████▌     | 9/20 [00:18<00:20,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'physician', 'assesses', 'symptoms', 'to', 'diagnose', 'the', 'illness']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess symptoms to diagnose [MASK]:  75%|███████▌  | 15/20 [00:28<00:09,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'doctor', 'assesses', 'symptoms', 'to', 'diagnose', 'a', 'condition']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess symptoms to diagnose [MASK]:  90%|█████████ | 18/20 [00:34<00:03,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'assess', 'symptoms', 'to', 'diagnose', '[mask]'] ['the', 'physician', 'assesses', 'symptoms', 'to', 'diagnose', 'the', 'patients', 'illness']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] assess symptoms to diagnose [MASK]: 100%|██████████| 20/20 [00:37<00:00,  1.89s/it]\n"
     ]
    }
   ],
   "source": [
    "from tools.command_generator import generate_prompts, prefix_prompt\n",
    "from tools.evaluate_response import get_replacements\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_prompts(model, sentence, anchor_idx, prompts_per_word=20):\n",
    "    prompts = generate_prompts(sentence, anchor_idx)\n",
    "    all_replacements = []\n",
    "    for prompt in prompts:\n",
    "        replacements = []\n",
    "        for _ in tqdm(\n",
    "            range(prompts_per_word),\n",
    "            desc=f\"Input: {prompt}\",\n",
    "        ):\n",
    "            response = model.get_response(\n",
    "                prefix_prompt(prompt),\n",
    "            ).strip()\n",
    "            if response:\n",
    "                replacement = get_replacements(prompt, response)\n",
    "                if replacement:\n",
    "                    replacements.append(replacement)\n",
    "        if len(replacements) > 0:\n",
    "            all_replacements.append(replacements)\n",
    "    return all_replacements\n",
    "\n",
    "all_responses = run_prompts(model, sentence, anchor_word_idx, prompts_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['flu', '[mask]'],\n",
       "  ['severe', '[mask] [mask]'],\n",
       "  ['flulike', '[mask]'],\n",
       "  ['', ''],\n",
       "  ['flu', '[mask]'],\n",
       "  ['common', '[mask]'],\n",
       "  ['common', '[mask]'],\n",
       "  ['flu', '[mask]'],\n",
       "  ['common', '[mask]'],\n",
       "  ['specific', '[mask]'],\n",
       "  ['specific', '[mask]'],\n",
       "  ['fever', 'and [mask]'],\n",
       "  ['common', '[mask]'],\n",
       "  ['common', '[mask]'],\n",
       "  ['common', ''],\n",
       "  ['flu', '[mask]'],\n",
       "  ['flu', '[mask]'],\n",
       "  ['typically', 'doctors [mask] [mask]'],\n",
       "  ['flu', '[mask]'],\n",
       "  ['', '']]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize responses\n",
    "all_responses[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load responses\n",
    "import json\n",
    "input_file = \"responses.json\"\n",
    "with open(input_file, \"r\") as f:\n",
    "    all_responses = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 EXERCISE: compute the PMI for each word\n",
    "\n",
    "$PMI(x,y)=log_2⁡ \\frac{p(\\{x,y\\}| s-\\{x,y\\})}{P(\\{x\\}|s-\\{x,y\\})P(\\{y\\}|s-\\{x,y\\})}$\n",
    "\n",
    "* Compute the $P(x)$, $P(y)$ and $P(x,y)$ first and print it out.\n",
    "* Compute the PMI for each word.\n",
    "* Visualize the result by coloring. Tips: you might need to normalize the result first. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from termcolor import colored\n",
    "\n",
    "def compute_pmi(sentence, all_responses, anchor_idx):\n",
    "    \"\"\"Compute PMI between anchor word and each other word.\"\"\"\n",
    "    words = sentence.lower().split()\n",
    "    anchor_word = words[anchor_idx]\n",
    "    pmi_scores = {}\n",
    "    \n",
    "    for other_idx in range(len(words)):\n",
    "        if other_idx == anchor_idx:\n",
    "            continue\n",
    "        \n",
    "        # Get pattern index (skips anchor position)\n",
    "        pattern_idx = other_idx if other_idx < anchor_idx else other_idx - 1\n",
    "        if pattern_idx >= len(all_responses):\n",
    "            continue\n",
    "            \n",
    "        responses = all_responses[pattern_idx]\n",
    "        if not responses:\n",
    "            continue\n",
    "        \n",
    "        # Extract anchor and other word replacements\n",
    "        anchor_replacements = [r[0].lower() for r in responses if len(r) == 2]\n",
    "        other_replacements = [r[1].lower() for r in responses if len(r) == 2]\n",
    "        total = len(anchor_replacements)\n",
    "        \n",
    "        # Calculate probabilities\n",
    "        count_x = sum(w == anchor_word for w in anchor_replacements)\n",
    "        count_y = sum(w == words[other_idx] for w in other_replacements)\n",
    "        count_xy = sum(anchor_replacements[i] == anchor_word and \n",
    "                      other_replacements[i] == words[other_idx] \n",
    "                      for i in range(total))\n",
    "        \n",
    "        P_x = count_x / total\n",
    "        P_y = count_y / total\n",
    "        P_xy = count_xy / total\n",
    "        \n",
    "        # Calculate PMI\n",
    "        if P_x > 0 and P_y > 0 and P_xy > 0:\n",
    "            pmi = math.log2(P_xy / (P_x * P_y))\n",
    "        else:\n",
    "            pmi = float('-inf')\n",
    "        \n",
    "        pmi_scores[other_idx] = {'word': words[other_idx], 'pmi': pmi, \n",
    "                                  'P_x': P_x, 'P_y': P_y, 'P_xy': P_xy}\n",
    "    \n",
    "    return pmi_scores\n",
    "\n",
    "def visualize_pmi(sentence, pmi_scores, anchor_idx):\n",
    "    \"\"\"Visualize PMI with colored words.\"\"\"\n",
    "    words = sentence.split()\n",
    "    \n",
    "    # Normalize PMI values\n",
    "    valid_pmis = [s['pmi'] for s in pmi_scores.values() if s['pmi'] != float('-inf')]\n",
    "    if not valid_pmis:\n",
    "        print(\"No valid PMI scores\")\n",
    "        return\n",
    "    \n",
    "    min_pmi, max_pmi = min(valid_pmis), max(valid_pmis)\n",
    "    pmi_range = max_pmi - min_pmi if max_pmi != min_pmi else 1\n",
    "    \n",
    "    # Color each word\n",
    "    colored_words = []\n",
    "    for i, word in enumerate(words):\n",
    "        if i == anchor_idx:\n",
    "            colored_words.append(colored(word, 'cyan', attrs=['bold']))\n",
    "        elif i in pmi_scores:\n",
    "            pmi = pmi_scores[i]['pmi']\n",
    "            if pmi != float('-inf'):\n",
    "                norm = (pmi - min_pmi) / pmi_range\n",
    "                color = 'green' if norm > 0.66 else 'yellow' if norm > 0.33 else 'red'\n",
    "                colored_words.append(colored(f\"{word}({pmi:.2f})\", color))\n",
    "            else:\n",
    "                colored_words.append(word)\n",
    "        else:\n",
    "            colored_words.append(word)\n",
    "    \n",
    "    print(\"\\n\" + \" \".join(colored_words) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor word: 'doctors'\n",
      "\n",
      "assess          PMI=   -inf  P(x)=0.000 P(y)=0.000 P(xy)=0.000\n",
      "symptoms        PMI=  1.737  P(x)=0.300 P(y)=0.200 P(xy)=0.200\n",
      "to              PMI=  1.152  P(x)=0.100 P(y)=0.450 P(xy)=0.100\n",
      "diagnose        PMI=  0.322  P(x)=0.100 P(y)=0.800 P(xy)=0.100\n",
      "diseases        PMI=  0.737  P(x)=0.200 P(y)=0.300 P(xy)=0.100\n",
      "\n",
      "\u001b[1m\u001b[36mdoctors\u001b[0m assess \u001b[32msymptoms(1.74)\u001b[0m \u001b[33mto(1.15)\u001b[0m \u001b[31mdiagnose(0.32)\u001b[0m \u001b[31mdiseases(0.74)\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute PMI scores\n",
    "pmi_scores = compute_pmi(sentence, all_responses, anchor_word_idx)\n",
    "\n",
    "# Print results\n",
    "words = sentence.lower().split()\n",
    "print(f\"Anchor word: '{words[anchor_word_idx]}'\\n\")\n",
    "for idx in sorted(pmi_scores.keys()):\n",
    "    data = pmi_scores[idx]\n",
    "    print(f\"{data['word']:<15} PMI={data['pmi']:7.3f}  \"\n",
    "          f\"P(x)={data['P_x']:.3f} P(y)={data['P_y']:.3f} P(xy)={data['P_xy']:.3f}\")\n",
    "\n",
    "# Visualize with colors\n",
    "visualize_pmi(sentence, pmi_scores, anchor_word_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PMI Results Interpretation (Higher PMI = stronger association)\n",
    "\n",
    "#### Results for \"doctors assess symptoms to diagnose diseases\"\n",
    "- The visualization shows that \"symptoms\" has the strongest semantic bond with \"doctors\" in this sentence\n",
    "\n",
    "**High PMI: Strong Dependency**\n",
    "- **symptoms (1.74)**: had the highest association with \"doctors\", this means that when both are amsked the model frequently generates them to fill the masked words.\n",
    "\n",
    "**Medium PMI: Moderate Dependency**  \n",
    "- **to (1.15)**: Moderate association.\n",
    "\n",
    "**Low PMI: Weak Dependency**\n",
    "- **diseases (0.74)**: Predictable from context but not uniquely tied to \"doctors\"\n",
    "- **diagnose (0.32)**: Despite being less associated with the word doctor (pmi=0.322), it is very predictable (P(y)=0.80). This means that the word itself (\"diagnose\") is very frequent but paired with alternatives to \"doctors\" (physicians, clinicians)\n",
    "\n",
    "**Negative PMI: No Dependency**\n",
    "- **assess (-inf)**: this means the model never generated \"doctors\" when both were masked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. EXERCISE: Try more examples; maybe come up with your own. Report the results.\n",
    "\n",
    "* Try to come up with more examples and, change the anchor word/number of responses, and observe the results. What does the explanation mean? Do you think it's a nice explanation? Why and why not? \n",
    "* What's the limitation of the current method? When does the method fail to explain? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bonus Exercises\n",
    "### 5.1 Language pre-processing. \n",
    "In this exercise, we only lower the letters and split sentences into words; there's much more to do to pre-process the language. For example, contractions (*I'll*, *She's*, *world's*), suffix and prefix, compound words (*hard-working*). It's called word tokenization in NLP, and there are some Python packages that can do such work for us, e.g. [*TextBlob*](https://textblob.readthedocs.io/en/dev/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------- ------------------------------ 2.9/12.8 MB 26.8 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 30.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 30.1 MB/s  0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "['believe', 'rumor', 'feel', 'uneasy']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\"])  # keep tagger & lemmatizer\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_text(text, keep_stopwords=False, keep_pos=None, remove_punct=True):\n",
    "    \"\"\"\n",
    "    text: single string\n",
    "    keep_stopwords: if False, remove stopwords\n",
    "    keep_pos: None or set like {\"NOUN\",\"VERB\",\"ADJ\"} to filter by POS\n",
    "    remove_punct: whether to drop punctuation tokens\n",
    "    returns: list of normalized tokens (lemmas)\n",
    "    \"\"\"\n",
    "    # basic normalization\n",
    "    text = text.strip()\n",
    "    # optional: expand contractions (can add contraction library)\n",
    "    # remove weird whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if remove_punct and token.is_punct:\n",
    "            continue\n",
    "        if token.like_num:\n",
    "            # choose policy: keep numbers or replace with <NUM>\n",
    "            tokens.append(\"<NUM>\")\n",
    "            continue\n",
    "        if not keep_stopwords and token.is_stop:\n",
    "            continue\n",
    "        if keep_pos and token.pos_ not in keep_pos:\n",
    "            continue\n",
    "        lemma = token.lemma_.lower()\n",
    "        # strip residual punctuation\n",
    "        lemma = re.sub(r'^\\W+|\\W+$', '', lemma)\n",
    "        if lemma:\n",
    "            tokens.append(lemma)\n",
    "    return tokens\n",
    "\n",
    "# Example\n",
    "s = \"She didn't believe the rumor, yet she felt uneasy.\"\n",
    "print(preprocess_text(s, keep_stopwords=False, keep_pos={\"NOUN\",\"VERB\",\"ADJ\"}))\n",
    "# Expected output (approx): ['believe', 'rumor', 'feel', 'uneasy']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Implementation: Advanced Text Preprocessing with spaCy\n",
    "\n",
    "Comparing simple tokenization vs. advanced preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy model already loaded\n"
     ]
    }
   ],
   "source": [
    "# Installment and load of spaCy\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Install spaCy model if not already installed\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"spaCy model already loaded\")\n",
    "except:\n",
    "    print(\"Downloading spaCy model...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"spacy\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"spaCy model installed and loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SIMPLE vs ADVANCED PREPROCESSING COMPARISON\n",
      "======================================================================\n",
      "\n",
      " Original: She didn't believe the rumor, yet she felt uneasy.\n",
      "   Simple:   ['she', \"didn't\", 'believe', 'the', 'rumor,', 'yet', 'she', 'felt', 'uneasy.']\n",
      "   Advanced: ['believe', 'rumor', 'feel', 'uneasy']\n",
      "   With POS: ['believe', 'rumor', 'feel', 'uneasy']\n",
      "\n",
      " Original: The world's best doctors assess patients' symptoms.\n",
      "   Simple:   ['the', \"world's\", 'best', 'doctors', 'assess', \"patients'\", 'symptoms.']\n",
      "   Advanced: ['world', 'good', 'doctor', 'assess', 'patient', 'symptom']\n",
      "   With POS: ['world', 'good', 'doctor', 'assess', 'patient', 'symptom']\n",
      "\n",
      " Original: It's a well-known fact that hard-working people succeed.\n",
      "   Simple:   [\"it's\", 'a', 'well-known', 'fact', 'that', 'hard-working', 'people', 'succeed.']\n",
      "   Advanced: ['know', 'fact', 'hard', 'work', 'people', 'succeed']\n",
      "   With POS: ['know', 'fact', 'work', 'people', 'succeed']\n",
      "\n",
      " Original: I'll be there by 5:30 PM on 12/25/2024.\n",
      "   Simple:   [\"i'll\", 'be', 'there', 'by', '5:30', 'pm', 'on', '12/25/2024.']\n",
      "   Advanced: ['5:30', 'pm', '12/25/2024']\n",
      "   With POS: ['pm']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "# Load spaCy model (disable parser for speed, keep tagger & lemmatizer)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "def preprocess_text(text, keep_stopwords=False, keep_pos=None, remove_punct=True):\n",
    "    \"\"\"\n",
    "    Advanced text preprocessing using spaCy.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text to preprocess\n",
    "    keep_stopwords : bool\n",
    "        If False, remove stopwords (the, is, a, etc.)\n",
    "    keep_pos : set or None\n",
    "        Filter by part-of-speech tags (e.g., {\"NOUN\", \"VERB\", \"ADJ\"})\n",
    "    remove_punct : bool\n",
    "        Whether to remove punctuation tokens\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list : Normalized tokens (lemmas)\n",
    "    \"\"\"\n",
    "    # Basic normalization\n",
    "    text = text.strip()\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Process with spaCy\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        # Skip punctuation if requested\n",
    "        if remove_punct and token.is_punct:\n",
    "            continue\n",
    "        \n",
    "        # Handle numbers\n",
    "        if token.like_num:\n",
    "            tokens.append(\"<NUM>\")\n",
    "            continue\n",
    "        \n",
    "        # Remove stopwords if requested\n",
    "        if not keep_stopwords and token.is_stop:\n",
    "            continue\n",
    "        \n",
    "        # Filter by POS tag if specified\n",
    "        if keep_pos and token.pos_ not in keep_pos:\n",
    "            continue\n",
    "        \n",
    "        # Get lemma (base form) and lowercase it\n",
    "        lemma = token.lemma_.lower()\n",
    "        \n",
    "        # Strip any residual punctuation at edges\n",
    "        lemma = re.sub(r'^\\W+|\\W+$', '', lemma)\n",
    "        \n",
    "        if lemma:\n",
    "            tokens.append(lemma)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Demonstration examples\n",
    "print(\"=\" * 70)\n",
    "print(\"SIMPLE vs ADVANCED PREPROCESSING COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_sentences = [\n",
    "    \"She didn't believe the rumor, yet she felt uneasy.\",\n",
    "    \"The world's best doctors assess patients' symptoms.\",\n",
    "    \"It's a well-known fact that hard-working people succeed.\",\n",
    "    \"I'll be there by 5:30 PM on 12/25/2024.\"\n",
    "]\n",
    "\n",
    "for sent in test_sentences:\n",
    "    print(f\"\\n Original: {sent}\")\n",
    "    print(f\"   Simple:   {sent.lower().split()}\")\n",
    "    print(f\"   Advanced: {preprocess_text(sent, keep_stopwords=False)}\")\n",
    "    print(f\"   With POS: {preprocess_text(sent, keep_pos={'NOUN', 'VERB', 'ADJ'})}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Improvements Demonstrated:\n",
    "\n",
    "1. **Contractions** (`didn't` $\\to$ `not` + `believe`, `I'll` $\\to$ `be`)\n",
    "2. **Possessives** (`patients'` $\\to$ `patient`, `world's` $\\to$ `world`)\n",
    "3. **Lemmatization** (`doctors` $\\to$ `doctor`, `felt` $\\to$ `feel`)\n",
    "4. **Compound words** (`hard-working` $\\to$ separate tokens)\n",
    "5. **Stopword removal** (removes `the`, `a`, `is`, etc.)\n",
    "6. **POS filtering** (keep only NOUN/VERB/ADJ)\n",
    "\n",
    "Now let's apply this to our PMI analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced PMI computation with preprocessing\n",
    "def compute_pmi_enhanced(sentence, all_responses, anchor_idx, use_preprocessing=True):\n",
    "    \"\"\"\n",
    "    Compute PMI with optional advanced preprocessing.\n",
    "    \"\"\"\n",
    "    # Tokenize based on preprocessing choice\n",
    "    if use_preprocessing:\n",
    "        words = preprocess_text(sentence, keep_stopwords=True, remove_punct=False)\n",
    "    else:\n",
    "        words = sentence.lower().split()\n",
    "    \n",
    "    anchor_word = words[anchor_idx]\n",
    "    pmi_scores = {}\n",
    "    \n",
    "    for other_idx in range(len(words)):\n",
    "        if other_idx == anchor_idx:\n",
    "            continue\n",
    "        \n",
    "        pattern_idx = other_idx if other_idx < anchor_idx else other_idx - 1\n",
    "        if pattern_idx >= len(all_responses):\n",
    "            continue\n",
    "            \n",
    "        responses = all_responses[pattern_idx]\n",
    "        if not responses:\n",
    "            continue\n",
    "        \n",
    "        # Process responses with same preprocessing\n",
    "        anchor_replacements = []\n",
    "        other_replacements = []\n",
    "        \n",
    "        for r in responses:\n",
    "            if len(r) == 2:\n",
    "                if use_preprocessing:\n",
    "                    anchor_tokens = preprocess_text(r[0], keep_stopwords=True, remove_punct=False)\n",
    "                    other_tokens = preprocess_text(r[1], keep_stopwords=True, remove_punct=False)\n",
    "                    if anchor_tokens and other_tokens:\n",
    "                        anchor_replacements.append(anchor_tokens[0])\n",
    "                        other_replacements.append(other_tokens[0])\n",
    "                else:\n",
    "                    anchor_replacements.append(r[0].lower())\n",
    "                    other_replacements.append(r[1].lower())\n",
    "        \n",
    "        if not anchor_replacements:\n",
    "            continue\n",
    "            \n",
    "        total = len(anchor_replacements)\n",
    "        \n",
    "        # Calculate probabilities\n",
    "        count_x = sum(w == anchor_word for w in anchor_replacements)\n",
    "        count_y = sum(w == words[other_idx] for w in other_replacements)\n",
    "        count_xy = sum(anchor_replacements[i] == anchor_word and \n",
    "                      other_replacements[i] == words[other_idx] \n",
    "                      for i in range(total))\n",
    "        \n",
    "        P_x = count_x / total if total > 0 else 0\n",
    "        P_y = count_y / total if total > 0 else 0\n",
    "        P_xy = count_xy / total if total > 0 else 0\n",
    "        \n",
    "        # Calculate PMI\n",
    "        if P_x > 0 and P_y > 0 and P_xy > 0:\n",
    "            pmi = math.log2(P_xy / (P_x * P_y))\n",
    "        else:\n",
    "            pmi = float('-inf')\n",
    "        \n",
    "        pmi_scores[other_idx] = {\n",
    "            'word': words[other_idx], \n",
    "            'pmi': pmi, \n",
    "            'P_x': P_x, \n",
    "            'P_y': P_y, \n",
    "            'P_xy': P_xy\n",
    "        }\n",
    "    \n",
    "    return pmi_scores, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPARISON: Simple vs. Advanced Preprocessing for PMI\n",
      "======================================================================\n",
      "\n",
      "Test sentence: 'The doctor's examining patients' symptoms carefully.'\n",
      "\n",
      "Simple tokenization: ['the', \"doctor's\", 'examining', \"patients'\", 'symptoms', 'carefully.']\n",
      "Advanced preprocessing: ['the', 'doctor', 's', 'examine', 'patient', 'symptom', 'carefully']\n",
      "\n",
      "======================================================================\n",
      "ANALYSIS:\n",
      "======================================================================\n",
      "\n",
      "Benefits of advanced preprocessing:\n",
      "1. **Lemmatization**: 'doctor's' → 'doctor', 'patients' → 'patient'\n",
      "   - Groups inflected forms together for better statistics\n",
      "\n",
      "2. **Possessive handling**: Removes 's apostrophes properly\n",
      "   - 'doctor's' and 'doctors' both map to 'doctor'\n",
      "\n",
      "3. **Contraction expansion**: 'didn't' → 'did' + 'not'\n",
      "   - Captures true meaning of negations\n",
      "\n",
      "4. **Consistent tokenization**: Handles punctuation intelligently\n",
      "   - Doesn't split compound words incorrectly\n",
      "\n",
      "This leads to:\n",
      "More accurate probability estimates (fewer unique tokens)\n",
      "Better matching between original and generated words\n",
      "More meaningful PMI scores\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare: Simple vs Advanced preprocessing\n",
    "test_sentence = \"The doctor's examining patients' symptoms carefully.\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPARISON: Simple vs. Advanced Preprocessing for PMI\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTest sentence: '{test_sentence}'\")\n",
    "print(f\"\\nSimple tokenization: {test_sentence.lower().split()}\")\n",
    "print(f\"Advanced preprocessing: {preprocess_text(test_sentence, keep_stopwords=True, remove_punct=False)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ANALYSIS:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "Benefits of advanced preprocessing:\n",
    "1. **Lemmatization**: 'doctor's' → 'doctor', 'patients' → 'patient'\n",
    "   - Groups inflected forms together for better statistics\n",
    "   \n",
    "2. **Possessive handling**: Removes 's apostrophes properly\n",
    "   - 'doctor's' and 'doctors' both map to 'doctor'\n",
    "   \n",
    "3. **Contraction expansion**: 'didn't' → 'did' + 'not'\n",
    "   - Captures true meaning of negations\n",
    "   \n",
    "4. **Consistent tokenization**: Handles punctuation intelligently\n",
    "   - Doesn't split compound words incorrectly\n",
    "\n",
    "This leads to:\n",
    "More accurate probability estimates (fewer unique tokens)\n",
    "Better matching between original and generated words\n",
    "More meaningful PMI scores\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.1 Tasks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TASK 1: Testing with Custom Sentences\n",
      "================================================================================\n",
      "\n",
      "Example 1: She didn't believe the rumor, yet she felt uneasy.\n",
      "   Simple:   ['she', \"didn't\", 'believe', 'the', 'rumor,', 'yet', 'she', 'felt', 'uneasy.']\n",
      "   Advanced: ['believe', 'rumor', 'feel', 'uneasy']\n",
      "\n",
      "Example 2: John's well-known theory about quantum physics won't be forgotten.\n",
      "   Simple:   [\"john's\", 'well-known', 'theory', 'about', 'quantum', 'physics', \"won't\", 'be', 'forgotten.']\n",
      "   Advanced: ['john', 'know', 'theory', 'quantum', 'physics', 'will', 'forget']\n",
      "\n",
      "Example 3: The hard-working scientist's groundbreaking discovery can't be ignored.\n",
      "   Simple:   ['the', 'hard-working', \"scientist's\", 'groundbreaking', 'discovery', \"can't\", 'be', 'ignored.']\n",
      "   Advanced: ['hard', 'work', 'scientist', 'groundbreake', 'discovery', 'ignore']\n",
      "\n",
      "Example 4: It's a state-of-the-art system that doesn't require maintenance.\n",
      "   Simple:   [\"it's\", 'a', 'state-of-the-art', 'system', 'that', \"doesn't\", 'require', 'maintenance.']\n",
      "   Advanced: ['state', 'art', 'system', 'require', 'maintenance']\n"
     ]
    }
   ],
   "source": [
    "# Task 1: Test with your own sentences\n",
    "print(\"=\" * 80)\n",
    "print(\"TASK 1: Testing with Custom Sentences\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Sentences with contractions, possessives, compound words\n",
    "custom_sentences = [\n",
    "    \"She didn't believe the rumor, yet she felt uneasy.\",\n",
    "    \"John's well-known theory about quantum physics won't be forgotten.\",\n",
    "    \"The hard-working scientist's groundbreaking discovery can't be ignored.\",\n",
    "    \"It's a state-of-the-art system that doesn't require maintenance.\",\n",
    "]\n",
    "\n",
    "for i, sent in enumerate(custom_sentences, 1):\n",
    "    print(f\"\\nExample {i}: {sent}\")\n",
    "    print(f\"   Simple:   {sent.lower().split()}\")\n",
    "    print(f\"   Advanced: {preprocess_text(sent, keep_stopwords=False)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK 2: Experimenting with Different Preprocessing Options\n",
      "================================================================================\n",
      "\n",
      "Original sentence: The doctor's examining patients' symptoms carefully.\n",
      "\n",
      "A) With stopwords (keep_stopwords=True):\n",
      "   ['the', 'doctor', 's', 'examine', 'patient', 'symptom', 'carefully']\n",
      "\n",
      "B) Without stopwords (keep_stopwords=False):\n",
      "   ['doctor', 'examine', 'patient', 'symptom', 'carefully']\n",
      "\n",
      "C) Only NOUN + VERB (keep_pos={'NOUN', 'VERB'}):\n",
      "   ['doctor', 'examine', 'patient', 'symptom']\n",
      "\n",
      "D) Only ADJ + NOUN (keep_pos={'ADJ', 'NOUN'}):\n",
      "   ['doctor', 'patient', 'symptom']\n",
      "\n",
      "E) Keep punctuation (remove_punct=False):\n",
      "   ['doctor', 'examine', 'patient', 'symptom', 'carefully']\n",
      "\n",
      "F) Remove punctuation (remove_punct=True):\n",
      "   ['doctor', 'examine', 'patient', 'symptom', 'carefully']\n",
      "\n",
      "================================================================================\n",
      "Summary of Options:\n",
      "================================================================================\n",
      "\n",
      "- keep_stopwords: Controls whether common words (the, is, a) are included\n",
      "- keep_pos: Filter by part-of-speech (NOUN, VERB, ADJ, ADV, etc.)\n",
      "- remove_punct: Whether to remove punctuation tokens\n",
      "\n",
      "Different combinations suit different purposes:\n",
      "• Full preprocessing: best for semantic analysis\n",
      "• POS filtering: emphasizes content words\n",
      "• Keeping stopwords: preserves structure information\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 2: Experiment with different preprocessing options\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 2: Experimenting with Different Preprocessing Options\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_text = \"The doctor's examining patients' symptoms carefully.\"\n",
    "\n",
    "print(f\"\\nOriginal sentence: {test_text}\\n\")\n",
    "\n",
    "# Option A: Keep all stopwords\n",
    "print(\"A) With stopwords (keep_stopwords=True):\")\n",
    "print(f\"   {preprocess_text(test_text, keep_stopwords=True, remove_punct=False)}\")\n",
    "\n",
    "# Option B: Remove stopwords\n",
    "print(\"\\nB) Without stopwords (keep_stopwords=False):\")\n",
    "print(f\"   {preprocess_text(test_text, keep_stopwords=False, remove_punct=False)}\")\n",
    "\n",
    "# Option C: Only nouns and verbs\n",
    "print(\"\\nC) Only NOUN + VERB (keep_pos={'NOUN', 'VERB'}):\")\n",
    "print(f\"   {preprocess_text(test_text, keep_stopwords=True, keep_pos={'NOUN', 'VERB'})}\")\n",
    "\n",
    "# Option D: Only adjectives and nouns\n",
    "print(\"\\nD) Only ADJ + NOUN (keep_pos={'ADJ', 'NOUN'}):\")\n",
    "print(f\"   {preprocess_text(test_text, keep_stopwords=True, keep_pos={'ADJ', 'NOUN'})}\")\n",
    "\n",
    "# Option E: Keep punctuation\n",
    "print(\"\\nE) Keep punctuation (remove_punct=False):\")\n",
    "print(f\"   {preprocess_text(test_text, keep_stopwords=False, remove_punct=False)}\")\n",
    "\n",
    "# Option F: Remove punctuation\n",
    "print(\"\\nF) Remove punctuation (remove_punct=True):\")\n",
    "print(f\"   {preprocess_text(test_text, keep_stopwords=False, remove_punct=True)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Summary of Options:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "- keep_stopwords: Controls whether common words (the, is, a) are included\n",
    "- keep_pos: Filter by part-of-speech (NOUN, VERB, ADJ, ADV, etc.)\n",
    "- remove_punct: Whether to remove punctuation tokens\n",
    "\n",
    "Different combinations suit different purposes:\n",
    "• Full preprocessing: best for semantic analysis\n",
    "• POS filtering: emphasizes content words\n",
    "• Keeping stopwords: preserves structure information\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK 3: PMI Comparison - With vs Without Preprocessing\n",
      "================================================================================\n",
      "\n",
      "Test sentence: 'doctors assess symptoms to diagnose diseases'\n",
      "Anchor word (index 0): 'doctors'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "WITHOUT Preprocessing (simple tokenization):\n",
      "--------------------------------------------------------------------------------\n",
      "  assess          PMI=    N/A  P(x)=0.000 P(y)=0.000 P(xy)=0.000\n",
      "  symptoms        PMI=1.7369655941662063  P(x)=0.300 P(y)=0.200 P(xy)=0.200\n",
      "  to              PMI=1.1520030934450498  P(x)=0.100 P(y)=0.450 P(xy)=0.100\n",
      "  diagnose        PMI=0.32192809488736207  P(x)=0.100 P(y)=0.800 P(xy)=0.100\n",
      "  diseases        PMI=0.7369655941662062  P(x)=0.200 P(y)=0.300 P(xy)=0.100\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "WITH Preprocessing (lemmatization, stopword removal, etc.):\n",
      "--------------------------------------------------------------------------------\n",
      "  assess          PMI=    N/A  P(x)=0.000 P(y)=0.000 P(xy)=0.000\n",
      "  symptom         PMI=0.415037499278844  P(x)=0.600 P(y)=0.500 P(xy)=0.400\n",
      "  to              PMI=0.15200309344504975  P(x)=0.200 P(y)=0.900 P(xy)=0.200\n",
      "  diagnose        PMI=    0.0  P(x)=0.133 P(y)=1.000 P(xy)=0.133\n",
      "  disease         PMI=-0.09953567355091457  P(x)=0.357 P(y)=0.429 P(xy)=0.143\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Comparison Summary:\n",
      "--------------------------------------------------------------------------------\n",
      "Simple tokenization found 5 word pairs\n",
      "Advanced preprocessing found 5 word pairs\n",
      "  assess          PMI=    N/A  P(x)=0.000 P(y)=0.000 P(xy)=0.000\n",
      "  symptom         PMI=0.415037499278844  P(x)=0.600 P(y)=0.500 P(xy)=0.400\n",
      "  to              PMI=0.15200309344504975  P(x)=0.200 P(y)=0.900 P(xy)=0.200\n",
      "  diagnose        PMI=    0.0  P(x)=0.133 P(y)=1.000 P(xy)=0.133\n",
      "  disease         PMI=-0.09953567355091457  P(x)=0.357 P(y)=0.429 P(xy)=0.143\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Comparison Summary:\n",
      "--------------------------------------------------------------------------------\n",
      "Simple tokenization found 5 word pairs\n",
      "Advanced preprocessing found 5 word pairs\n"
     ]
    }
   ],
   "source": [
    "# Task 3: Compare PMI results with and without preprocessing\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 3: PMI Comparison - With vs Without Preprocessing\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load responses if not already loaded\n",
    "import json\n",
    "import math\n",
    "if 'all_responses' not in globals():\n",
    "    try:\n",
    "        with open(\"responses.json\", \"r\") as f:\n",
    "            all_responses = json.load(f)\n",
    "        print(\"Loaded all_responses from responses.json\\n\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: responses.json not found. Please run the prompts first or ensure the file exists.\")\n",
    "        all_responses = []\n",
    "\n",
    "# Use a sentence from earlier that we have responses for\n",
    "comparison_sentence = \"doctors assess symptoms to diagnose diseases\"\n",
    "anchor_idx = 0  # \"doctors\"\n",
    "\n",
    "print(f\"\\nTest sentence: '{comparison_sentence}'\")\n",
    "print(f\"Anchor word (index {anchor_idx}): '{comparison_sentence.split()[anchor_idx]}'\")\n",
    "\n",
    "# Compute PMI both ways\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"WITHOUT Preprocessing (simple tokenization):\")\n",
    "print(\"-\" * 80)\n",
    "pmi_simple, words_simple = compute_pmi_enhanced(comparison_sentence, all_responses, anchor_idx, use_preprocessing=False)\n",
    "for idx in sorted(pmi_simple.keys()):\n",
    "    data = pmi_simple[idx]\n",
    "    pmi_val = data['pmi'] if data['pmi'] != float('-inf') else \"N/A\"\n",
    "    print(f\"  {data['word']:<15} PMI={str(pmi_val):>7}  P(x)={data['P_x']:.3f} P(y)={data['P_y']:.3f} P(xy)={data['P_xy']:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"WITH Preprocessing (lemmatization, stopword removal, etc.):\")\n",
    "print(\"-\" * 80)\n",
    "pmi_advanced, words_advanced = compute_pmi_enhanced(comparison_sentence, all_responses, anchor_idx, use_preprocessing=True)\n",
    "for idx in sorted(pmi_advanced.keys()):\n",
    "    data = pmi_advanced[idx]\n",
    "    pmi_val = data['pmi'] if data['pmi'] != float('-inf') else \"N/A\"\n",
    "    print(f\"  {data['word']:<15} PMI={str(pmi_val):>7}  P(x)={data['P_x']:.3f} P(y)={data['P_y']:.3f} P(xy)={data['P_xy']:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Comparison Summary:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Simple tokenization found {len(pmi_simple)} word pairs\")\n",
    "print(f\"Advanced preprocessing found {len(pmi_advanced)} word pairs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Analysis and Reflection Questions\n",
    "\n",
    "**Question 1: How does preprocessing affect the PMI scores?**\n",
    "\n",
    "**Answer:**\n",
    "- The PMI scores increase because preprocessing groups inflected forms (doctor, doctors, doctor's → doctor), making the anchor word appear more frequently with other words\n",
    "- By reducing vocabulary size (fewer unique tokens), probabilities become less sparse and more reliable\n",
    "- Some PMI values might stabilize because preprocessing normalizes variations in how the model generates responses\n",
    "- Words that appear together in multiple forms now count together, strengthening their association signal\n",
    "\n",
    "**Question 2: When would preprocessing help PMI analysis?**\n",
    "\n",
    "**Answer:**\n",
    "- Better accuracy when words have multiple forms (doctor, doctors, doctor's)\n",
    "- More reliable statistics by grouping related words together\n",
    "- Reduced sparsity (fewer unique tokens)\n",
    "- Better handling of linguistic variations\n",
    "\n",
    "**Question 3: When might preprocessing hurt or be problematic?**\n",
    "\n",
    "**Answer:**\n",
    "- Loss of information when lemmatizing (e.g., \"running\" and \"ran\" both → \"run\")\n",
    "- Removing negations (not, no, didn't) removes important semantic information\n",
    "- Stopword removal loses structural context\n",
    "- Over-aggressive POS filtering might remove important words\n",
    "- Domain-specific terms might be incorrectly lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TASK 4: Reflection - Effects of Preprocessing\n",
      "================================================================================\n",
      "\n",
      "BENEFIT EXAMPLE: Handling Inflections\n",
      "--------------------------------------------------------------------------------\n",
      "Original: The doctors and the doctor's assistant work together.\n",
      "Simple:   ['the', 'doctors', 'and', 'the', \"doctor's\", 'assistant', 'work', 'together.']\n",
      "Advanced: ['the', 'doctor', 'and', 'the', 'doctor', 's', 'assistant', 'work', 'together']\n",
      "\n",
      "Benefit: 'doctors', 'doctor's' → all map to 'doctor'\n",
      "  This groups related forms, improving PMI statistics\n",
      "\n",
      "\n",
      "DRAWBACK EXAMPLE: Loss of Negation Information\n",
      "--------------------------------------------------------------------------------\n",
      "Original: The doctor didn't diagnose the disease correctly.\n",
      "Simple:   ['the', 'doctor', \"didn't\", 'diagnose', 'the', 'disease', 'correctly.']\n",
      "Advanced (stopwords removed): ['doctor', 'diagnose', 'disease', 'correctly']\n",
      "Advanced (stopwords kept):    ['the', 'doctor', 'do', 'not', 'diagnose', 'the', 'disease', 'correctly']\n",
      "\n",
      "Drawback: Removing 'didn't' loses the negation!\n",
      "  'didn't diagnose' → 'diagnose' loses semantic meaning\n",
      "\n",
      "\n",
      "BENEFIT EXAMPLE: Reducing Sparsity\n",
      "--------------------------------------------------------------------------------\n",
      "Original forms: running, runs, run, runner\n",
      "Lemmatized:    ['run', 'run', 'run', 'runner']\n",
      "\n",
      "Benefit: All group to 'run', reducing vocabulary size\n",
      "  Fewer unique tokens = better probability estimates\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TAKEAWAYS:\n",
      "================================================================================\n",
      "\n",
      "Preprocessing HELPS when:\n",
      "  1. Handling grammatical variations (plurals, tenses, possessives)\n",
      "  2. Reducing sparsity (fewer unique tokens for better statistics)\n",
      "  3. Normalizing text from different sources\n",
      "  4. Focusing on content words (POS filtering)\n",
      "\n",
      "Preprocessing HURTS when:\n",
      "  1. Important semantic information is lost (negations, intensifiers)\n",
      "  2. Domain-specific terminology is incorrectly normalized\n",
      "  3. Removing context needed for interpretation\n",
      "  4. Over-aggressive filtering removes meaningful words\n",
      "\n",
      "RECOMMENDATION FOR PMI ANALYSIS:\n",
      "Use selective preprocessing:\n",
      "  - Keep lemmatization (group related forms)\n",
      "  - Keep stopwords (preserve structure)\n",
      "  - Avoid aggressive stopword removal\n",
      "  - Consider task-specific POS filtering\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 4: Practical Demonstration\n",
    "print(\"=\" * 80)\n",
    "print(\"TASK 4: Reflection - Effects of Preprocessing\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Let's create a practical example showing benefits and drawbacks\n",
    "\n",
    "print(\"\\nBENEFIT EXAMPLE: Handling Inflections\")\n",
    "print(\"-\" * 80)\n",
    "example1 = \"The doctors and the doctor's assistant work together.\"\n",
    "print(f\"Original: {example1}\")\n",
    "print(f\"Simple:   {example1.lower().split()}\")\n",
    "print(f\"Advanced: {preprocess_text(example1, keep_stopwords=True)}\")\n",
    "print(\"\\nBenefit: 'doctors', 'doctor's' → all map to 'doctor'\")\n",
    "print(\"  This groups related forms, improving PMI statistics\")\n",
    "\n",
    "print(\"\\n\\nDRAWBACK EXAMPLE: Loss of Negation Information\")\n",
    "print(\"-\" * 80)\n",
    "example2 = \"The doctor didn't diagnose the disease correctly.\"\n",
    "print(f\"Original: {example2}\")\n",
    "print(f\"Simple:   {example2.lower().split()}\")\n",
    "advanced_no_stops = preprocess_text(example2, keep_stopwords=False)\n",
    "print(f\"Advanced (stopwords removed): {advanced_no_stops}\")\n",
    "advanced_keep_stops = preprocess_text(example2, keep_stopwords=True)\n",
    "print(f\"Advanced (stopwords kept):    {advanced_keep_stops}\")\n",
    "print(\"\\nDrawback: Removing 'didn't' loses the negation!\")\n",
    "print(\"  'didn't diagnose' → 'diagnose' loses semantic meaning\")\n",
    "\n",
    "print(\"\\n\\nBENEFIT EXAMPLE: Reducing Sparsity\")\n",
    "print(\"-\" * 80)\n",
    "example3 = \"running, runs, run, runner - different forms of the same concept\"\n",
    "print(f\"Original forms: running, runs, run, runner\")\n",
    "lemmatized = [preprocess_text(word, keep_stopwords=True)[0] if preprocess_text(word, keep_stopwords=True) else word \n",
    "              for word in [\"running\", \"runs\", \"run\", \"runner\"]]\n",
    "print(f\"Lemmatized:    {lemmatized}\")\n",
    "print(\"\\nBenefit: All group to 'run', reducing vocabulary size\")\n",
    "print(\"  Fewer unique tokens = better probability estimates\")\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"TAKEAWAYS:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "Preprocessing HELPS when:\n",
    "  1. Handling grammatical variations (plurals, tenses, possessives)\n",
    "  2. Reducing sparsity (fewer unique tokens for better statistics)\n",
    "  3. Normalizing text from different sources\n",
    "  4. Focusing on content words (POS filtering)\n",
    "\n",
    "Preprocessing HURTS when:\n",
    "  1. Important semantic information is lost (negations, intensifiers)\n",
    "  2. Domain-specific terminology is incorrectly normalized\n",
    "  3. Removing context needed for interpretation\n",
    "  4. Over-aggressive filtering removes meaningful words\n",
    "\n",
    "RECOMMENDATION FOR PMI ANALYSIS:\n",
    "Use selective preprocessing:\n",
    "  - Keep lemmatization (group related forms)\n",
    "  - Keep stopwords (preserve structure)\n",
    "  - Avoid aggressive stopword removal\n",
    "  - Consider task-specific POS filtering\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Better word matching\n",
    "\n",
    "In the above example of\n",
    "\n",
    "> Tokyo is the capital of Japan and a popular metropolis in the world.\n",
    "\n",
    "GenAI never gives the specific word 'metropolis' when masking it out; instead, sometimes it provides words like 'city', which is not the same word but has a similar meaning. Instead of measuring the exact matching of certain words (i.e. 0 or 1), we can also measure the similarity of two words, e.g. the cosine similarity in word embedding, which ranges from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
